{"func_code_string": "def __msgc_step3_discontinuity_localization(self):\n        \"\"\"\n        Estimate discontinuity in basis of low resolution image segmentation.\n        :return: discontinuity in low resolution\n        \"\"\"\n        import scipy\n\n        start = self._start_time\n        seg = 1 - self.segmentation.astype(np.int8)\n        self.stats[\"low level object voxels\"] = np.sum(seg)\n        self.stats[\"low level image voxels\"] = np.prod(seg.shape)\n        # in seg is now stored low resolution segmentation\n        # back to normal parameters\n        # step 2: discontinuity localization\n        # self.segparams = sparams_hi\n        seg_border = scipy.ndimage.filters.laplace(seg, mode=\"constant\")\n        logger.debug(\"seg_border: %s\", scipy.stats.describe(seg_border, axis=None))\n        # logger.debug(str(np.max(seg_border)))\n        # logger.debug(str(np.min(seg_border)))\n        seg_border[seg_border != 0] = 1\n        logger.debug(\"seg_border: %s\", scipy.stats.describe(seg_border, axis=None))\n        # scipy.ndimage.morphology.distance_transform_edt\n        boundary_dilatation_distance = self.segparams[\"boundary_dilatation_distance\"]\n        seg = scipy.ndimage.morphology.binary_dilation(\n            seg_border,\n            # seg,\n            np.ones(\n                [\n                    (boundary_dilatation_distance * 2) + 1,\n                    (boundary_dilatation_distance * 2) + 1,\n                    (boundary_dilatation_distance * 2) + 1,\n                ]\n            ),\n        )\n        if self.keep_temp_properties:\n            self.temp_msgc_lowres_discontinuity = seg\n        else:\n            self.temp_msgc_lowres_discontinuity = None\n\n        if self.debug_images:\n            import sed3\n\n            pd = sed3.sed3(seg_border)  # ), contour=seg)\n            pd.show()\n            pd = sed3.sed3(seg)  # ), contour=seg)\n            pd.show()\n        # segzoom = scipy.ndimage.interpolation.zoom(seg.astype('float'), zoom,\n        #                                                order=0).astype('int8')\n        self.stats[\"t3\"] = time.time() - start\n        return seg"}
{"func_code_string": "def __multiscale_gc_lo2hi_run(self):  # , pyed):\n        \"\"\"\n        Run Graph-Cut segmentation with refinement of low resolution multiscale graph.\n        In first step is performed normal GC on low resolution data\n        Second step construct finer grid on edges of segmentation from first\n        step.\n        There is no option for use without `use_boundary_penalties`\n        \"\"\"\n        # from PyQt4.QtCore import pyqtRemoveInputHook\n        # pyqtRemoveInputHook()\n        self._msgc_lo2hi_resize_init()\n        self.__msgc_step0_init()\n\n        hard_constraints = self.__msgc_step12_low_resolution_segmentation()\n        # ===== high resolution data processing\n        seg = self.__msgc_step3_discontinuity_localization()\n\n        self.stats[\"t3.1\"] = (time.time() - self._start_time)\n        graph = Graph(\n            seg,\n            voxelsize=self.voxelsize,\n            nsplit=self.segparams[\"block_size\"],\n            edge_weight_table=self._msgc_npenalty_table,\n            compute_low_nodes_index=True,\n        )\n\n        # graph.run() = graph.generate_base_grid() + graph.split_voxels()\n        # graph.run()\n        graph.generate_base_grid()\n        self.stats[\"t3.2\"] = (time.time() - self._start_time)\n        graph.split_voxels()\n\n        self.stats[\"t3.3\"] = (time.time() - self._start_time)\n\n        self.stats.update(graph.stats)\n        self.stats[\"t4\"] = (time.time() - self._start_time)\n        mul_mask, mul_val = self.__msgc_tlinks_area_weight_from_low_segmentation(seg)\n        area_weight = 1\n        unariesalt = self.__create_tlinks(\n            self.img,\n            self.voxelsize,\n            self.seeds,\n            area_weight=area_weight,\n            hard_constraints=hard_constraints,\n            mul_mask=None,\n            mul_val=None,\n        )\n        # N-links prepared\n        self.stats[\"t5\"] = (time.time() - self._start_time)\n        un, ind = np.unique(graph.msinds, return_index=True)\n        self.stats[\"t6\"] = (time.time() - self._start_time)\n\n        self.stats[\"t7\"] = (time.time() - self._start_time)\n        unariesalt2_lo2hi = np.hstack(\n            [unariesalt[ind, 0, 0].reshape(-1, 1), unariesalt[ind, 0, 1].reshape(-1, 1)]\n        )\n        nlinks_lo2hi = np.hstack([graph.edges, graph.edges_weights.reshape(-1, 1)])\n        if self.debug_images:\n            import sed3\n\n            ed = sed3.sed3(unariesalt[:, :, 0].reshape(self.img.shape))\n            ed.show()\n            import sed3\n\n            ed = sed3.sed3(unariesalt[:, :, 1].reshape(self.img.shape))\n            ed.show()\n            # ed = sed3.sed3(seg)\n            # ed.show()\n            # import sed3\n            # ed = sed3.sed3(graph.data)\n            # ed.show()\n            # import sed3\n            # ed = sed3.sed3(graph.msinds)\n            # ed.show()\n\n        # nlinks, unariesalt2, msinds = self.__msgc_step45678_construct_graph(area_weight, hard_constraints, seg)\n        # self.__msgc_step9_finish_perform_gc_and_reshape(nlinks, unariesalt2, msinds)\n        self.__msgc_step9_finish_perform_gc_and_reshape(\n            nlinks_lo2hi, unariesalt2_lo2hi, graph.msinds\n        )\n        self._msgc_lo2hi_resize_clean_finish()"}
{"func_code_string": "def __multiscale_gc_hi2lo_run(self):  # , pyed):\n        \"\"\"\n        Run Graph-Cut segmentation with simplifiyng of high resolution multiscale graph.\n        In first step is performed normal GC on low resolution data\n        Second step construct finer grid on edges of segmentation from first\n        step.\n        There is no option for use without `use_boundary_penalties`\n        \"\"\"\n        # from PyQt4.QtCore import pyqtRemoveInputHook\n        # pyqtRemoveInputHook()\n\n        self.__msgc_step0_init()\n        hard_constraints = self.__msgc_step12_low_resolution_segmentation()\n        # ===== high resolution data processing\n        seg = self.__msgc_step3_discontinuity_localization()\n        nlinks, unariesalt2, msinds = self.__msgc_step45678_hi2lo_construct_graph(\n            hard_constraints, seg\n        )\n        self.__msgc_step9_finish_perform_gc_and_reshape(nlinks, unariesalt2, msinds)"}
{"func_code_string": "def __ordered_values_by_indexes(self, data, inds):\n        \"\"\"\n        Return values (intensities) by indexes.\n\n        Used for multiscale graph cut.\n        data = [[0 1 1],\n                [0 2 2],\n                [0 2 2]]\n\n        inds = [[0 1 2],\n                [3 4 4],\n                [5 4 4]]\n\n        return: [0, 1, 1, 0, 2, 0]\n\n        If the data are not consistent, it will take the maximal value\n\n        \"\"\"\n        # get unique labels and their first indexes\n        # lab, linds = np.unique(inds, return_index=True)\n        # compute values by indexes\n        # values = data.reshape(-1)[linds]\n\n        # alternative slow implementation\n        # if there are different data on same index, it will take\n        # maximal value\n        # lab = np.unique(inds)\n        # values = [0]*len(lab)\n        # for label in lab:\n        #     values[label] = np.max(data[inds == label])\n        #\n        # values = np.asarray(values)\n\n        # yet another implementation\n        values = [None] * (np.max(inds) + 1)\n\n        linear_inds = inds.ravel()\n        linear_data = data.ravel()\n        for i in range(0, len(linear_inds)):\n            # going over all data pixels\n\n            if values[linear_inds[i]] is None:\n                # this index is found for first\n                values[linear_inds[i]] = linear_data[i]\n            elif values[linear_inds[i]] < linear_data[i]:\n                # here can be changed maximal or minimal value\n                values[linear_inds[i]] = linear_data[i]\n\n        values = np.asarray(values)\n\n        return values"}
{"func_code_string": "def __hi2lo_multiscale_indexes(self, mask, orig_shape):  # , zoom):\n        \"\"\"\n        Function computes multiscale indexes of ndarray.\n\n        mask: Says where is original resolution (0) and where is small\n        resolution (1). Mask is in small resolution.\n\n        orig_shape: Original shape of input data.\n        zoom: Usually number greater then 1\n\n        result = [[0 1 2],\n                  [3 4 4],\n                  [5 4 4]]\n        \"\"\"\n\n        mask_orig = zoom_to_shape(mask, orig_shape, dtype=np.int8)\n\n        inds_small = np.arange(mask.size).reshape(mask.shape)\n        inds_small_in_orig = zoom_to_shape(inds_small, orig_shape, dtype=np.int8)\n        inds_orig = np.arange(np.prod(orig_shape)).reshape(orig_shape)\n\n        # inds_orig = inds_orig * mask_orig\n        inds_orig += np.max(inds_small_in_orig) + 1\n        # print 'indexes'\n        # import py3DSeedEditor as ped\n        # import pdb; pdb.set_trace() # BREAKPOINT\n\n        #  '==' is not the same as 'is' for numpy.array\n        inds_small_in_orig[mask_orig == True] = inds_orig[mask_orig == True]  # noqa\n        inds = inds_small_in_orig\n        # print np.max(inds)\n        # print np.min(inds)\n        inds = relabel_squeeze(inds)\n        logger.debug(\n            \"Index after relabeling: %s\", scipy.stats.describe(inds, axis=None)\n        )\n        # logger.debug(\"Minimal index after relabeling: \" + str(np.min(inds)))\n        # inds_orig[mask_orig==True] = 0\n        # inds_small_in_orig[mask_orig==False] = 0\n        # inds = (inds_orig + np.max(inds_small_in_orig) + 1) + inds_small_in_orig\n\n        return inds, mask_orig"}
{"func_code_string": "def interactivity(self, min_val=None, max_val=None, qt_app=None):\n        \"\"\"\n        Interactive seed setting with 3d seed editor\n        \"\"\"\n        from .seed_editor_qt import QTSeedEditor\n        from PyQt4.QtGui import QApplication\n\n        if min_val is None:\n            min_val = np.min(self.img)\n\n        if max_val is None:\n            max_val = np.max(self.img)\n\n        window_c = (max_val + min_val) / 2  # .astype(np.int16)\n        window_w = max_val - min_val  # .astype(np.int16)\n\n        if qt_app is None:\n            qt_app = QApplication(sys.argv)\n\n        pyed = QTSeedEditor(\n            self.img,\n            modeFun=self.interactivity_loop,\n            voxelSize=self.voxelsize,\n            seeds=self.seeds,\n            volume_unit=self.volume_unit,\n        )\n\n        pyed.changeC(window_c)\n        pyed.changeW(window_w)\n\n        qt_app.exec_()"}
{"func_code_string": "def set_seeds(self, seeds):\n        \"\"\"\n        Function for manual seed setting. Sets variable seeds and prepares\n        voxels for density model.\n        :param seeds: ndarray (0 - nothing, 1 - object, 2 - background,\n        3 - object just hard constraints, no model training, 4 - background \n        just hard constraints, no model training)\n        \"\"\"\n        if self.img.shape != seeds.shape:\n            raise Exception(\"Seeds must be same size as input image\")\n\n        self.seeds = seeds.astype(\"int8\")\n        self.voxels1 = self.img[self.seeds == 1]\n        self.voxels2 = self.img[self.seeds == 2]"}
{"func_code_string": "def run(self, run_fit_model=True):\n        \"\"\"\n        Run the Graph Cut segmentation according to preset parameters.\n\n        :param run_fit_model: Allow to skip model fit when the model is prepared before\n        :return:\n        \"\"\"\n\n        if run_fit_model:\n            self.fit_model(self.img, self.voxelsize, self.seeds)\n\n        self._start_time = time.time()\n        if self.segparams[\"method\"].lower() in (\"graphcut\", \"gc\"):\n            self.__single_scale_gc_run()\n        elif self.segparams[\"method\"].lower() in (\n            \"multiscale_graphcut\",\n            \"multiscale_gc\",\n            \"msgc\",\n            \"msgc_lo2hi\",\n            \"lo2hi\",\n            \"multiscale_graphcut_lo2hi\",\n        ):\n            logger.debug(\"performing multiscale Graph-Cut lo2hi\")\n            self.__multiscale_gc_lo2hi_run()\n        elif self.segparams[\"method\"].lower() in (\n            \"msgc_hi2lo\",\n            \"hi2lo\",\n            \"multiscale_graphcut_hi2lo\",\n        ):\n            logger.debug(\"performing multiscale Graph-Cut hi2lo\")\n            self.__multiscale_gc_hi2lo_run()\n        else:\n            logger.error(\"Unknown segmentation method: \" + self.segparams[\"method\"])"}
{"func_code_string": "def __set_hard_hard_constraints(self, tdata1, tdata2, seeds):\n        \"\"\"\n        it works with seed labels:\n        0: nothing\n        1: object 1 - full seeds\n        2: object 2 - full seeds\n        3: object 1 - not a training seeds\n        4: object 2 - not a training seeds\n        \"\"\"\n        seeds_mask = (seeds == 1) | (seeds == 3)\n        tdata2[seeds_mask] = np.max(tdata2) + 1\n        tdata1[seeds_mask] = 0\n\n        seeds_mask = (seeds == 2) | (seeds == 4)\n        tdata1[seeds_mask] = np.max(tdata1) + 1\n        tdata2[seeds_mask] = 0\n\n        return tdata1, tdata2"}
{"func_code_string": "def __similarity_for_tlinks_obj_bgr(\n        self,\n        data,\n        voxelsize,\n        # voxels1, voxels2,\n        # seeds, otherfeatures=None\n    ):\n        \"\"\"\n        Compute edge values for graph cut tlinks based on image intensity\n        and texture.\n        \"\"\"\n        # self.fit_model(data, voxelsize, seeds)\n        # There is a need to have small vaues for good fit\n        # R(obj) = -ln( Pr (Ip | O) )\n        # R(bck) = -ln( Pr (Ip | B) )\n        # Boykov2001b\n        # ln is computed in likelihood\n        tdata1 = (-(self.mdl.likelihood_from_image(data, voxelsize, 1))) * 10\n        tdata2 = (-(self.mdl.likelihood_from_image(data, voxelsize, 2))) * 10\n\n        # to spare some memory\n        dtype = np.int16\n        if np.any(tdata1 > 32760):\n            dtype = np.float32\n        if np.any(tdata2 > 32760):\n            dtype = np.float32\n\n        if self.segparams[\"use_apriori_if_available\"] and self.apriori is not None:\n            logger.debug(\"using apriori information\")\n            gamma = self.segparams[\"apriori_gamma\"]\n            a1 = (-np.log(self.apriori * 0.998 + 0.001)) * 10\n            a2 = (-np.log(0.999 - (self.apriori * 0.998))) * 10\n            # logger.debug('max ' + str(np.max(tdata1)) + ' min ' + str(np.min(tdata1)))\n            # logger.debug('max ' + str(np.max(tdata2)) + ' min ' + str(np.min(tdata2)))\n            # logger.debug('max ' + str(np.max(a1)) + ' min ' + str(np.min(a1)))\n            # logger.debug('max ' + str(np.max(a2)) + ' min ' + str(np.min(a2)))\n            tdata1u = (((1 - gamma) * tdata1) + (gamma * a1)).astype(dtype)\n            tdata2u = (((1 - gamma) * tdata2) + (gamma * a2)).astype(dtype)\n            tdata1 = tdata1u\n            tdata2 = tdata2u\n            # logger.debug('   max ' + str(np.max(tdata1)) + ' min ' + str(np.min(tdata1)))\n            # logger.debug('   max ' + str(np.max(tdata2)) + ' min ' + str(np.min(tdata2)))\n            # logger.debug('gamma ' + str(gamma))\n\n            # import sed3\n            # ed = sed3.show_slices(tdata1)\n            # ed = sed3.show_slices(tdata2)\n            del tdata1u\n            del tdata2u\n            del a1\n            del a2\n\n        # if np.any(tdata1 < 0) or np.any(tdata2 <0):\n        #     logger.error(\"Problem with tlinks. Likelihood is < 0\")\n\n        # if self.debug_images:\n        #     self.__show_debug_tdata_images(tdata1, tdata2, suptitle=\"likelihood\")\n        return tdata1, tdata2"}
{"func_code_string": "def __create_nlinks(self, data, inds=None, boundary_penalties_fcn=None):\n        \"\"\"\n        Compute nlinks grid from data shape information. For boundary penalties\n        are data (intensities) values are used.\n\n        ins: Default is None. Used for multiscale GC. This are indexes of\n        multiscale pixels. Next example shows one superpixel witn index 2.\n        inds = [\n            [1 2 2],\n            [3 2 2],\n            [4 5 6]]\n\n        boundary_penalties_fcn: is function with one argument - axis. It can\n            it can be used for setting penalty weights between neighbooring\n            pixels.\n\n        \"\"\"\n        # use the gerneral graph algorithm\n        # first, we construct the grid graph\n        start = time.time()\n        if inds is None:\n            inds = np.arange(data.size).reshape(data.shape)\n        # if not self.segparams['use_boundary_penalties'] and \\\n        #         boundary_penalties_fcn is None :\n        if boundary_penalties_fcn is None:\n            # This is faster for some specific format\n            edgx = np.c_[inds[:, :, :-1].ravel(), inds[:, :, 1:].ravel()]\n            edgy = np.c_[inds[:, :-1, :].ravel(), inds[:, 1:, :].ravel()]\n            edgz = np.c_[inds[:-1, :, :].ravel(), inds[1:, :, :].ravel()]\n\n        else:\n            logger.info(\"use_boundary_penalties\")\n\n            bpw = self.segparams[\"boundary_penalties_weight\"]\n\n            bpa = boundary_penalties_fcn(2)\n            # id1=inds[:, :, :-1].ravel()\n            edgx = np.c_[\n                inds[:, :, :-1].ravel(),\n                inds[:, :, 1:].ravel(),\n                # cc * np.ones(id1.shape)\n                bpw * bpa[:, :, 1:].ravel(),\n            ]\n\n            bpa = boundary_penalties_fcn(1)\n            # id1 =inds[:, 1:, :].ravel()\n            edgy = np.c_[\n                inds[:, :-1, :].ravel(),\n                inds[:, 1:, :].ravel(),\n                # cc * np.ones(id1.shape)]\n                bpw * bpa[:, 1:, :].ravel(),\n            ]\n\n            bpa = boundary_penalties_fcn(0)\n            # id1 = inds[1:, :, :].ravel()\n            edgz = np.c_[\n                inds[:-1, :, :].ravel(),\n                inds[1:, :, :].ravel(),\n                # cc * np.ones(id1.shape)]\n                bpw * bpa[1:, :, :].ravel(),\n            ]\n\n        # import pdb; pdb.set_trace()\n        edges = np.vstack([edgx, edgy, edgz]).astype(np.int32)\n        # edges - seznam indexu hran, kteres spolu sousedi\\\n        elapsed = time.time() - start\n        self.stats[\"_create_nlinks time\"] = elapsed\n        logger.info(\"__create nlinks time \" + str(elapsed))\n        return edges"}
{"func_code_string": "def debug_get_reconstructed_similarity(\n        self,\n        data3d=None,\n        voxelsize=None,\n        seeds=None,\n        area_weight=1,\n        hard_constraints=True,\n        return_unariesalt=False,\n    ):\n        \"\"\"\n        Use actual model to calculate similarity. If no input is given the last image is used.\n        :param data3d:\n        :param voxelsize:\n        :param seeds:\n        :param area_weight:\n        :param hard_constraints:\n        :param return_unariesalt:\n        :return:\n        \"\"\"\n        if data3d is None:\n            data3d = self.img\n        if voxelsize is None:\n            voxelsize = self.voxelsize\n        if seeds is None:\n            seeds = self.seeds\n\n        unariesalt = self.__create_tlinks(\n            data3d,\n            voxelsize,\n            # voxels1, voxels2,\n            seeds,\n            area_weight,\n            hard_constraints,\n        )\n        if return_unariesalt:\n            return unariesalt\n        else:\n            return self._reshape_unariesalt_to_similarity(unariesalt, data3d.shape)"}
{"func_code_string": "def debug_show_reconstructed_similarity(\n        self,\n        data3d=None,\n        voxelsize=None,\n        seeds=None,\n        area_weight=1,\n        hard_constraints=True,\n        show=True,\n        bins=20,\n        slice_number=None,\n    ):\n        \"\"\"\n        Show tlinks.\n        :param data3d: ndarray with input data\n        :param voxelsize:\n        :param seeds:\n        :param area_weight:\n        :param hard_constraints:\n        :param show:\n        :param bins: histogram bins number\n        :param slice_number:\n        :return:\n        \"\"\"\n\n        unariesalt = self.debug_get_reconstructed_similarity(\n            data3d,\n            voxelsize=voxelsize,\n            seeds=seeds,\n            area_weight=area_weight,\n            hard_constraints=hard_constraints,\n            return_unariesalt=True,\n        )\n\n        self._debug_show_unariesalt(\n            unariesalt, show=show, bins=bins, slice_number=slice_number\n        )"}
{"func_code_string": "def debug_inspect_node(self, node_msindex):\n        \"\"\"\n        Get info about the node. See pycut.inspect_node() for details.\n        Processing is done in temporary shape.\n\n        :param node_seed:\n        :return: node_unariesalt, node_neighboor_edges_and_weights, node_neighboor_seeds\n        \"\"\"\n        return inspect_node(self.nlinks, self.unariesalt2, self.msinds, node_msindex)"}
{"func_code_string": "def debug_interactive_inspect_node(self):\n        \"\"\"\n        Call after segmentation to see selected node neighborhood.\n        User have to select one node by click.\n        :return:\n        \"\"\"\n        if (\n            np.sum(\n                np.abs(\n                    np.asarray(self.msinds.shape) - np.asarray(self.segmentation.shape)\n                )\n            )\n            == 0\n        ):\n            segmentation = self.segmentation\n        else:\n            segmentation = self.temp_msgc_resized_segmentation\n\n        logger.info(\"Click to select one voxel of interest\")\n        import sed3\n\n        ed = sed3.sed3(self.msinds, contour=segmentation == 0)\n        ed.show()\n        edseeds = ed.seeds\n        node_msindex = get_node_msindex(self.msinds, edseeds)\n\n        node_unariesalt, node_neighboor_edges_and_weights, node_neighboor_seeds = self.debug_inspect_node(\n            node_msindex\n        )\n        import sed3\n\n        ed = sed3.sed3(\n            self.msinds, contour=segmentation == 0, seeds=node_neighboor_seeds\n        )\n        ed.show()\n\n        return (\n            node_unariesalt,\n            node_neighboor_edges_and_weights,\n            node_neighboor_seeds,\n            node_msindex,\n        )"}
{"func_code_string": "def _ssgc_prepare_data_and_run_computation(\n        self,\n        # voxels1, voxels2,\n        hard_constraints=True,\n        area_weight=1,\n    ):\n        \"\"\"\n        Setting of data.\n        You need set seeds if you want use hard_constraints.\n        \"\"\"\n        # from PyQt4.QtCore import pyqtRemoveInputHook\n        # pyqtRemoveInputHook()\n        # import pdb; pdb.set_trace() # BREAKPOINT\n\n        unariesalt = self.__create_tlinks(\n            self.img,\n            self.voxelsize,\n            # voxels1, voxels2,\n            self.seeds,\n            area_weight,\n            hard_constraints,\n        )\n        #  n\u011bkter\u00e9m testu  organ semgmentation dosahuj\u00ed unaries -15. co\u017e je podin\u00e9\n        # sta\u010d\u00ed vyhodit print p\u0159ed if a je to vid\u011bt\n        logger.debug(\"unaries %.3g , %.3g\" % (np.max(unariesalt), np.min(unariesalt)))\n        # create potts pairwise\n        # pairwiseAlpha = -10\n        pairwise = -(np.eye(2) - 1)\n        pairwise = (self.segparams[\"pairwise_alpha\"] * pairwise).astype(np.int32)\n        # pairwise = np.array([[0,30],[30,0]]).astype(np.int32)\n        # print pairwise\n\n        self.iparams = {}\n\n        if self.segparams[\"use_boundary_penalties\"]:\n            sigma = self.segparams[\"boundary_penalties_sigma\"]\n            # set boundary penalties function\n            # Default are penalties based on intensity differences\n            boundary_penalties_fcn = lambda ax: self._boundary_penalties_array(\n                axis=ax, sigma=sigma\n            )\n        else:\n            boundary_penalties_fcn = None\n        nlinks = self.__create_nlinks(\n            self.img, boundary_penalties_fcn=boundary_penalties_fcn\n        )\n\n        self.stats[\"tlinks shape\"].append(unariesalt.reshape(-1, 2).shape)\n        self.stats[\"nlinks shape\"].append(nlinks.shape)\n        # we flatten the unaries\n        # result_graph = cut_from_graph(nlinks, unaries.reshape(-1, 2),\n        # pairwise)\n        start = time.time()\n        if self.debug_images:\n            self._debug_show_unariesalt(unariesalt)\n        result_graph = pygco.cut_from_graph(nlinks, unariesalt.reshape(-1, 2), pairwise)\n        elapsed = time.time() - start\n        self.stats[\"gc time\"] = elapsed\n        result_labeling = result_graph.reshape(self.img.shape)\n\n        return result_labeling"}
{"func_code_string": "def resize_to_shape(data, shape, zoom=None, mode=\"nearest\", order=0):\n    \"\"\"\n    Function resize input data to specific shape.\n    :param data: input 3d array-like data\n    :param shape: shape of output data\n    :param zoom: zoom is used for back compatibility\n    :mode: default is 'nearest'\n    \"\"\"\n    # @TODO remove old code in except part\n    # TODO use function from library in future\n\n    try:\n        # rint 'pred vyjimkou'\n        # aise Exception ('test without skimage')\n        # rint 'za vyjimkou'\n        import skimage\n        import skimage.transform\n\n        # Now we need reshape  seeds and segmentation to original size\n\n        # with warnings.catch_warnings():\n        #     warnings.filterwarnings(\"ignore\", \".*'constant', will be changed to.*\")\n        segm_orig_scale = skimage.transform.resize(\n            data, shape, order=0, preserve_range=True, mode=\"reflect\"\n        )\n\n        segmentation = segm_orig_scale\n        logger.debug(\"resize to orig with skimage\")\n    except:\n        if zoom is None:\n            zoom = shape / np.asarray(data.shape).astype(np.double)\n        segmentation = resize_to_shape_with_zoom(\n            data, zoom=zoom, mode=mode, order=order\n        )\n\n    return segmentation"}
{"func_code_string": "def seed_zoom(seeds, zoom):\n    \"\"\"\n    Smart zoom for sparse matrix. If there is resize to bigger resolution\n    thin line of label could be lost. This function prefers labels larger\n    then zero. If there is only one small voxel in larger volume with zeros\n    it is selected.\n    \"\"\"\n    # import scipy\n    # loseeds=seeds\n    labels = np.unique(seeds)\n    # remove first label - 0\n    labels = np.delete(labels, 0)\n    # @TODO smart interpolation for seeds in one block\n    #        loseeds = scipy.ndimage.interpolation.zoom(\n    #            seeds, zoom, order=0)\n    loshape = np.ceil(np.array(seeds.shape) * 1.0 / zoom).astype(np.int)\n    loseeds = np.zeros(loshape, dtype=np.int8)\n    loseeds = loseeds.astype(np.int8)\n    for label in labels:\n        a, b, c = np.where(seeds == label)\n        loa = np.round(a // zoom)\n        lob = np.round(b // zoom)\n        loc = np.round(c // zoom)\n        # loseeds = np.zeros(loshape)\n\n        loseeds[loa, lob, loc] += label\n        # this is to detect conflict seeds\n        loseeds[loseeds > label] = 100\n\n    # remove conflict seeds\n    loseeds[loseeds > 99] = 0\n\n    # import py3DSeedEditor\n    # ped = py3DSeedEditor.py3DSeedEditor(loseeds)\n    # ped.show()\n\n    return loseeds"}
{"func_code_string": "def zoom_to_shape(data, shape, dtype=None):\n    \"\"\"\n    Zoom data to specific shape.\n    \"\"\"\n    import scipy\n    import scipy.ndimage\n\n    zoomd = np.array(shape) / np.array(data.shape, dtype=np.double)\n    import warnings\n\n    datares = scipy.ndimage.interpolation.zoom(data, zoomd, order=0, mode=\"reflect\")\n\n    if datares.shape != shape:\n        logger.warning(\"Zoom with different output shape\")\n    dataout = np.zeros(shape, dtype=dtype)\n    shpmin = np.minimum(dataout.shape, shape)\n\n    dataout[: shpmin[0], : shpmin[1], : shpmin[2]] = datares[\n        : shpmin[0], : shpmin[1], : shpmin[2]\n    ]\n    return datares"}
{"func_code_string": "def crop(data, crinfo):\n    \"\"\"\n    Crop the data.\n\n    crop(data, crinfo)\n\n    :param crinfo: min and max for each axis - [[minX, maxX], [minY, maxY], [minZ, maxZ]]\n\n    \"\"\"\n    crinfo = fix_crinfo(crinfo)\n    return data[\n        __int_or_none(crinfo[0][0]) : __int_or_none(crinfo[0][1]),\n        __int_or_none(crinfo[1][0]) : __int_or_none(crinfo[1][1]),\n        __int_or_none(crinfo[2][0]) : __int_or_none(crinfo[2][1]),\n    ]"}
{"func_code_string": "def combinecrinfo(crinfo1, crinfo2):\n    \"\"\"\n    Combine two crinfos. First used is crinfo1, second used is crinfo2.\n    \"\"\"\n    crinfo1 = fix_crinfo(crinfo1)\n    crinfo2 = fix_crinfo(crinfo2)\n\n    crinfo = [\n        [crinfo1[0][0] + crinfo2[0][0], crinfo1[0][0] + crinfo2[0][1]],\n        [crinfo1[1][0] + crinfo2[1][0], crinfo1[1][0] + crinfo2[1][1]],\n        [crinfo1[2][0] + crinfo2[2][0], crinfo1[2][0] + crinfo2[2][1]],\n    ]\n\n    return crinfo"}
{"func_code_string": "def crinfo_from_specific_data(data, margin=0):\n    \"\"\"\n    Create crinfo of minimum orthogonal nonzero block in input data.\n\n    :param data: input data\n    :param margin: add margin to minimum block\n    :return:\n    \"\"\"\n    # hled\u00e1me automatick\u00fd o\u0159ez, nonzero d\u00e1 indexy\n    logger.debug(\"crinfo\")\n    logger.debug(str(margin))\n    nzi = np.nonzero(data)\n    logger.debug(str(nzi))\n\n    if np.isscalar(margin):\n        margin = [margin] * 3\n\n    x1 = np.min(nzi[0]) - margin[0]\n    x2 = np.max(nzi[0]) + margin[0] + 1\n    y1 = np.min(nzi[1]) - margin[0]\n    y2 = np.max(nzi[1]) + margin[0] + 1\n    z1 = np.min(nzi[2]) - margin[0]\n    z2 = np.max(nzi[2]) + margin[0] + 1\n\n    # o\u0161et\u0159en\u00ed mez\u00ed pol\u00ed\n    if x1 < 0:\n        x1 = 0\n    if y1 < 0:\n        y1 = 0\n    if z1 < 0:\n        z1 = 0\n\n    if x2 > data.shape[0]:\n        x2 = data.shape[0] - 1\n    if y2 > data.shape[1]:\n        y2 = data.shape[1] - 1\n    if z2 > data.shape[2]:\n        z2 = data.shape[2] - 1\n\n    # o\u0159ez\n    crinfo = [[x1, x2], [y1, y2], [z1, z2]]\n    return crinfo"}
{"func_code_string": "def uncrop(data, crinfo, orig_shape, resize=False, outside_mode=\"constant\", cval=0):\n    \"\"\"\n    Put some boundary to input image.\n\n\n    :param data: input data\n    :param crinfo: array with minimum and maximum index along each axis\n        [[minX, maxX],[minY, maxY],[minZ, maxZ]]. If crinfo is None, the whole input image is placed into [0, 0, 0].\n        If crinfo is just series of three numbers, it is used as an initial point for input image placement.\n    :param orig_shape: shape of uncropped image\n    :param resize: True or False (default). Usefull if the data.shape does not fit to crinfo shape.\n    :param outside_mode: 'constant', 'nearest'\n    :return:\n    \"\"\"\n\n    if crinfo is None:\n        crinfo = list(zip([0] * data.ndim, orig_shape))\n    elif np.asarray(crinfo).size == data.ndim:\n        crinfo = list(zip(crinfo, np.asarray(crinfo) + data.shape))\n\n    crinfo = fix_crinfo(crinfo)\n    data_out = np.ones(orig_shape, dtype=data.dtype) * cval\n\n    # print 'uncrop ', crinfo\n    # print orig_shape\n    # print data.shape\n    if resize:\n        data = resize_to_shape(data, crinfo[:, 1] - crinfo[:, 0])\n\n    startx = np.round(crinfo[0][0]).astype(int)\n    starty = np.round(crinfo[1][0]).astype(int)\n    startz = np.round(crinfo[2][0]).astype(int)\n\n    data_out[\n        # np.round(crinfo[0][0]).astype(int):np.round(crinfo[0][1]).astype(int)+1,\n        # np.round(crinfo[1][0]).astype(int):np.round(crinfo[1][1]).astype(int)+1,\n        # np.round(crinfo[2][0]).astype(int):np.round(crinfo[2][1]).astype(int)+1\n        startx : startx + data.shape[0],\n        starty : starty + data.shape[1],\n        startz : startz + data.shape[2],\n    ] = data\n\n    if outside_mode == \"nearest\":\n        # for ax in range(data.ndims):\n        # ax = 0\n\n        # copy border slice to pixels out of boundary - the higher part\n        for ax in range(data.ndim):\n            # the part under the crop\n            start = np.round(crinfo[ax][0]).astype(int)\n            slices = [slice(None), slice(None), slice(None)]\n            slices[ax] = start\n            repeated_slice = np.expand_dims(data_out[slices], ax)\n            append_sz = start\n            if append_sz > 0:\n                tile0 = np.repeat(repeated_slice, append_sz, axis=ax)\n                slices = [slice(None), slice(None), slice(None)]\n                slices[ax] = slice(None, start)\n                # data_out[start + data.shape[ax] : , :, :] = tile0\n                data_out[slices] = tile0\n                # plt.imshow(np.squeeze(repeated_slice))\n                # plt.show()\n\n            # the part over the crop\n            start = np.round(crinfo[ax][0]).astype(int)\n            slices = [slice(None), slice(None), slice(None)]\n            slices[ax] = start + data.shape[ax] - 1\n            repeated_slice = np.expand_dims(data_out[slices], ax)\n            append_sz = data_out.shape[ax] - (start + data.shape[ax])\n            if append_sz > 0:\n                tile0 = np.repeat(repeated_slice, append_sz, axis=ax)\n                slices = [slice(None), slice(None), slice(None)]\n                slices[ax] = slice(start + data.shape[ax], None)\n                # data_out[start + data.shape[ax] : , :, :] = tile0\n                data_out[slices] = tile0\n                # plt.imshow(np.squeeze(repeated_slice))\n                # plt.show()\n\n    return data_out"}
{"func_code_string": "def fix_crinfo(crinfo, to=\"axis\"):\n    \"\"\"\n    Function recognize order of crinfo and convert it to proper format.\n    \"\"\"\n\n    crinfo = np.asarray(crinfo)\n    if crinfo.shape[0] == 2:\n        crinfo = crinfo.T\n\n    return crinfo"}
{"func_code_string": "def grid_edges(shape, inds=None, return_directions=True):\n    \"\"\"\n    Get list of grid edges\n    :param shape:\n    :param inds:\n    :param return_directions:\n    :return:\n    \"\"\"\n    if inds is None:\n        inds = np.arange(np.prod(shape)).reshape(shape)\n    # if not self.segparams['use_boundary_penalties'] and \\\n    #         boundary_penalties_fcn is None :\n    if len(shape) == 2:\n        edgx = np.c_[inds[:, :-1].ravel(), inds[:, 1:].ravel()]\n        edgy = np.c_[inds[:-1, :].ravel(), inds[1:, :].ravel()]\n\n        edges = [edgx, edgy]\n\n        directions = [\n            np.ones([edgx.shape[0]], dtype=np.int8) * 0,\n            np.ones([edgy.shape[0]], dtype=np.int8) * 1,\n        ]\n\n    elif len(shape) == 3:\n        # This is faster for some specific format\n        edgx = np.c_[inds[:, :, :-1].ravel(), inds[:, :, 1:].ravel()]\n        edgy = np.c_[inds[:, :-1, :].ravel(), inds[:, 1:, :].ravel()]\n        edgz = np.c_[inds[:-1, :, :].ravel(), inds[1:, :, :].ravel()]\n        edges = [edgx, edgy, edgz]\n    else:\n        logger.error(\"Expected 2D or 3D data\")\n\n    # for all edges along first direction put 0, for second direction put 1, for third direction put 3\n    if return_directions:\n        directions = []\n        for idirection in range(len(shape)):\n            directions.append(\n                np.ones([edges[idirection].shape[0]], dtype=np.int8) * idirection\n            )\n    edges = np.concatenate(edges)\n    if return_directions:\n        edge_dir = np.concatenate(directions)\n        return edges, edge_dir\n    else:\n        return edges"}
{"func_code_string": "def gen_grid_2d(shape, voxelsize):\n    \"\"\"\n    Generate list of edges for a base grid.\n    \"\"\"\n    nr, nc = shape\n    nrm1, ncm1 = nr - 1, nc - 1\n    # sh = nm.asarray(shape)\n    # calculate number of edges, in 2D: (nrows * (ncols - 1)) + ((nrows - 1) * ncols)\n    nedges = 0\n    for direction in range(len(shape)):\n        sh = copy.copy(list(shape))\n        sh[direction] += -1\n        nedges += nm.prod(sh)\n\n    nedges_old = ncm1 * nr + nrm1 * nc\n    edges = nm.zeros((nedges, 2), dtype=nm.int16)\n    edge_dir = nm.zeros((ncm1 * nr + nrm1 * nc,), dtype=nm.bool)\n    nodes = nm.zeros((nm.prod(shape), 3), dtype=nm.float32)\n\n    # edges\n    idx = 0\n    row = nm.zeros((ncm1, 2), dtype=nm.int16)\n    row[:, 0] = nm.arange(ncm1)\n    row[:, 1] = nm.arange(ncm1) + 1\n    for ii in range(nr):\n        edges[slice(idx, idx + ncm1), :] = row + nc * ii\n        idx += ncm1\n\n    edge_dir[slice(0, idx)] = 0  # horizontal dir\n\n    idx0 = idx\n    col = nm.zeros((nrm1, 2), dtype=nm.int16)\n    col[:, 0] = nm.arange(nrm1) * nc\n    col[:, 1] = nm.arange(nrm1) * nc + nc\n    for ii in range(nc):\n        edges[slice(idx, idx + nrm1), :] = col + ii\n        idx += nrm1\n\n    edge_dir[slice(idx0, idx)] = 1  # vertical dir\n\n    # nodes\n    idx = 0\n    row = nm.zeros((nc, 3), dtype=nm.float32)\n    row[:, 0] = voxelsize[0] * (nm.arange(nc) + 0.5)\n    row[:, 1] = voxelsize[1] * 0.5\n    for ii in range(nr):\n        nodes[slice(idx, idx + nc), :] = row\n        row[:, 1] += voxelsize[1]\n        idx += nc\n\n    return nodes, edges, edge_dir"}
{"func_code_string": "def write_grid_to_vtk(fname, nodes, edges, node_flag=None, edge_flag=None):\n    \"\"\"\n    Write nodes and edges to VTK file\n    :param fname: VTK filename\n    :param nodes:\n    :param edges:\n    :param node_flag: set if this node is really used in output\n    :param edge_flag: set if this flag is used in output\n    :return:\n    \"\"\"\n\n    if node_flag is None:\n        node_flag = np.ones([nodes.shape[0]], dtype=np.bool)\n    if edge_flag is None:\n        edge_flag = np.ones([edges.shape[0]], dtype=np.bool)\n    nodes = make_nodes_3d(nodes)\n    f = open(fname, \"w\")\n\n    f.write(\"# vtk DataFile Version 2.6\\n\")\n    f.write(\"output file\\nASCII\\nDATASET UNSTRUCTURED_GRID\\n\")\n\n    idxs = nm.where(node_flag > 0)[0]\n    nnd = len(idxs)\n    aux = -nm.ones(node_flag.shape, dtype=nm.int32)\n    aux[idxs] = nm.arange(nnd, dtype=nm.int32)\n    f.write(\"\\nPOINTS %d float\\n\" % nnd)\n    for ndi in idxs:\n        f.write(\"%.6f %.6f %.6f\\n\" % tuple(nodes[ndi, :]))\n\n    idxs = nm.where(edge_flag > 0)[0]\n    ned = len(idxs)\n    f.write(\"\\nCELLS %d %d\\n\" % (ned, ned * 3))\n    for edi in idxs:\n        f.write(\"2 %d %d\\n\" % tuple(aux[edges[edi, :]]))\n\n    f.write(\"\\nCELL_TYPES %d\\n\" % ned)\n    for edi in idxs:\n        f.write(\"3\\n\")"}
{"func_code_string": "def add_nodes(self, coors, node_low_or_high=None):\n        \"\"\"\n        Add new nodes at the end of the list.\n        \"\"\"\n        last = self.lastnode\n        if type(coors) is nm.ndarray:\n            if len(coors.shape) == 1:\n                coors = coors.reshape((1, coors.size))\n\n            nadd = coors.shape[0]\n            idx = slice(last, last + nadd)\n        else:\n            nadd = 1\n            idx = self.lastnode\n        right_dimension = coors.shape[1]\n        self.nodes[idx, :right_dimension] = coors\n        self.node_flag[idx] = True\n        self.lastnode += nadd\n        self.nnodes += nadd"}
{"func_code_string": "def add_edges(self, conn, edge_direction, edge_group=None, edge_low_or_high=None):\n        \"\"\"\n        Add new edges at the end of the list.\n        :param edge_direction: direction flag\n        :param edge_group: describes group of edges from same low super node and same direction\n        :param edge_low_or_high: zero for low to low resolution, one for high to high or high to low resolution.\n        It is used to set weight from weight table.\n        \"\"\"\n        last = self.lastedge\n        if type(conn) is nm.ndarray:\n            nadd = conn.shape[0]\n            idx = slice(last, last + nadd)\n            if edge_group is None:\n                edge_group = nm.arange(nadd) + last\n        else:\n            nadd = 1\n            idx = nm.array([last])\n            conn = nm.array(conn).reshape((1, 2))\n            if edge_group is None:\n                edge_group = idx\n\n        self.edges[idx, :] = conn\n        self.edge_flag[idx] = True\n        # t_start0 = time.time()\n        # self.edge_flag_idx.extend(list(range(idx.start, idx.stop)))\n        # self.stats[\"t split 082\"] += time.time() - t_start0\n        self.edge_dir[idx] = edge_direction\n        self.edge_group[idx] = edge_group\n        # TODO change this just to array of low_or_high_resolution\n        if edge_low_or_high is not None and self._edge_weight_table is not None:\n            self.edges_weights[idx] = self._edge_weight_table[\n                edge_low_or_high, edge_direction\n            ]\n        self.lastedge += nadd\n        self.nedges += nadd"}
{"func_code_string": "def _edge_group_substitution(\n        self, ndid, nsplit, idxs, sr_tab, ndoffset, ed_remove, into_or_from\n    ):\n        \"\"\"\n        Reconnect edges.\n        :param ndid: id of low resolution edges\n        :param nsplit: number of split\n        :param idxs: indexes of low resolution\n        :param sr_tab:\n        :param ndoffset:\n        :param ed_remove:\n        :param into_or_from: if zero, connection of input edges is done. If one, connection of output edges\n        is performed.\n        :return:\n        \"\"\"\n        # this is useful for type(idxs) == np.ndarray\n        eidxs = idxs[nm.where(self.edges[idxs, 1 - into_or_from] == ndid)[0]]\n        # selected_edges = self.edges[idxs, 1 - into_or_from]\n        # selected_edges == ndid\n        # whre = nm.where(self.edges[idxs, 1 - into_or_from] == ndid)\n        # whre0 = (nm.where(self.edges[idxs, 1 - into_or_from] == ndid) == ndid)[0]\n        # eidxs = [idxs[i] for i in idxs]\n        for igrp in self.edges_by_group(eidxs):\n            if igrp.shape[0] > 1:\n                # high resolution block to high resolution block\n                # all directions are the same\n                directions = self.edge_dir[igrp[0]]\n                edge_indexes = sr_tab[directions, :].T.flatten() + ndoffset\n                # debug code\n                # if len(igrp) != len(edge_indexes):\n                #     print(\"Problem \")\n                self.edges[igrp, 1] = edge_indexes\n                if self._edge_weight_table is not None:\n                    self.edges_weights[igrp] = self._edge_weight_table[1, directions]\n            else:\n                # low res block to hi res block, if into_or_from is set to 0\n                # hig res block to low res block, if into_or_from is set to 1\n                ed_remove.append(igrp[0])\n                # number of new edges is equal to number of pixels on one side of the box (in 2D and D too)\n                nnewed = np.power(nsplit, self.data.ndim - 1)\n                muleidxs = nm.tile(igrp, nnewed)\n                # copy the low-res edge multipletime\n                newed = self.edges[muleidxs, :]\n                neweddir = self.edge_dir[muleidxs]\n                local_node_ids = sr_tab[\n                    self.edge_dir[igrp] + self.data.ndim * into_or_from, :\n                ].T.flatten()\n                # first or second (the actual) node id is substitued by new node indexes\n                newed[:, 1 - into_or_from] = local_node_ids + ndoffset\n                if self._edge_weight_table is not None:\n                    self.add_edges(\n                        newed, neweddir, self.edge_group[igrp], edge_low_or_high=1\n                    )\n                else:\n                    self.add_edges(\n                        newed, neweddir, self.edge_group[igrp], edge_low_or_high=None\n                    )\n        return ed_remove"}
{"func_code_string": "def generate_base_grid(self, vtk_filename=None):\n        \"\"\"\n        Run first step of algorithm. Next step is split_voxels\n        :param vtk_filename:\n        :return:\n        \"\"\"\n        nd, ed, ed_dir = self.gen_grid_fcn(self.data.shape, self.voxelsize)\n        self.add_nodes(nd)\n        self.add_edges(ed, ed_dir, edge_low_or_high=0)\n\n        if vtk_filename is not None:\n            self.write_vtk(vtk_filename)"}
{"func_code_string": "def split_voxels(self, vtk_filename=None):\n        \"\"\"\n        Second step of algorithm\n        :return:()\n        \"\"\"\n        self.cache = {}\n        self.stats[\"t graph 10\"] = time.time() - self.start_time\n        self.msi = MultiscaleArray(self.data.shape, block_size=self.nsplit)\n\n        # old implementation\n        # idxs = nm.where(self.data)\n        # nr, nc = self.data.shape\n        # for k, (ir, ic) in enumerate(zip(*idxs)):\n        #     ndid = ic + ir * nc\n        #     self.split_voxel(ndid, self.nsplit)\n\n        # new_implementation\n        # for ndid in np.flatnonzero(self.data):\n        #     self.split_voxel(ndid, self.nsplit)\n\n        # even newer implementation\n        self.stats[\"t graph 11\"] = time.time() - self.start_time\n        for ndid, val in enumerate(self.data.ravel()):\n            t_split_start = time.time()\n            if val == 0:\n                if self.compute_msindex:\n                    self.msi.set_block_lowres(ndid, ndid)\n                self.stats[\"t graph low\"] += time.time() - t_split_start\n            else:\n                self.split_voxel(ndid)\n                self.stats[\"t graph high\"] += time.time() - t_split_start\n\n        self.stats[\"t graph 13\"] = time.time() - self.start_time\n        self.finish()\n        if vtk_filename is not None:\n            self.write_vtk(vtk_filename)\n        self.stats[\"t graph 14\"] = time.time() - self.start_time"}
{"func_code_string": "def mul_block(self, index, val):\n        \"\"\"Multiply values in block\"\"\"\n        self._prepare_cache_slice(index)\n        self.msinds[self.cache_slice] *= val"}
{"func_code_string": "def select_from_fv_by_seeds(fv, seeds, unique_cls):\n    \"\"\"\n    Tool to make simple feature functions take features from feature array by seeds.\n    :param fv: ndarray with lineariezed feature. It's shape is MxN, where M is number of image pixels and N is number\n    of features\n    :param seeds: ndarray with seeds. Does not to be linear.\n    :param unique_cls: number of used seeds clases. Like [1, 2]\n    :return: fv_selection, seeds_selection - selection from feature vector and selection from seeds\n    \"\"\"\n    logger.debug(\"seeds\" + str(seeds))\n    # fvlin = fv.reshape(-1, int(fv.size/seeds.size))\n    expected_shape = [seeds.size, int(fv.size/seeds.size)]\n    if fv.shape[0] != expected_shape[0] or fv.shape[1] != expected_shape[1]:\n        raise AssertionError(\"Wrong shape of input feature vector array fv\")\n    # sd = seeds.reshape(-1, 1)\n    selection = np.in1d(seeds, unique_cls)\n    fv_selection = fv[selection]\n    seeds_selection = seeds.flatten()[selection]\n    # sd = sd[]\n    return fv_selection, seeds_selection"}
{"func_code_string": "def return_fv_by_seeds(fv, seeds=None, unique_cls=None):\n    \"\"\"\n    Return features selected by seeds and unique_cls or selection from features and corresponding seed classes.\n\n    :param fv: ndarray with lineariezed feature. It's shape is MxN, where M is number of image pixels and N is number\n    of features\n    :param seeds: ndarray with seeds. Does not to be linear.\n    :param unique_cls: number of used seeds clases. Like [1, 2]\n    :return: fv, sd - selection from feature vector and selection from seeds or just fv for whole image\n    \"\"\"\n    if seeds is not None:\n        if unique_cls is not None:\n            return select_from_fv_by_seeds(fv, seeds, unique_cls)\n        else:\n            raise AssertionError(\"Input unique_cls has to be not None if seeds is not None.\")\n    else:\n        return fv"}
{"func_code_string": "def expand(self, expression):\n        \"\"\"Expands logical constructions.\"\"\"\n        self.logger.debug(\"expand : expression %s\", str(expression))\n        if not is_string(expression):\n            return expression\n\n        result = self._pattern.sub(lambda var: str(self._variables[var.group(1)]), expression)\n\n        result = result.strip()\n        self.logger.debug('expand : %s - result : %s', expression, result)\n\n        if is_number(result):\n            if result.isdigit():\n                self.logger.debug('     expand is integer !!!')\n                return int(result)\n            else:\n                self.logger.debug('     expand is float !!!')\n                return float(result)\n        return result"}
{"func_code_string": "def get_gutter_client(\n        alias='default',\n        cache=CLIENT_CACHE,\n        **kwargs\n):\n    \"\"\"\n    Creates gutter clients and memoizes them in a registry for future quick access.\n\n    Args:\n        alias (str or None): Name of the client. Used for caching.\n            If name is falsy then do not use the cache.\n        cache (dict): cache to store gutter managers in.\n        **kwargs: kwargs to be passed the Manger class.\n\n    Returns (Manager):\n        A gutter client.\n\n    \"\"\"\n    from gutter.client.models import Manager\n\n    if not alias:\n        return Manager(**kwargs)\n    elif alias not in cache:\n        cache[alias] = Manager(**kwargs)\n\n    return cache[alias]"}
{"func_code_string": "def _modulo(self, decimal_argument):\n        \"\"\"\n        The mod operator is prone to floating point errors, so use decimal.\n\n        101.1 % 100\n        >>> 1.0999999999999943\n\n        decimal_context.divmod(Decimal('100.1'), 100)\n        >>> (Decimal('1'), Decimal('0.1'))\n        \"\"\"\n        _times, remainder = self._context.divmod(decimal_argument, 100)\n\n        # match the builtin % behavior by adding the N to the result if negative\n        return remainder if remainder >= 0 else remainder + 100"}
{"func_code_string": "def enabled_for(self, inpt):\n        \"\"\"\n        Checks to see if this switch is enabled for the provided input.\n\n        If ``compounded``, all switch conditions must be ``True`` for the switch\n        to be enabled.  Otherwise, *any* condition needs to be ``True`` for the\n        switch to be enabled.\n\n        The switch state is then checked to see if it is ``GLOBAL`` or\n        ``DISABLED``.  If it is not, then the switch is ``SELECTIVE`` and each\n        condition is checked.\n\n        Keyword Arguments:\n        inpt -- An instance of the ``Input`` class.\n        \"\"\"\n\n        signals.switch_checked.call(self)\n        signal_decorated = partial(self.__signal_and_return, inpt)\n\n        if self.state is self.states.GLOBAL:\n            return signal_decorated(True)\n        elif self.state is self.states.DISABLED:\n            return signal_decorated(False)\n\n        conditions_dict = ConditionsDict.from_conditions_list(self.conditions)\n        conditions = conditions_dict.get_by_input(inpt)\n\n        if conditions:\n            result = self.__enabled_func(\n                cond.call(inpt)\n                for cond\n                in conditions\n                if cond.argument(inpt).applies\n            )\n        else:\n            result = None\n\n        return signal_decorated(result)"}
{"func_code_string": "def call(self, inpt):\n        \"\"\"\n        Returns if the condition applies to the ``inpt``.\n\n        If the class ``inpt`` is an instance of is not the same class as the\n        condition's own ``argument``, then ``False`` is returned.  This also\n        applies to the ``NONE`` input.\n\n        Otherwise, ``argument`` is called, with ``inpt`` as the instance and\n        the value is compared to the ``operator`` and the Value is returned.  If\n        the condition is ``negative``, then then ``not`` the value is returned.\n\n        Keyword Arguments:\n        inpt -- An instance of the ``Input`` class.\n        \"\"\"\n        if inpt is Manager.NONE_INPUT:\n            return False\n\n        # Call (construct) the argument with the input object\n        argument_instance = self.argument(inpt)\n\n        if not argument_instance.applies:\n            return False\n\n        application = self.__apply(argument_instance, inpt)\n\n        if self.negative:\n            application = not application\n\n        return application"}
{"func_code_string": "def switches(self):\n        \"\"\"\n        List of all switches currently registered.\n        \"\"\"\n        results = [\n            switch for name, switch in self.storage.iteritems()\n            if name.startswith(self.__joined_namespace)\n        ]\n\n        return results"}
{"func_code_string": "def switch(self, name):\n        \"\"\"\n        Returns the switch with the provided ``name``.\n\n        If ``autocreate`` is set to ``True`` and no switch with that name\n        exists, a ``DISABLED`` switch will be with that name.\n\n        Keyword Arguments:\n        name -- A name of a switch.\n        \"\"\"\n        try:\n            switch = self.storage[self.__namespaced(name)]\n        except KeyError:\n            if not self.autocreate:\n                raise ValueError(\"No switch named '%s' registered in '%s'\" % (name, self.namespace))\n\n            switch = self.__create_and_register_disabled_switch(name)\n\n        switch.manager = self\n        return switch"}
{"func_code_string": "def register(self, switch, signal=signals.switch_registered):\n        '''\n        Register a switch and persist it to the storage.\n        '''\n        if not switch.name:\n            raise ValueError('Switch name cannot be blank')\n\n        switch.manager = self\n        self.__persist(switch)\n\n        signal.call(switch)"}
{"func_code_string": "def verify(obj, times=1, atleast=None, atmost=None, between=None,\n           inorder=False):\n    \"\"\"Central interface to verify interactions.\n\n    `verify` uses a fluent interface::\n\n        verify(<obj>, times=2).<method_name>(<args>)\n\n    `args` can be as concrete as necessary. Often a catch-all is enough,\n    especially if you're working with strict mocks, bc they throw at call\n    time on unwanted, unconfigured arguments::\n\n        from mockito import ANY, ARGS, KWARGS\n        when(manager).add_tasks(1, 2, 3)\n        ...\n        # no need to duplicate the specification; every other argument pattern\n        # would have raised anyway.\n        verify(manager).add_tasks(1, 2, 3)  # duplicates `when`call\n        verify(manager).add_tasks(*ARGS)\n        verify(manager).add_tasks(...)       # Py3\n        verify(manager).add_tasks(Ellipsis)  # Py2\n\n    \"\"\"\n\n    if isinstance(obj, str):\n        obj = get_obj(obj)\n\n    verification_fn = _get_wanted_verification(\n        times=times, atleast=atleast, atmost=atmost, between=between)\n    if inorder:\n        verification_fn = verification.InOrder(verification_fn)\n\n    # FIXME?: Catch error if obj is neither a Mock nor a known stubbed obj\n    theMock = _get_mock_or_raise(obj)\n\n    class Verify(object):\n        def __getattr__(self, method_name):\n            return invocation.VerifiableInvocation(\n                theMock, method_name, verification_fn)\n\n    return Verify()"}
{"func_code_string": "def when(obj, strict=None):\n    \"\"\"Central interface to stub functions on a given `obj`\n\n    `obj` should be a module, a class or an instance of a class; it can be\n    a Dummy you created with :func:`mock`. ``when`` exposes a fluent interface\n    where you configure a stub in three steps::\n\n        when(<obj>).<method_name>(<args>).thenReturn(<value>)\n\n    Compared to simple *patching*, stubbing in mockito requires you to specify\n    conrete `args` for which the stub will answer with a concrete `<value>`.\n    All invocations that do not match this specific call signature will be\n    rejected. They usually throw at call time.\n\n    Stubbing in mockito's sense thus means not only to get rid of unwanted\n    side effects, but effectively to turn function calls into constants.\n\n    E.g.::\n\n        # Given ``dog`` is an instance of a ``Dog``\n        when(dog).bark('Grrr').thenReturn('Wuff')\n        when(dog).bark('Miau').thenRaise(TypeError())\n\n        # With this configuration set up:\n        assert dog.bark('Grrr') == 'Wuff'\n        dog.bark('Miau')  # will throw TypeError\n        dog.bark('Wuff')  # will throw unwanted interaction\n\n    Stubbing can effectively be used as monkeypatching; usage shown with\n    the `with` context managing::\n\n        with when(os.path).exists('/foo').thenReturn(True):\n            ...\n\n    Most of the time verifying your interactions is not necessary, because\n    your code under tests implicitly verifies the return value by evaluating\n    it. See :func:`verify` if you need to, see also :func:`expect` to setup\n    expected call counts up front.\n\n    If your function is pure side effect and does not return something, you\n    can omit the specific answer. The default then is `None`::\n\n        when(manager).do_work()\n\n    `when` verifies the method name, the expected argument signature, and the\n    actual, factual arguments your code under test uses against the original\n    object and its function so its easier to spot changing interfaces.\n\n    Sometimes it's tedious to spell out all arguments::\n\n        from mockito import ANY, ARGS, KWARGS\n        when(requests).get('http://example.com/', **KWARGS).thenReturn(...)\n        when(os.path).exists(ANY)\n        when(os.path).exists(ANY(str))\n\n    .. note:: You must :func:`unstub` after stubbing, or use `with`\n        statement.\n\n    Set ``strict=False`` to bypass the function signature checks.\n\n    See related :func:`when2` which has a more pythonic interface.\n\n    \"\"\"\n\n    if isinstance(obj, str):\n        obj = get_obj(obj)\n\n    if strict is None:\n        strict = True\n    theMock = _get_mock(obj, strict=strict)\n\n    class When(object):\n        def __getattr__(self, method_name):\n            return invocation.StubbedInvocation(\n                theMock, method_name, strict=strict)\n\n    return When()"}
{"func_code_string": "def when2(fn, *args, **kwargs):\n    \"\"\"Stub a function call with the given arguments\n\n    Exposes a more pythonic interface than :func:`when`. See :func:`when` for\n    more documentation.\n\n    Returns `AnswerSelector` interface which exposes `thenReturn`,\n    `thenRaise`, and `thenAnswer` as usual. Always `strict`.\n\n    Usage::\n\n        # Given `dog` is an instance of a `Dog`\n        when2(dog.bark, 'Miau').thenReturn('Wuff')\n\n    .. note:: You must :func:`unstub` after stubbing, or use `with`\n        statement.\n\n    \"\"\"\n    obj, name = get_obj_attr_tuple(fn)\n    theMock = _get_mock(obj, strict=True)\n    return invocation.StubbedInvocation(theMock, name)(*args, **kwargs)"}
{"func_code_string": "def patch(fn, attr_or_replacement, replacement=None):\n    \"\"\"Patch/Replace a function.\n\n    This is really like monkeypatching, but *note* that all interactions\n    will be recorded and can be verified. That is, using `patch` you stay in\n    the domain of mockito.\n\n    Two ways to call this. Either::\n\n        patch(os.path.exists, lambda str: True)  # two arguments\n        # OR\n        patch(os.path, 'exists', lambda str: True)  # three arguments\n\n    If called with three arguments, the mode is *not* strict to allow *adding*\n    methods. If called with two arguments, mode is always `strict`.\n\n    .. note:: You must :func:`unstub` after stubbing, or use `with`\n        statement.\n\n    \"\"\"\n    if replacement is None:\n        replacement = attr_or_replacement\n        return when2(fn, Ellipsis).thenAnswer(replacement)\n    else:\n        obj, name = fn, attr_or_replacement\n        theMock = _get_mock(obj, strict=True)\n        return invocation.StubbedInvocation(\n            theMock, name, strict=False)(Ellipsis).thenAnswer(replacement)"}
{"func_code_string": "def expect(obj, strict=None,\n           times=None, atleast=None, atmost=None, between=None):\n    \"\"\"Stub a function call, and set up an expected call count.\n\n    Usage::\n\n        # Given `dog` is an instance of a `Dog`\n        expect(dog, times=1).bark('Wuff').thenReturn('Miau')\n        dog.bark('Wuff')\n        dog.bark('Wuff')  # will throw at call time: too many invocations\n\n        # maybe if you need to ensure that `dog.bark()` was called at all\n        verifyNoUnwantedInteractions()\n\n    .. note:: You must :func:`unstub` after stubbing, or use `with`\n        statement.\n\n    See :func:`when`, :func:`when2`, :func:`verifyNoUnwantedInteractions`\n\n    \"\"\"\n    if strict is None:\n        strict = True\n    theMock = _get_mock(obj, strict=strict)\n\n    verification_fn = _get_wanted_verification(\n        times=times, atleast=atleast, atmost=atmost, between=between)\n\n    class Expect(object):\n        def __getattr__(self, method_name):\n            return invocation.StubbedInvocation(\n                theMock, method_name, verification=verification_fn,\n                strict=strict)\n\n    return Expect()"}
{"func_code_string": "def unstub(*objs):\n    \"\"\"Unstubs all stubbed methods and functions\n\n    If you don't pass in any argument, *all* registered mocks and\n    patched modules, classes etc. will be unstubbed.\n\n    Note that additionally, the underlying registry will be cleaned.\n    After an `unstub` you can't :func:`verify` anymore because all\n    interactions will be forgotten.\n    \"\"\"\n\n    if objs:\n        for obj in objs:\n            mock_registry.unstub(obj)\n    else:\n        mock_registry.unstub_all()"}
{"func_code_string": "def verifyZeroInteractions(*objs):\n    \"\"\"Verify that no methods have been called on given objs.\n\n    Note that strict mocks usually throw early on unexpected, unstubbed\n    invocations. Partial mocks ('monkeypatched' objects or modules) do not\n    support this functionality at all, bc only for the stubbed invocations\n    the actual usage gets recorded. So this function is of limited use,\n    nowadays.\n\n    \"\"\"\n    for obj in objs:\n        theMock = _get_mock_or_raise(obj)\n\n        if len(theMock.invocations) > 0:\n            raise VerificationError(\n                \"\\nUnwanted interaction: %s\" % theMock.invocations[0])"}
{"func_code_string": "def verifyNoUnwantedInteractions(*objs):\n    \"\"\"Verifies that expectations set via `expect` are met\n\n    E.g.::\n\n        expect(os.path, times=1).exists(...).thenReturn(True)\n        os.path('/foo')\n        verifyNoUnwantedInteractions(os.path)  # ok, called once\n\n    If you leave out the argument *all* registered objects will\n    be checked.\n\n    .. note:: **DANGERZONE**: If you did not :func:`unstub` correctly,\n        it is possible that old registered mocks, from other tests\n        leak.\n\n    See related :func:`expect`\n    \"\"\"\n\n    if objs:\n        theMocks = map(_get_mock_or_raise, objs)\n    else:\n        theMocks = mock_registry.get_registered_mocks()\n\n    for mock in theMocks:\n        for i in mock.stubbed_invocations:\n            i.verify()"}
{"func_code_string": "def verifyStubbedInvocationsAreUsed(*objs):\n    \"\"\"Ensure stubs are actually used.\n\n    This functions just ensures that stubbed methods are actually used. Its\n    purpose is to detect interface changes after refactorings. It is meant\n    to be invoked usually without arguments just before :func:`unstub`.\n\n    \"\"\"\n    if objs:\n        theMocks = map(_get_mock_or_raise, objs)\n    else:\n        theMocks = mock_registry.get_registered_mocks()\n\n\n    for mock in theMocks:\n        for i in mock.stubbed_invocations:\n            if not i.allow_zero_invocations and i.used < len(i.answers):\n                raise VerificationError(\"\\nUnused stub: %s\" % i)"}
{"func_code_string": "def get_function_host(fn):\n    \"\"\"Destructure a given function into its host and its name.\n\n    The 'host' of a function is a module, for methods it is usually its\n    instance or its class. This is safe only for methods, for module wide,\n    globally declared names it must be considered experimental.\n\n    For all reasonable fn: ``getattr(*get_function_host(fn)) == fn``\n\n    Returns tuple (host, fn-name)\n    Otherwise should raise TypeError\n    \"\"\"\n\n    obj = None\n    try:\n        name = fn.__name__\n        obj = fn.__self__\n    except AttributeError:\n        pass\n\n    if obj is None:\n        # Due to how python imports work, everything that is global on a module\n        # level must be regarded as not safe here. For now, we go for the extra\n        # mile, TBC, because just specifying `os.path.exists` would be 'cool'.\n        #\n        # TLDR;:\n        # E.g. `inspect.getmodule(os.path.exists)` returns `genericpath` bc\n        # that's where `exists` is defined and comes from. But from the point\n        # of view of the user `exists` always comes and is used from `os.path`\n        # which points e.g. to `ntpath`. We thus must patch `ntpath`.\n        # But that's the same for most imports::\n        #\n        #     # b.py\n        #     from a import foo\n        #\n        # Now asking `getmodule(b.foo)` it tells you `a`, but we access and use\n        # `b.foo` and we therefore must patch `b`.\n\n        obj, name = find_invoking_frame_and_try_parse()\n        # safety check!\n        assert getattr(obj, name) == fn\n\n\n    return obj, name"}
{"func_code_string": "def get_obj(path):\n    \"\"\"Return obj for given dotted path.\n\n    Typical inputs for `path` are 'os' or 'os.path' in which case you get a\n    module; or 'os.path.exists' in which case you get a function from that\n    module.\n\n    Just returns the given input in case it is not a str.\n\n    Note: Relative imports not supported.\n    Raises ImportError or AttributeError as appropriate.\n\n    \"\"\"\n    # Since we usually pass in mocks here; duck typing is not appropriate\n    # (mocks respond to every attribute).\n    if not isinstance(path, str):\n        return path\n\n    if path.startswith('.'):\n        raise TypeError('relative imports are not supported')\n\n    parts = path.split('.')\n    head, tail = parts[0], parts[1:]\n\n    obj = importlib.import_module(head)\n\n    # Normally a simple reduce, but we go the extra mile\n    # for good exception messages.\n    for i, name in enumerate(tail):\n        try:\n            obj = getattr(obj, name)\n        except AttributeError:\n            # Note the [:i] instead of [:i+1], so we get the path just\n            # *before* the AttributeError, t.i. the part of it that went ok.\n            module = '.'.join([head] + tail[:i])\n            try:\n                importlib.import_module(module)\n            except ImportError:\n                raise AttributeError(\n                    \"object '%s' has no attribute '%s'\" % (module, name))\n            else:\n                raise AttributeError(\n                    \"module '%s' has no attribute '%s'\" % (module, name))\n    return obj"}
{"func_code_string": "def get_obj_attr_tuple(path):\n    \"\"\"Split path into (obj, attribute) tuple.\n\n    Given `path` is 'os.path.exists' will thus return `(os.path, 'exists')`\n\n    If path is not a str, delegates to `get_function_host(path)`\n\n    \"\"\"\n    if not isinstance(path, str):\n        return get_function_host(path)\n\n    if path.startswith('.'):\n        raise TypeError('relative imports are not supported')\n\n    try:\n        leading, end = path.rsplit('.', 1)\n    except ValueError:\n        raise TypeError('path must have dots')\n\n    return get_obj(leading), end"}
{"func_code_string": "def spy(object):\n    \"\"\"Spy an object.\n\n    Spying means that all functions will behave as before, so they will\n    be side effects, but the interactions can be verified afterwards.\n\n    Returns Dummy-like, almost empty object as proxy to `object`.\n\n    The *returned* object must be injected and used by the code under test;\n    after that all interactions can be verified as usual.\n    T.i. the original object **will not be patched**, and has no further\n    knowledge as before.\n\n    E.g.::\n\n        import time\n        time = spy(time)\n        # inject time\n        do_work(..., time)\n        verify(time).time()\n\n    \"\"\"\n    if inspect.isclass(object) or inspect.ismodule(object):\n        class_ = None\n    else:\n        class_ = object.__class__\n\n    class Spy(_Dummy):\n        if class_:\n            __class__ = class_\n\n        def __getattr__(self, method_name):\n            return RememberedProxyInvocation(theMock, method_name)\n\n        def __repr__(self):\n            name = 'Spied'\n            if class_:\n                name += class_.__name__\n            return \"<%s id=%s>\" % (name, id(self))\n\n\n    obj = Spy()\n    theMock = Mock(obj, strict=True, spec=object)\n\n    mock_registry.register(obj, theMock)\n    return obj"}
{"func_code_string": "def spy2(fn):  # type: (...) -> None\n    \"\"\"Spy usage of given `fn`.\n\n    Patches the module, class or object `fn` lives in, so that all\n    interactions can be recorded; otherwise executes `fn` as before, so\n    that all side effects happen as before.\n\n    E.g.::\n\n        import time\n        spy(time.time)\n        do_work(...)  # nothing injected, uses global patched `time` module\n        verify(time).time()\n\n    Note that builtins often cannot be patched because they're read-only.\n\n\n    \"\"\"\n    if isinstance(fn, str):\n        answer = get_obj(fn)\n    else:\n        answer = fn\n\n    when2(fn, Ellipsis).thenAnswer(answer)"}
{"func_code_string": "def mock(config_or_spec=None, spec=None, strict=OMITTED):\n    \"\"\"Create 'empty' objects ('Mocks').\n\n    Will create an empty unconfigured object, that you can pass\n    around. All interactions (method calls) will be recorded and can be\n    verified using :func:`verify` et.al.\n\n    A plain `mock()` will be not `strict`, and thus all methods regardless\n    of the arguments will return ``None``.\n\n    .. note:: Technically all attributes will return an internal interface.\n        Because of that a simple ``if mock().foo:`` will surprisingly pass.\n\n    If you set strict to ``True``: ``mock(strict=True)`` all unexpected\n    interactions will raise an error instead.\n\n    You configure a mock using :func:`when`, :func:`when2` or :func:`expect`.\n    You can also very conveniently just pass in a dict here::\n\n        response = mock({'text': 'ok', 'raise_for_status': lambda: None})\n\n    You can also create an empty Mock which is specced against a given\n    `spec`: ``mock(requests.Response)``. These mock are by default strict,\n    thus they raise if you want to stub a method, the spec does not implement.\n    Mockito will also match the function signature.\n\n    You can pre-configure a specced mock as well::\n\n        response = mock({'json': lambda: {'status': 'Ok'}},\n                        spec=requests.Response)\n\n    Mocks are by default callable. Configure the callable behavior using\n    `when`::\n\n        dummy = mock()\n        when(dummy).__call_(1).thenReturn(2)\n\n    All other magic methods must be configured this way or they will raise an\n    AttributeError.\n\n\n    See :func:`verify` to verify your interactions after usage.\n\n    \"\"\"\n\n    if type(config_or_spec) is dict:\n        config = config_or_spec\n    else:\n        config = {}\n        spec = config_or_spec\n\n    if strict is OMITTED:\n        strict = False if spec is None else True\n\n\n    class Dummy(_Dummy):\n        if spec:\n            __class__ = spec  # make isinstance work\n\n        def __getattr__(self, method_name):\n            if strict:\n                raise AttributeError(\n                    \"'Dummy' has no attribute %r configured\" % method_name)\n            return functools.partial(\n                remembered_invocation_builder, theMock, method_name)\n\n        def __repr__(self):\n            name = 'Dummy'\n            if spec:\n                name += spec.__name__\n            return \"<%s id=%s>\" % (name, id(self))\n\n\n    # That's a tricky one: The object we will return is an *instance* of our\n    # Dummy class, but the mock we register will point and patch the class.\n    # T.i. so that magic methods (`__call__` etc.) can be configured.\n    obj = Dummy()\n    theMock = Mock(Dummy, strict=strict, spec=spec)\n\n    for n, v in config.items():\n        if inspect.isfunction(v):\n            invocation.StubbedInvocation(theMock, n)(Ellipsis).thenAnswer(v)\n        else:\n            setattr(obj, n, v)\n\n    mock_registry.register(obj, theMock)\n    return obj"}
{"func_code_string": "def importPuppetClasses(self, smartProxyId):\n        \"\"\" Function importPuppetClasses\n        Force the reload of puppet classes\n\n        @param smartProxyId: smartProxy Id\n        @return RETURN: the API result\n        \"\"\"\n        return self.api.create('{}/{}/import_puppetclasses'\n                               .format(self.objName, smartProxyId), '{}')"}
{"func_code_string": "def get_templates(model):\n    \"\"\" Return a list of templates usable by a model. \"\"\"\n    for template_name, template in templates.items():\n        if issubclass(template.model, model):\n            yield (template_name, template.layout._meta.verbose_name)"}
{"func_code_string": "def attach(*layouts, **kwargs):\n    \"\"\"\n    Registers the given layout(s) classes\n    admin site:\n\n    @pages.register(Page)\n    class Default(PageLayout):\n        pass\n    \"\"\"\n\n    def _model_admin_wrapper(layout_class):\n        register(layout_class, layouts[0])\n        return layout_class\n    return _model_admin_wrapper"}
{"func_code_string": "def enhance(self):\n        \"\"\" Function enhance\n        Enhance the object with new item or enhanced items\n        \"\"\"\n        self.update({'os_default_templates':\n                     SubDict(self.api, self.objName,\n                             self.payloadObj, self.key,\n                             SubItemOsDefaultTemplate)})\n        self.update({'config_templates':\n                     SubDict(self.api, self.objName,\n                             self.payloadObj, self.key,\n                             SubItemConfigTemplate)})\n        self.update({'ptables':\n                     SubDict(self.api, self.objName,\n                             self.payloadObj, self.key,\n                             SubItemPTable)})\n        self.update({'media':\n                     SubDict(self.api, self.objName,\n                             self.payloadObj, self.key,\n                             SubItemMedia)})\n        self.update({'architectures':\n                     SubDict(self.api, self.objName,\n                             self.payloadObj, self.key,\n                             SubItemArchitecture)})"}
{"func_code_string": "def get_api_envs():\n    \"\"\"Get required API keys from environment variables.\"\"\"\n    client_id = os.environ.get('CLIENT_ID')\n    user_id = os.environ.get('USER_ID')\n    if not client_id or not user_id:\n        raise ValueError('API keys are not found in the environment')\n    return client_id, user_id"}
{"func_code_string": "def api_call(method, end_point, params=None, client_id=None, access_token=None):\n    \"\"\"Call given API end_point with API keys.\n    :param method: HTTP method (e.g. 'get', 'delete').\n    :param end_point: API endpoint (e.g. 'users/john/sets').\n    :param params: Dictionary to be sent in the query string (e.g. {'myparam': 'myval'})\n    :param client_id: Quizlet client ID as string.\n    :param access_token: Quizlet access token as string.\n    client_id and access_token are mutually exclusive but mandatory.\n    \"\"\"\n    if bool(client_id) == bool(access_token):\n        raise ValueError('Either client_id or access_token')\n\n    url = 'https://api.quizlet.com/2.0/{}'.format(end_point)\n\n    if not params:\n        params = {}\n    if client_id:\n        params['client_id'] = client_id\n\n    headers = {'Authorization': 'Bearer {}'.format(access_token)} if access_token else None\n\n    response = requests.request(method, url, params=params, headers=headers)\n\n    if int(response.status_code / 100) != 2:\n        error_title = ''\n        try:\n            error_title += ', ' + response.json()['error_title']\n        except ValueError:\n            pass\n        except KeyError:\n            pass\n        raise ValueError(\n            '{} returned {}{}'.format(url, response.status_code, error_title)\n        )\n\n    try:\n        return response.json()\n    except json.decoder.JSONDecodeError:\n        pass"}
{"func_code_string": "def request_upload_secret(self, secret_id):\n        \"\"\"\n        :return: json with \"keyId\" as secret and \"url\" for posting key\n        \"\"\"\n        return self._router.post_request_upload_secret(org_id=self.organizationId,\n                                                       instance_id=self.instanceId,\n                                                       secret_id=secret_id).json()"}
{"func_code_string": "def checkAndCreate(self, key, payload, domainId):\n        \"\"\" Function checkAndCreate\n        Check if a subnet exists and create it if not\n\n        @param key: The targeted subnet\n        @param payload: The targeted subnet description\n        @param domainId: The domainId to be attached wiuth the subnet\n        @return RETURN: The id of the subnet\n        \"\"\"\n        if key not in self:\n            self[key] = payload\n        oid = self[key]['id']\n        if not oid:\n            return False\n        #~ Ensure subnet contains the domain\n        subnetDomainIds = []\n        for domain in self[key]['domains']:\n            subnetDomainIds.append(domain['id'])\n        if domainId not in subnetDomainIds:\n            subnetDomainIds.append(domainId)\n            self[key][\"domain_ids\"] = subnetDomainIds\n            if len(self[key][\"domains\"]) is not len(subnetDomainIds):\n                return False\n        return oid"}
{"func_code_string": "def removeDomain(self, subnetId, domainId):\n        \"\"\" Function removeDomain\n        Delete a domain from a subnet\n\n        @param subnetId: The subnet Id\n        @param domainId: The domainId to be attached wiuth the subnet\n        @return RETURN: boolean\n        \"\"\"\n        subnetDomainIds = []\n        for domain in self[subnetId]['domains']:\n            subnetDomainIds.append(domain['id'])\n        subnetDomainIds.remove(domainId)\n        self[subnetId][\"domain_ids\"] = subnetDomainIds\n        return len(self[subnetId][\"domains\"]) is len(subnetDomainIds)"}
{"func_code_string": "def exclusive(via=threading.Lock):\n    \"\"\"\n    Mark a callable as exclusive\n\n    :param via: factory for a Lock to guard the callable\n\n    Guards the callable against being entered again before completion.\n    Explicitly raises a :py:exc:`RuntimeError` on violation.\n\n    :note: If applied to a method, it is exclusive across all instances.\n    \"\"\"\n    def make_exclusive(fnc):\n        fnc_guard = via()\n\n        @functools.wraps(fnc)\n        def exclusive_call(*args, **kwargs):\n            if fnc_guard.acquire(blocking=False):\n                try:\n                    return fnc(*args, **kwargs)\n                finally:\n                    fnc_guard.release()\n            else:\n                raise RuntimeError('exclusive call to %s violated')\n        return exclusive_call\n    return make_exclusive"}
{"func_code_string": "def service(flavour):\n    r\"\"\"\n    Mark a class as implementing a Service\n\n    Each Service class must have a ``run`` method, which does not take any arguments.\n    This method is :py:meth:`~.ServiceRunner.adopt`\\ ed after the daemon starts, unless\n\n    * the Service has been garbage collected, or\n    * the ServiceUnit has been :py:meth:`~.ServiceUnit.cancel`\\ ed.\n\n    For each service instance, its :py:class:`~.ServiceUnit` is available at ``service_instance.__service_unit__``.\n    \"\"\"\n    def service_unit_decorator(raw_cls):\n        __new__ = raw_cls.__new__\n\n        def __new_service__(cls, *args, **kwargs):\n            if __new__ is object.__new__:\n                self = __new__(cls)\n            else:\n                self = __new__(cls, *args, **kwargs)\n            service_unit = ServiceUnit(self, flavour)\n            self.__service_unit__ = service_unit\n            return self\n\n        raw_cls.__new__ = __new_service__\n        if raw_cls.run.__doc__ is None:\n            raw_cls.run.__doc__ = \"Service entry point\"\n        return raw_cls\n    return service_unit_decorator"}
{"func_code_string": "def execute(self, payload, *args, flavour: ModuleType, **kwargs):\n        \"\"\"\n        Synchronously run ``payload`` and provide its output\n\n        If ``*args*`` and/or ``**kwargs`` are provided, pass them to ``payload`` upon execution.\n        \"\"\"\n        if args or kwargs:\n            payload = functools.partial(payload, *args, **kwargs)\n        return self._meta_runner.run_payload(payload, flavour=flavour)"}
{"func_code_string": "def adopt(self, payload, *args, flavour: ModuleType, **kwargs):\n        \"\"\"\n        Concurrently run ``payload`` in the background\n\n        If ``*args*`` and/or ``**kwargs`` are provided, pass them to ``payload`` upon execution.\n        \"\"\"\n        if args or kwargs:\n            payload = functools.partial(payload, *args, **kwargs)\n        self._meta_runner.register_payload(payload, flavour=flavour)"}
{"func_code_string": "def accept(self):\n        \"\"\"\n        Start accepting synchronous, asynchronous and service payloads\n\n        Since services are globally defined, only one :py:class:`ServiceRunner`\n        may :py:meth:`accept` payloads at any time.\n        \"\"\"\n        if self._meta_runner:\n            raise RuntimeError('payloads scheduled for %s before being started' % self)\n        self._must_shutdown = False\n        self._logger.info('%s starting', self.__class__.__name__)\n        # force collecting objects so that defunct, migrated and overwritten services are destroyed now\n        gc.collect()\n        self._adopt_services()\n        self.adopt(self._accept_services, flavour=trio)\n        self._meta_runner.run()"}
{"func_code_string": "def shutdown(self):\n        \"\"\"Shutdown the accept loop and stop running payloads\"\"\"\n        self._must_shutdown = True\n        self._is_shutdown.wait()\n        self._meta_runner.stop()"}
{"func_code_string": "def milestones(ctx, list, close):\n    \"\"\"View/edit/close milestones on github\n    \"\"\"\n    repos = get_repos(ctx.parent.agile.get('labels'))\n    if list:\n        _list_milestones(repos)\n    elif close:\n        click.echo('Closing milestones \"%s\"' % close)\n        _close_milestone(repos, close)\n    else:\n        click.echo(ctx.get_help())"}
{"func_code_string": "def start_console(local_vars={}):\n    '''Starts a console; modified from code.interact'''\n    transforms.CONSOLE_ACTIVE = True\n    transforms.remove_not_allowed_in_console()\n    sys.ps1 = prompt\n    console = ExperimentalInteractiveConsole(locals=local_vars)\n    console.interact(banner=banner)"}
{"func_code_string": "def push(self, line):\n        \"\"\"Transform and push a line to the interpreter.\n\n        The line should not have a trailing newline; it may have\n        internal newlines.  The line is appended to a buffer and the\n        interpreter's runsource() method is called with the\n        concatenated contents of the buffer as source.  If this\n        indicates that the command was executed or invalid, the buffer\n        is reset; otherwise, the command is incomplete, and the buffer\n        is left as it was after the line was appended.  The return\n        value is 1 if more input is required, 0 if the line was dealt\n        with in some way (this is the same as runsource()).\n\n        \"\"\"\n        if transforms.FROM_EXPERIMENTAL.match(line):\n            transforms.add_transformers(line)\n            self.buffer.append(\"\\n\")\n        else:\n            self.buffer.append(line)\n\n        add_pass = False\n        if line.rstrip(' ').endswith(\":\"):\n            add_pass = True\n        source = \"\\n\".join(self.buffer)\n        if add_pass:\n            source += \"pass\"\n        source = transforms.transform(source)\n        if add_pass:\n            source = source.rstrip(' ')\n            if source.endswith(\"pass\"):\n                source = source[:-4]\n\n        # some transformations may strip an empty line meant to end a block\n        if not self.buffer[-1]:\n            source += \"\\n\"\n        try:\n            more = self.runsource(source, self.filename)\n        except SystemExit:\n            os._exit(1)\n\n        if not more:\n            self.resetbuffer()\n        return more"}
{"func_code_string": "def dump(obj, f, preserve=False):\n    \"\"\"Write dict object into file\n\n    :param obj: the object to be dumped into toml\n    :param f: the file object\n    :param preserve: optional flag to preserve the inline table in result\n    \"\"\"\n    if not f.write:\n        raise TypeError('You can only dump an object into a file object')\n    encoder = Encoder(f, preserve=preserve)\n    return encoder.write_dict(obj)"}
{"func_code_string": "def dumps(obj, preserve=False):\n    \"\"\"Stringifies a dict as toml\n\n    :param obj: the object to be dumped into toml\n    :param preserve: optional flag to preserve the inline table in result\n    \"\"\"\n    f = StringIO()\n    dump(obj, f, preserve)\n    return f.getvalue()"}
{"func_code_string": "def license_loader(lic_dir=LIC_DIR):\n    \"\"\"Loads licenses from the given directory.\"\"\"\n    lics = []\n    for ln in os.listdir(lic_dir):\n        lp = os.path.join(lic_dir, ln)\n        with open(lp) as lf:\n            txt = lf.read()\n            lic = License(txt)\n            lics.append(lic)\n    return lics"}
{"func_code_string": "def get_vector(self, max_choice=3):\n        \"\"\"Return pseudo-choice vectors.\"\"\"\n        vec = {}\n        for dim in ['forbidden', 'required', 'permitted']:\n            if self.meta[dim] is None:\n                continue\n            dim_vec = map(lambda x: (x, max_choice), self.meta[dim])\n            vec[dim] = dict(dim_vec)\n        return vec"}
{"func_code_string": "def entity(ctx, debug, uncolorize, **kwargs):\n    \"\"\"\n    CLI for tonomi.com using contrib-python-qubell-client\n\n    To enable completion:\n\n      eval \"$(_NOMI_COMPLETE=source nomi)\"\n    \"\"\"\n    global PROVIDER_CONFIG\n\n    if debug:\n        log.basicConfig(level=log.DEBUG)\n        log.getLogger(\"requests.packages.urllib3.connectionpool\").setLevel(log.DEBUG)\n    for (k, v) in kwargs.iteritems():\n        if v:\n            QUBELL[k] = v\n    PROVIDER_CONFIG = {\n        'configuration.provider': PROVIDER['provider_type'],\n        'configuration.legacy-regions': PROVIDER['provider_region'],\n        'configuration.endpoint-url': '',\n        'configuration.legacy-security-group': '',\n        'configuration.identity': PROVIDER['provider_identity'],\n        'configuration.credential': PROVIDER['provider_credential']\n    }\n\n    class UserContext(object):\n        def __init__(self):\n            self.platform = None\n            self.unauthenticated_platform = None\n            self.colorize = not (uncolorize)\n\n        def get_platform(self):\n            if not self.platform:\n                assert QUBELL[\"tenant\"], \"No platform URL provided. Set QUBELL_TENANT or use --tenant option.\"\n                if not QUBELL[\"token\"]:\n                    assert QUBELL[\"user\"], \"No username. Set QUBELL_USER or use --user option.\"\n                    assert QUBELL[\"password\"], \"No password provided. Set QUBELL_PASSWORD or use --password option.\"\n\n                self.platform = QubellPlatform.connect(\n                    tenant=QUBELL[\"tenant\"],\n                    user=QUBELL[\"user\"],\n                    password=QUBELL[\"password\"],\n                    token=QUBELL[\"token\"])\n            return self.platform\n\n        def get_unauthenticated_platform(self):\n            if not self.unauthenticated_platform:\n                assert QUBELL[\"tenant\"], \"No platform URL provided. Set QUBELL_TENANT or use --tenant option.\"\n\n                self.unauthenticated_platform = QubellPlatform.connect(tenant=QUBELL[\"tenant\"])\n\n            return self.unauthenticated_platform\n\n    ctx = click.get_current_context()\n    ctx.obj = UserContext()"}
{"func_code_string": "def import_app(files, category, overwrite, id, name):\n    \"\"\" Upload application from file.\n\n    By default, file name will be used as application name, with \"-vXX.YYY\" suffix stripped.\n    Application is looked up by one of these classifiers, in order of priority:\n    app-id, app-name, filename.\n\n    If app-id is provided, looks up existing application and updates its manifest.\n    If app-id is NOT specified, looks up by name, or creates new application.\n\n    \"\"\"\n    platform = _get_platform()\n    org = platform.get_organization(QUBELL[\"organization\"])\n    if category:\n        category = org.categories[category]\n    regex = re.compile(r\"^(.*?)(-v(\\d+)|)\\.[^.]+$\")\n    if (id or name) and len(files) > 1:\n        raise Exception(\"--id and --name are supported only for single-file mode\")\n\n    for filename in files:\n        click.echo(\"Importing \" + filename, nl=False)\n        if not name:\n            match = regex.match(basename(filename))\n            if not match:\n                click.echo(_color(\"RED\", \"FAIL\") + \" unknown filename format\")\n                break\n            name = regex.match(basename(filename)).group(1)\n        click.echo(\" => \", nl=False)\n        app = None\n        try:\n            app = org.get_application(id=id, name=name)\n            if app and not overwrite:\n                click.echo(\"%s %s already exists %s\" % (\n                    app.id, _color(\"BLUE\", app and app.name or name), _color(\"RED\", \"FAIL\")))\n                break\n        except NotFoundError:\n            if id:\n                click.echo(\"%s %s not found %s\" % (\n                    id or \"\", _color(\"BLUE\", app and app.name or name), _color(\"RED\", \"FAIL\")))\n                break\n        click.echo(_color(\"BLUE\", app and app.name or name) + \" \", nl=False)\n        try:\n            with file(filename, \"r\") as f:\n                if app:\n                    app.update(name=app.name,\n                               category=category and category.id or app.category,\n                               manifest=Manifest(content=f.read()))\n                else:\n                    app = org.application(id=id, name=name, manifest=Manifest(content=f.read()))\n                    if category:\n                        app.update(category=category.id)\n            click.echo(app.id + _color(\"GREEN\", \" OK\"))\n        except IOError as e:\n            click.echo(_color(\"RED\", \" FAIL\") + \" \" + e.message)\n            break"}
{"func_code_string": "def show_account():\n    \"\"\"\n    Exports current account configuration in\n    shell-friendly form. Takes into account\n    explicit top-level flags like --organization.\n    \"\"\"\n    click.echo(\"# tonomi api\")\n    for (key, env) in REVERSE_MAPPING.items():\n        value = QUBELL.get(key, None)\n        if value:\n            click.echo(\"export %s='%s'\" % (env, value))\n    if any(map(lambda x: PROVIDER.get(x), REVERSE_PROVIDER_MAPPING.keys())):\n        click.echo(\"# cloud account\")\n        for (key, env) in REVERSE_PROVIDER_MAPPING.items():\n            value = PROVIDER.get(key, None)\n            if value:\n                click.echo(\"export %s='%s'\" % (env, value))"}
{"func_code_string": "def generate_session_token(refresh_token, verbose):\n    \"\"\"\n    Generates new session token from the given refresh token.\n    :param refresh_token: refresh token to generate from\n    :param verbose: whether expiration time should be added to output\n    \"\"\"\n\n    platform = _get_platform(authenticated=False)\n    session_token, expires_in = platform.generate_session_token(refresh_token)\n\n    if verbose:\n        click.echo(\"%s\\n\\n%s\" % (session_token, _color('YELLOW', \"Expires in %d seconds\" % expires_in)))\n    else:\n        click.echo(session_token)"}
{"func_code_string": "def runcommand(cosmology='WMAP5'):\n    \"\"\" Example interface commands \"\"\"\n\n    # Return the WMAP5 cosmology concentration predicted for\n    # z=0 range of masses\n    Mi = [1e8, 1e9, 1e10]\n    zi = 0\n    print(\"Concentrations for haloes of mass %s at z=%s\" % (Mi, zi))\n    output = commah.run(cosmology=cosmology, zi=zi, Mi=Mi)\n\n    print(output['c'].flatten())\n\n    # Return the WMAP5 cosmology concentration predicted for\n    # z=0 range of masses AND cosmological parameters\n    Mi = [1e8, 1e9, 1e10]\n    zi = 0\n    print(\"Concentrations for haloes of mass %s at z=%s\" % (Mi, zi))\n    output, cosmo = commah.run(cosmology=cosmology, zi=zi, Mi=Mi,\n                               retcosmo=True)\n\n    print(output['c'].flatten())\n    print(cosmo)\n\n    # Return the WMAP5 cosmology concentration predicted for MW\n    # mass (2e12 Msol) across redshift\n    Mi = 2e12\n    z = [0, 0.5, 1, 1.5, 2, 2.5]\n    output = commah.run(cosmology=cosmology, zi=0, Mi=Mi, z=z)\n    for zval in z:\n        print(\"M(z=0)=%s has c(z=%s)=%s\"\n              % (Mi, zval, output[output['z'] == zval]['c'].flatten()))\n\n    # Return the WMAP5 cosmology concentration predicted for MW\n    # mass (2e12 Msol) across redshift\n    Mi = 2e12\n    zi = [0, 0.5, 1, 1.5, 2, 2.5]\n    output = commah.run(cosmology=cosmology, zi=zi, Mi=Mi)\n    for zval in zi:\n        print(\"M(z=%s)=%s has concentration %s\"\n              % (zval, Mi, output[(output['zi'] == zval) &\n                                  (output['z'] == zval)]['c'].flatten()))\n\n    # Return the WMAP5 cosmology concentration and\n    # rarity of high-z cluster\n    Mi = 2e14\n    zi = 6\n    output = commah.run(cosmology=cosmology, zi=zi, Mi=Mi)\n    print(\"Concentrations for haloes of mass %s at z=%s\" % (Mi, zi))\n    print(output['c'].flatten())\n    print(\"Mass variance sigma of haloes of mass %s at z=%s\" % (Mi, zi))\n    print(output['sig'].flatten())\n    print(\"Fluctuation for haloes of mass %s at z=%s\" % (Mi, zi))\n    print(output['nu'].flatten())\n\n    # Return the WMAP5 cosmology accretion rate prediction\n    # for haloes at range of redshift and mass\n    Mi = [1e8, 1e9, 1e10]\n    zi = [0]\n    z = [0, 0.5, 1, 1.5, 2, 2.5]\n    output = commah.run(cosmology=cosmology, zi=zi, Mi=Mi, z=z)\n    for Mval in Mi:\n        print(\"dM/dt for halo of mass %s at z=%s across redshift %s is: \"\n              % (Mval, zi, z))\n        print(output[output['Mi'] == Mval]['dMdt'].flatten())\n\n    # Return the WMAP5 cosmology Halo Mass History for haloes with M(z=0) = 1e8\n    M = [1e8]\n    z = [0, 0.5, 1, 1.5, 2, 2.5]\n    print(\"Halo Mass History for z=0 mass of %s across z=%s\" % (M, z))\n    output = commah.run(cosmology=cosmology, zi=0, Mi=M, z=z)\n    print(output['Mz'].flatten())\n\n    # Return the WMAP5 cosmology formation redshifts for haloes at\n    # range of redshift and mass\n    M = [1e8, 1e9, 1e10]\n    z = [0]\n    print(\"Formation Redshifts for haloes of mass %s at z=%s\" % (M, z))\n    output = commah.run(cosmology=cosmology, zi=0, Mi=M, z=z)\n    for Mval in M:\n        print(output[output['Mi'] == Mval]['zf'].flatten())\n\n    return(\"Done\")"}
{"func_code_string": "def plotcommand(cosmology='WMAP5', plotname=None):\n    \"\"\" Example ways to interrogate the dataset and plot the commah output \"\"\"\n\n    # Plot the c-M relation as a functon of redshift\n    xarray = 10**(np.arange(1, 15, 0.2))\n    yval = 'c'\n\n    # Specify the redshift range\n    zarray = np.arange(0, 5, 0.5)\n\n    xtitle = r\"Halo Mass (M$_{sol}$)\"\n    ytitle = r\"Concentration\"\n    linelabel = \"z=\"\n\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    ax.set_xlabel(xtitle)\n    ax.set_ylabel(ytitle)\n    plt.ylim([2, 30])\n\n    colors = cm.rainbow(np.linspace(0, 1, len(zarray)))\n\n    for zind, zval in enumerate(zarray):\n        output = commah.run(cosmology=cosmology, zi=zval, Mi=xarray)\n\n        # Access the column yval from the data file\n        yarray = output[yval].flatten()\n\n        # Plot each line in turn with different colour\n        ax.plot(xarray, yarray, label=linelabel+str(zval), color=colors[zind])\n        # Overplot the D08 predictions in black\n        ax.plot(xarray, commah.commah.cduffy(zval, xarray), color=\"black\")\n\n    ax.set_xscale('log')\n    ax.set_yscale('log')\n\n    leg = ax.legend(loc=1)\n    # Make box totally transparent\n    leg.get_frame().set_alpha(0)\n    leg.get_frame().set_edgecolor('white')\n    for label in leg.get_texts():\n        label.set_fontsize('small')  # the font size\n    for label in leg.get_lines():\n        label.set_linewidth(4)   # the legend line width\n\n    if plotname:\n        fig.tight_layout(pad=0.2)\n        print(\"Plotting to '%s_CM_relation.png'\" % (plotname))\n        fig.savefig(plotname+\"_CM_relation.png\", dpi=fig.dpi*5)\n    else:\n        plt.show()\n\n    # Plot the c-z relation as a function of mass (so always Mz=M0)\n    xarray = 10**(np.arange(0, 1, 0.05)) - 1\n    yval = 'c'\n\n    # Specify the mass range\n    zarray = 10**np.arange(6, 14, 2)\n\n    xtitle = r\"Redshift\"\n    ytitle = r\"NFW Concentration\"\n    linelabel = r\"log$_{10}$ M$_{z}$(M$_{sol}$)=\"\n\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    ax.set_xlabel(xtitle)\n    ax.set_ylabel(ytitle)\n    colors = cm.rainbow(np.linspace(0, 1, len(zarray)))\n\n    for zind, zval in enumerate(zarray):\n        output = commah.run(cosmology=cosmology, zi=xarray, Mi=zval)\n\n        # Access the column yval from the data file\n        yarray = output[yval].flatten()\n\n        # Plot each line in turn with different colours\n        ax.plot(xarray, yarray,\n                label=linelabel+\"{0:.1f}\".format(np.log10(zval)),\n                color=colors[zind],)\n\n    leg = ax.legend(loc=1)\n    # Make box totally transparent\n    leg.get_frame().set_alpha(0)\n    leg.get_frame().set_edgecolor('white')\n    for label in leg.get_texts():\n        label.set_fontsize('small')  # the font size\n    for label in leg.get_lines():\n        label.set_linewidth(4)   # the legend line width\n\n    if plotname:\n        fig.tight_layout(pad=0.2)\n        print(\"Plotting to '%s_Cz_relation.png'\" % (plotname))\n        fig.savefig(plotname+\"_Cz_relation.png\", dpi=fig.dpi*5)\n    else:\n        plt.show()\n\n    # Plot the zf-z relation for different masses (so always Mz=M0)\n    xarray = 10**(np.arange(0, 1, 0.05)) - 1\n    yval = 'zf'\n\n    # Specify the mass range\n    zarray = 10**np.arange(6, 14, 2)\n\n    xtitle = r\"Redshift\"\n    ytitle = r\"Formation Redshift\"\n    linelabel = r\"log$_{10}$ M$_{z}$(M$_{sol}$)=\"\n\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    ax.set_xlabel(xtitle)\n    ax.set_ylabel(ytitle)\n    colors = cm.rainbow(np.linspace(0, 1, len(zarray)))\n\n    for zind, zval in enumerate(zarray):\n        output = commah.run(cosmology=cosmology, zi=xarray, Mi=zval)\n\n        yarray = output[yval].flatten()\n\n        # Plot each line in turn with different colour\n        ax.plot(xarray, yarray,\n                label=linelabel+\"{0:.1f}\".format(np.log10(zval)),\n                color=colors[zind],)\n\n    leg = ax.legend(loc=2)\n    # Make box totally transparent\n    leg.get_frame().set_alpha(0)\n    leg.get_frame().set_edgecolor('white')\n    for label in leg.get_texts():\n        label.set_fontsize('small')  # the font size\n    for label in leg.get_lines():\n        label.set_linewidth(4)   # the legend line width\n\n    if plotname:\n        fig.tight_layout(pad=0.2)\n        print(\"Plotting to '%s_zfz_relation.png'\" % (plotname))\n        fig.savefig(plotname+\"_zfz_relation.png\", dpi=fig.dpi*5)\n    else:\n        plt.show()\n\n    # Plot the dM/dt-z relation for different masses (so always Mz=M0)\n    xarray = 10**(np.arange(0, 1, 0.05)) - 1\n    yval = 'dMdt'\n\n    # Specify the mass range\n    zarray = 10**np.arange(10, 14, 0.5)\n\n    xtitle = r\"log$_{10}$ (1+z)\"\n    ytitle = r\"log$_{10}$ Accretion Rate M$_{sol}$ yr$^{-1}$\"\n    linelabel = r\"log$_{10}$ M$_z$(M$_{sol}$)=\"\n\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    ax.set_xlabel(xtitle)\n    ax.set_ylabel(ytitle)\n    colors = cm.rainbow(np.linspace(0, 1, len(zarray)))\n\n    cosmo = commah.getcosmo(cosmology)\n    for zind, zval in enumerate(zarray):\n        output = commah.run(cosmology=cosmology, zi=xarray, Mi=zval,\n                            com=False, mah=True)\n\n        yarray = output[yval].flatten()\n\n        # Plot each line in turn with different colour\n        ax.plot(np.log10(xarray+1.), np.log10(yarray),\n                label=linelabel+\"{0:.1f}\".format(np.log10(zval)),\n                color=colors[zind],)\n\n        # Plot the semi-analytic approximate formula from Correa et al 2015b\n        semianalytic_approx = 71.6 * (zval / 1e12) * (cosmo['h'] / 0.7) *\\\n            (-0.24 + 0.75 * (xarray + 1)) * np.sqrt(\n            cosmo['omega_M_0'] * (xarray + 1)**3 + cosmo['omega_lambda_0'])\n\n        ax.plot(np.log10(xarray + 1), np.log10(semianalytic_approx),\n                color='black')\n\n    leg = ax.legend(loc=2)\n    # Make box totally transparent\n    leg.get_frame().set_alpha(0)\n    leg.get_frame().set_edgecolor('white')\n    for label in leg.get_texts():\n        label.set_fontsize('small')  # the font size\n    for label in leg.get_lines():\n        label.set_linewidth(4)   # the legend line width\n\n    if plotname:\n        fig.tight_layout(pad=0.2)\n        print(\"Plotting to '%s_dMdtz_relation.png'\" % (plotname))\n        fig.savefig(plotname+\"_dMdtz_relation.png\", dpi=fig.dpi*5)\n    else:\n        plt.show()\n\n    # Plot the dMdt-M relation as a function of redshift\n    xarray = 10**(np.arange(10, 14, 0.5))\n    yval = 'dMdt'\n\n    # Specify the redshift range\n    zarray = np.arange(0, 5, 0.5)\n\n    xtitle = r\"Halo Mass M$_{sol}$\"\n    ytitle = r\"Accretion Rate M$_{sol}$ yr$^{-1}$\"\n    linelabel = \"z=\"\n\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    ax.set_xlabel(xtitle)\n    ax.set_ylabel(ytitle)\n    colors = cm.rainbow(np.linspace(0, 1, len(zarray)))\n\n    for zind, zval in enumerate(zarray):\n        output = commah.run(cosmology=cosmology, zi=zval, Mi=xarray,\n                            com=False, mah=True)\n\n        yarray = output[yval].flatten()\n\n        # Plot each line in turn with different colour\n        ax.plot(xarray, yarray, label=linelabel+str(zval),\n                color=colors[zind],)\n\n    ax.set_xscale('log')\n    ax.set_yscale('log')\n\n    leg = ax.legend(loc=2)\n    # Make box totally transparent\n    leg.get_frame().set_alpha(0)\n    leg.get_frame().set_edgecolor('white')\n    for label in leg.get_texts():\n        label.set_fontsize('small')  # the font size\n    for label in leg.get_lines():\n        label.set_linewidth(4)   # the legend line width\n\n    if plotname:\n        fig.tight_layout(pad=0.2)\n        print(\"Plotting to '%s_MAH_M_relation.png'\" % (plotname))\n        fig.savefig(plotname+\"_MAH_M_relation.png\", dpi=fig.dpi*5)\n    else:\n        plt.show()\n\n    # Plot the (dM/M)dt-M relation as a function of redshift\n    xarray = 10**(np.arange(10, 14, 0.5))\n    yval = 'dMdt'\n\n    # Specify the redshift range\n    zarray = np.arange(0, 5, 0.5)\n\n    xtitle = r\"Halo Mass M$_{sol}$\"\n    ytitle = r\"Specific Accretion Rate yr$^{-1}$\"\n    linelabel = \"z=\"\n\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    ax.set_xlabel(xtitle)\n    ax.set_ylabel(ytitle)\n    colors = cm.rainbow(np.linspace(0, 1, len(zarray)))\n\n    for zind, zval in enumerate(zarray):\n        output = commah.run(cosmology=cosmology, zi=zval, Mi=xarray,\n                            mah=True, com=False)\n\n        yarray = output[yval].flatten()\n\n        # Plot each line in turn with different colour\n        ax.plot(xarray, yarray/xarray, label=linelabel+str(zval),\n                color=colors[zind],)\n\n    ax.set_xscale('log')\n    ax.set_yscale('log')\n\n    leg = ax.legend(loc=1)\n    # Make box totally transparent\n    leg.get_frame().set_alpha(0)\n    leg.get_frame().set_edgecolor('white')\n    for label in leg.get_texts():\n        label.set_fontsize('small')  # the font size\n    for label in leg.get_lines():\n        label.set_linewidth(4)   # the legend line width\n\n    if plotname:\n        fig.tight_layout(pad=0.2)\n        print(\"Plotting to '%s_specificMAH_M_relation.png'\" % (plotname))\n        fig.savefig(plotname+\"_specificMAH_M_relation.png\", dpi=fig.dpi*5)\n    else:\n        plt.show()\n\n    # Plot the Mz-z relation as a function of mass\n    # (so mass is decreasing to zero as z-> inf)\n    xarray = 10**(np.arange(0, 1, 0.05)) - 1\n    yval = 'Mz'\n\n    # Specify the mass range\n    zarray = 10**np.arange(10, 14, 0.5)\n\n    xtitle = r\"Redshift\"\n    ytitle = r\"M(z) (M$_{sol}$)\"\n    linelabel = r\"log$_{10}$ M$_{0}$(M$_{sol}$)=\"\n\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    ax.set_xlabel(xtitle)\n    ax.set_ylabel(ytitle)\n    colors = cm.rainbow(np.linspace(0, 1, len(zarray)))\n\n    for zind, zval in enumerate(zarray):\n        output = commah.run(cosmology=cosmology, zi=0, Mi=zval, z=xarray)\n\n        yarray = output[yval].flatten()\n\n        # Plot each line in turn with different colour\n        ax.plot(xarray, yarray,\n                label=linelabel+\"{0:.1f}\".format(np.log10(zval)),\n                color=colors[zind],)\n\n    ax.set_yscale('log')\n\n    leg = ax.legend(loc=1)\n    # Make box totally transparent\n    leg.get_frame().set_alpha(0)\n    leg.get_frame().set_edgecolor('white')\n    for label in leg.get_texts():\n        label.set_fontsize('small')  # the font size\n    for label in leg.get_lines():\n        label.set_linewidth(4)   # the legend line width\n\n    if plotname:\n        fig.tight_layout(pad=0.2)\n        print(\"Plotting to '%s_Mzz_relation.png'\" % (plotname))\n        fig.savefig(plotname+\"_Mzz_relation.png\", dpi=fig.dpi*5)\n    else:\n        plt.show()\n\n    # Plot the Mz/M0-z relation as a function of mass\n    xarray = 10**(np.arange(0, 1, 0.02)) - 1\n    yval = 'Mz'\n\n    # Specify the mass range\n    zarray = 10**np.arange(10, 14, 0.5)\n\n    xtitle = r\"Redshift\"\n    ytitle = r\"log$_{10}$ M(z)/M$_{0}$\"\n    linelabel = r\"log$_{10}$ M$_{0}$(M$_{sol}$)=\"\n\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    ax.set_xlabel(xtitle)\n    ax.set_ylabel(ytitle)\n    colors = cm.rainbow(np.linspace(0, 1, len(zarray)))\n\n    for zind, zval in enumerate(zarray):\n        output = commah.run(cosmology=cosmology, zi=0, Mi=zval, z=xarray)\n\n        yarray = output[yval].flatten()\n\n        # Plot each line in turn with different colour\n        ax.plot(xarray, np.log10(yarray/zval),\n                label=linelabel+\"{0:.1f}\".format(np.log10(zval)),\n                color=colors[zind],)\n\n    leg = ax.legend(loc=3)\n    # Make box totally transparent\n    leg.get_frame().set_alpha(0)\n    leg.get_frame().set_edgecolor('white')\n    for label in leg.get_texts():\n        label.set_fontsize('small')  # the font size\n    for label in leg.get_lines():\n        label.set_linewidth(4)   # the legend line width\n\n    if plotname:\n        fig.tight_layout(pad=0.2)\n        print(\"Plotting to '%s_MzM0z_relation.png'\" % (plotname))\n        fig.savefig(plotname+\"_MzM0z_relation.png\", dpi=fig.dpi*5)\n    else:\n        plt.show()\n\n    return(\"Done\")"}
{"func_code_string": "def enhance(self):\n        \"\"\" Function enhance\n        Enhance the object with new item or enhanced items\n        \"\"\"\n        self.update({'puppetclasses':\n                     SubDict(self.api, self.objName,\n                             self.payloadObj, self.key,\n                             SubItemPuppetClasses)})\n        self.update({'parameters':\n                     SubDict(self.api, self.objName,\n                             self.payloadObj, self.key,\n                             SubItemParameter)})\n        self.update({'smart_class_parameters':\n                    SubDict(self.api, self.objName,\n                            self.payloadObj, self.key,\n                            ItemSmartClassParameter)})"}
{"func_code_string": "def add_transformers(line):\n    '''Extract the transformers names from a line of code of the form\n       from __experimental__ import transformer1 [,...]\n       and adds them to the globally known dict\n    '''\n    assert FROM_EXPERIMENTAL.match(line)\n\n    line = FROM_EXPERIMENTAL.sub(' ', line)\n    # we now have: \" transformer1 [,...]\"\n    line = line.split(\"#\")[0]    # remove any end of line comments\n    # and insert each transformer as an item in a list\n    for trans in line.replace(' ', '').split(','):\n        import_transformer(trans)"}
{"func_code_string": "def import_transformer(name):\n    '''If needed, import a transformer, and adds it to the globally known dict\n       The code inside a module where a transformer is defined should be\n       standard Python code, which does not need any transformation.\n       So, we disable the import hook, and let the normal module import\n       do its job - which is faster and likely more reliable than our\n       custom method.\n    '''\n    if name in transformers:\n        return transformers[name]\n\n    # We are adding a transformer built from normal/standard Python code.\n    # As we are not performing transformations, we temporarily disable\n    # our import hook, both to avoid potential problems AND because we\n    # found that this resulted in much faster code.\n    hook = sys.meta_path[0]\n    sys.meta_path = sys.meta_path[1:]\n    try:\n        transformers[name] = __import__(name)\n        # Some transformers are not allowed in the console.\n        # If an attempt is made to activate one of them in the console,\n        # we replace it by a transformer that does nothing and print a\n        # message specific to that transformer as written in its module.\n        if CONSOLE_ACTIVE:\n            if hasattr(transformers[name], \"NO_CONSOLE\"):\n                print(transformers[name].NO_CONSOLE)\n                transformers[name] = NullTransformer()\n    except ImportError:\n        sys.stderr.write(\"Warning: Import Error in add_transformers: %s not found\\n\" % name)\n        transformers[name] = NullTransformer()\n    except Exception as e:\n        sys.stderr.write(\"Unexpected exception in transforms.import_transformer%s\\n \" %\n                         e.__class__.__name__)\n    finally:\n        sys.meta_path.insert(0, hook) # restore import hook\n\n    return transformers[name]"}
{"func_code_string": "def extract_transformers_from_source(source):\n    '''Scan a source for lines of the form\n       from __experimental__ import transformer1 [,...]\n       identifying transformers to be used. Such line is passed to the\n       add_transformer function, after which it is removed from the\n       code to be executed.\n    '''\n    lines = source.split('\\n')\n    linenumbers = []\n    for number, line in enumerate(lines):\n        if FROM_EXPERIMENTAL.match(line):\n            add_transformers(line)\n            linenumbers.insert(0, number)\n\n    # drop the \"fake\" import from the source code\n    for number in linenumbers:\n        del lines[number]\n    return '\\n'.join(lines)"}
{"func_code_string": "def remove_not_allowed_in_console():\n    '''This function should be called from the console, when it starts.\n\n    Some transformers are not allowed in the console and they could have\n    been loaded prior to the console being activated. We effectively remove them\n    and print an information message specific to that transformer\n    as written in the transformer module.\n\n    '''\n    not_allowed_in_console = []\n    if CONSOLE_ACTIVE:\n        for name in transformers:\n            tr_module = import_transformer(name)\n            if hasattr(tr_module, \"NO_CONSOLE\"):\n                not_allowed_in_console.append((name, tr_module))\n        for name, tr_module in not_allowed_in_console:\n            print(tr_module.NO_CONSOLE)\n            # Note: we do not remove them, so as to avoid seeing the\n            # information message displayed again if an attempt is\n            # made to re-import them from a console instruction.\n            transformers[name] = NullTransformer()"}
{"func_code_string": "def transform(source):\n    '''Used to convert the source code, making use of known transformers.\n\n       \"transformers\" are modules which must contain a function\n\n           transform_source(source)\n\n       which returns a tranformed source.\n       Some transformers (for example, those found in the standard library\n       module lib2to3) cannot cope with non-standard syntax; as a result, they\n       may fail during a first attempt. We keep track of all failing\n       transformers and keep retrying them until either they all succeeded\n       or a fixed set of them fails twice in a row.\n    '''\n    source = extract_transformers_from_source(source)\n\n    # Some transformer fail when multiple non-Python constructs\n    # are present. So, we loop multiple times keeping track of\n    # which transformations have been unsuccessfully performed.\n    not_done = transformers\n    while True:\n        failed = {}\n        for name in not_done:\n            tr_module = import_transformer(name)\n            try:\n                source = tr_module.transform_source(source)\n            except Exception as e:\n                failed[name] = tr_module\n                # from traceback import print_exc\n                # print(\"Unexpected exception in transforms.transform\",\n                #       e.__class__.__name__)\n                # print_exc()\n\n        if not failed:\n            break\n        # Insanity is doing the same Tting over and overaAgain and\n        # expecting different results ...\n        # If the exact same set of transformations are not performed\n        # twice in a row, there is no point in trying out a third time.\n        if failed == not_done:\n            print(\"Warning: the following transforms could not be done:\")\n            for key in failed:\n                print(key)\n            break\n        not_done = failed  # attempt another pass\n\n    return source"}
{"func_code_string": "def _match(self, request, response):\n        \"\"\"Match all requests/responses that satisfy the following conditions:\n\n        * An Admin App; i.e. the path is something like /admin/some_app/\n        * The ``include_flag`` is not in the response's content\n\n        \"\"\"\n        is_html = 'text/html' in response.get('Content-Type', '')\n        if is_html and hasattr(response, 'rendered_content'):\n            correct_path = PATH_MATCHER.match(request.path) is not None\n            not_included = self.include_flag not in response.rendered_content\n            return correct_path and not_included\n        return False"}
{"func_code_string": "def _chosen_css(self):\n        \"\"\"Read the minified CSS file including STATIC_URL in the references\n        to the sprite images.\"\"\"\n        css = render_to_string(self.css_template, {})\n        for sprite in self.chosen_sprites:  # rewrite path to sprites in the css\n            css = css.replace(sprite, settings.STATIC_URL + \"img/\" + sprite)\n        return css"}
{"func_code_string": "def _embed(self, request, response):\n        \"\"\"Embed Chosen.js directly in html of the response.\"\"\"\n        if self._match(request, response):\n            # Render the <link> and the <script> tags to include Chosen.\n            head = render_to_string(\n                \"chosenadmin/_head_css.html\",\n                {\"chosen_css\": self._chosen_css()}\n            )\n            body = render_to_string(\n                \"chosenadmin/_script.html\",\n                {\"chosen_js\": self._chosen_js()}\n            )\n\n            # Re-write the Response's content to include our new html\n            content = response.rendered_content\n            content = content.replace('</head>', head)\n            content = content.replace('</body>', body)\n            response.content = content\n        return response"}
{"func_code_string": "def clean_up(self):\n        \"\"\"\n        Close the I2C bus\n        \"\"\"\n        self.log.debug(\"Closing I2C bus for address: 0x%02X\" % self.address)\n        self.bus.close()"}
{"func_code_string": "def write_quick(self):\n        \"\"\"\n        Send only the read / write bit\n        \"\"\"\n        self.bus.write_quick(self.address)\n        self.log.debug(\"write_quick: Sent the read / write bit\")"}
{"func_code_string": "def write_byte(self, cmd, value):\n        \"\"\"\n        Writes an 8-bit byte to the specified command register\n        \"\"\"\n        self.bus.write_byte_data(self.address, cmd, value)\n        self.log.debug(\n            \"write_byte: Wrote 0x%02X to command register 0x%02X\" % (\n                value, cmd\n            )\n        )"}
{"func_code_string": "def write_word(self, cmd, value):\n        \"\"\"\n        Writes a 16-bit word to the specified command register\n        \"\"\"\n        self.bus.write_word_data(self.address, cmd, value)\n        self.log.debug(\n            \"write_word: Wrote 0x%04X to command register 0x%02X\" % (\n                value, cmd\n            )\n        )"}
{"func_code_string": "def write_raw_byte(self, value):\n        \"\"\"\n        Writes an 8-bit byte directly to the bus\n        \"\"\"\n        self.bus.write_byte(self.address, value)\n        self.log.debug(\"write_raw_byte: Wrote 0x%02X\" % value)"}
{"func_code_string": "def write_block_data(self, cmd, block):\n        \"\"\"\n        Writes a block of bytes to the bus using I2C format to the specified\n        command register\n        \"\"\"\n        self.bus.write_i2c_block_data(self.address, cmd, block)\n        self.log.debug(\n            \"write_block_data: Wrote [%s] to command register 0x%02X\" % (\n                ', '.join(['0x%02X' % x for x in block]),\n                cmd\n            )\n        )"}
{"func_code_string": "def read_raw_byte(self):\n        \"\"\"\n        Read an 8-bit byte directly from the bus\n        \"\"\"\n        result = self.bus.read_byte(self.address)\n        self.log.debug(\"read_raw_byte: Read 0x%02X from the bus\" % result)\n        return result"}
{"func_code_string": "def read_block_data(self, cmd, length):\n        \"\"\"\n        Read a block of bytes from the bus from the specified command register\n        Amount of bytes read in is defined by length\n        \"\"\"\n        results = self.bus.read_i2c_block_data(self.address, cmd, length)\n        self.log.debug(\n            \"read_block_data: Read [%s] from command register 0x%02X\" % (\n                ', '.join(['0x%02X' % x for x in results]),\n                cmd\n            )\n        )\n        return results"}
{"func_code_string": "def read_unsigned_byte(self, cmd):\n        \"\"\"\n        Read an unsigned byte from the specified command register\n        \"\"\"\n        result = self.bus.read_byte_data(self.address, cmd)\n        self.log.debug(\n            \"read_unsigned_byte: Read 0x%02X from command register 0x%02X\" % (\n                result, cmd\n            )\n        )\n        return result"}
{"func_code_string": "def read_unsigned_word(self, cmd, little_endian=True):\n        \"\"\"\n        Read an unsigned word from the specified command register\n        We assume the data is in little endian mode, if it is in big endian\n        mode then set little_endian to False\n        \"\"\"\n        result = self.bus.read_word_data(self.address, cmd)\n\n        if not little_endian:\n            result = ((result << 8) & 0xFF00) + (result >> 8)\n\n        self.log.debug(\n            \"read_unsigned_word: Read 0x%04X from command register 0x%02X\" % (\n                result, cmd\n            )\n        )\n        return result"}
{"func_code_string": "def __connect_to_bus(self, bus):\n        \"\"\"\n        Attempt to connect to an I2C bus\n        \"\"\"\n        def connect(bus_num):\n            try:\n                self.log.debug(\"Attempting to connect to bus %s...\" % bus_num)\n                self.bus = smbus.SMBus(bus_num)\n                self.log.debug(\"Success\")\n            except IOError:\n                self.log.debug(\"Failed\")\n                raise\n\n        # If the bus is not explicitly stated, try 0 and then try 1 if that\n        # fails\n        if bus is None:\n            try:\n                connect(0)\n                return\n            except IOError:\n                pass\n\n            try:\n                connect(1)\n                return\n            except IOError:\n                raise\n        else:\n            try:\n                connect(bus)\n                return\n            except IOError:\n                raise"}
{"func_code_string": "def get_formset(self, request, obj=None, **kwargs):\n        \"\"\" Default user to the current version owner. \"\"\"\n        data = super().get_formset(request, obj, **kwargs)\n        if obj:\n            data.form.base_fields['user'].initial = request.user.id\n        return data"}
{"func_code_string": "def reload(self):\n        \"\"\" Function reload\n        Reload the full object to ensure sync\n        \"\"\"\n        realData = self.load()\n        self.clear()\n        self.update(realData)"}
{"func_code_string": "def updateAfterDecorator(function):\n        \"\"\" Function updateAfterDecorator\n        Decorator to ensure local dict is sync with remote foreman\n        \"\"\"\n        def _updateAfterDecorator(self, *args, **kwargs):\n            ret = function(self, *args, **kwargs)\n            self.reload()\n            return ret\n        return _updateAfterDecorator"}
{"func_code_string": "def updateBeforeDecorator(function):\n        \"\"\" Function updateAfterDecorator\n        Decorator to ensure local dict is sync with remote foreman\n        \"\"\"\n        def _updateBeforeDecorator(self, *args, **kwargs):\n            if self.forceFullSync:\n                self.reload()\n            return function(self, *args, **kwargs)\n        return _updateBeforeDecorator"}
{"func_code_string": "def load(self):\n        \"\"\" Function load\n        Get the list of all objects\n\n        @return RETURN: A ForemanItem list\n        \"\"\"\n        return {x[self.index]: self.itemType(self.api, x['id'],\n                                             self.objName, self.payloadObj,\n                                             x)\n                for x in self.api.list(self.objName,\n                                       limit=self.searchLimit)}"}
{"func_code_string": "def checkAndCreate(self, key, payload):\n        \"\"\" Function checkAndCreate\n        Check if an object exists and create it if not\n\n        @param key: The targeted object\n        @param payload: The targeted object description\n        @return RETURN: The id of the object\n        \"\"\"\n        if key not in self:\n            self[key] = payload\n        return self[key]['id']"}
{"func_code_string": "def operations():\n    \"\"\"\n    Class decorator stores all calls into list.\n    Can be used until .invalidate() is called.\n    :return: decorated class\n    \"\"\"\n    def decorator(func):\n        @wraps(func)\n        def wrapped_func(*args, **kwargs):\n\n            self = args[0]\n            assert self.__can_use, \"User operation queue only in 'with' block\"\n\n            def defaults_dict():\n                f_args, varargs, keywords, defaults = inspect.getargspec(func)\n                defaults = defaults or []\n                return dict(zip(f_args[-len(defaults)+len(args[1:]):], defaults[len(args[1:]):]))\n\n            route_args = dict(defaults_dict().items() + kwargs.items())\n\n            func(*args, **kwargs)\n            self.operations.append((func.__name__, args[1:], route_args, ))\n\n        return wrapped_func\n    def decorate(clazz):\n\n        for attr in clazz.__dict__:\n            if callable(getattr(clazz, attr)):\n                setattr(clazz, attr, decorator(getattr(clazz, attr)))\n        def __init__(self):  # simple parameter-less constructor\n            self.operations = []\n            self.__can_use = True\n        def invalidate(self):\n            self.__can_use = False\n        clazz.__init__ = __init__\n        clazz.invalidate = invalidate\n        return clazz\n    return decorate"}
{"func_code_string": "def process_actions(action_ids=None):\n    \"\"\"\n    Process actions in the publishing schedule.\n\n    Returns the number of actions processed.\n    \"\"\"\n    actions_taken = 0\n    action_list = PublishAction.objects.prefetch_related(\n        'content_object',\n    ).filter(\n        scheduled_time__lte=timezone.now(),\n    )\n\n    if action_ids is not None:\n        action_list = action_list.filter(id__in=action_ids)\n\n    for action in action_list:\n        action.process_action()\n        action.delete()\n        actions_taken += 1\n\n    return actions_taken"}
{"func_code_string": "def celery_enabled():\n    \"\"\"\n    Return a boolean if Celery tasks are enabled for this app.\n\n    If the ``GLITTER_PUBLISHER_CELERY`` setting is ``True`` or ``False`` - then that value will be\n    used. However if the setting isn't defined, then this will be enabled automatically if Celery\n    is installed.\n    \"\"\"\n    enabled = getattr(settings, 'GLITTER_PUBLISHER_CELERY', None)\n\n    if enabled is None:\n        try:\n            import celery  # noqa\n            enabled = True\n        except ImportError:\n            enabled = False\n\n    return enabled"}
{"func_code_string": "def checkAndCreate(self, key, payload):\n        \"\"\" Function checkAndCreate\n        Check if an object exists and create it if not\n\n        @param key: The targeted object\n        @param payload: The targeted object description\n        @return RETURN: The id of the object\n        \"\"\"\n        if key not in self:\n            if 'templates' in payload:\n                templates = payload.pop('templates')\n            self[key] = payload\n            self.reload()\n        return self[key]['id']"}
{"func_code_string": "def __collect_interfaces_return(interfaces):\n        \"\"\"Collect new style (44.1+) return values to old-style kv-list\"\"\"\n        acc = []\n        for (interfaceName, interfaceData) in interfaces.items():\n            signalValues = interfaceData.get(\"signals\", {})\n            for (signalName, signalValue) in signalValues.items():\n                pinName = \"{0}.{1}\".format(interfaceName, signalName)\n                acc.append({'id': pinName, 'value': signalValue})\n        return acc"}
{"func_code_string": "def return_values(self):\n        \"\"\" Guess what api we are using and return as public api does.\n        Private has {'id':'key', 'value':'keyvalue'} format, public has {'key':'keyvalue'}\n        \"\"\"\n\n        j = self.json()\n        #TODO: FIXME: get rid of old API when its support will be removed\n        public_api_value = j.get('returnValues')\n        old_private_value = j.get('endpoints')\n        new_private_value = self.__collect_interfaces_return(j.get('interfaces', {}))\n\n        retvals = new_private_value or old_private_value or public_api_value or []\n        # TODO: Public api hack.\n        if self._router.public_api_in_use:\n            return retvals\n        return self.__parse(retvals)"}
{"func_code_string": "def get_activitylog(self, after=None, severity=None, start=None, end=None):\n        \"\"\"\n        Returns activitylog object\n        severity - filter severity ('INFO', DEBUG')\n        start/end - time or log text\n\n        \"\"\"\n        if after:\n            log_raw = self._router.get_instance_activitylog(org_id=self.organizationId,\n                                                            instance_id=self.instanceId,\n                                                            params={\"after\": after}).json()\n        else:\n            log_raw = self._router.get_instance_activitylog(org_id=self.organizationId,\n                                                            instance_id=self.instanceId).json()\n\n        return ActivityLog(log_raw, severity=severity, start=start, end=end)"}
{"func_code_string": "def json(self):\n        \"\"\"\n        return __cached_json, if accessed withing 300 ms.\n        This allows to optimize calls when many parameters of entity requires withing short time.\n        \"\"\"\n\n        if self.fresh():\n            return self.__cached_json\n        # noinspection PyAttributeOutsideInit\n        self.__last_read_time = time.time()\n        self.__cached_json = self._router.get_instance(org_id=self.organizationId, instance_id=self.instanceId).json()\n\n        return self.__cached_json"}
{"func_code_string": "def get_most_recent_update_time(self):\n        \"\"\"\n        Indicated most recent update of the instance, assumption based on:\n        - if currentWorkflow exists, its startedAt time is most recent update.\n        - else max of workflowHistory startedAt is most recent update.\n        \"\"\"\n        def parse_time(t):\n            if t:\n                return time.gmtime(t/1000)\n            return None\n        try:\n            max_wf_started_at = max([i.get('startedAt') for i in self.workflowHistory])\n            return parse_time(max_wf_started_at)\n        except ValueError:\n            return None"}
{"func_code_string": "def _is_projection_updated_instance(self):\n        \"\"\"\n        This method tries to guess if instance was update since last time.\n        If return True, definitely Yes, if False, this means more unknown\n        :return: bool\n        \"\"\"\n        last = self._last_workflow_started_time\n        if not self._router.public_api_in_use:\n            most_recent = self.get_most_recent_update_time()\n        else:\n            most_recent = None\n        if last and most_recent:\n            return last < most_recent\n        return False"}
{"func_code_string": "def find(self, item, description='', event_type=''):\n        \"\"\"\n        Find regexp in activitylog\n        find record as if type are in description.\n        \"\"\"\n        # TODO: should be refactored, dumb logic\n        if ': ' in item:\n            splited = item.split(': ', 1)\n            if splited[0] in self.TYPES:\n                description = item.split(': ')[1]\n                event_type = item.split(': ')[0]\n            else:\n                description = item\n        else:\n            if not description:\n                description = item\n\n        if event_type:\n            found = [x['time'] for x in self.log if re.search(description, x['description'])\n                     and x['eventTypeText'] == event_type]\n        else:\n            found = [x['time'] for x in self.log if re.search(description, x['description'])]\n\n        if len(found):\n            return found\n        raise exceptions.NotFoundError(\"Item '{}' is not found with (description='{}', event_type='{}')\".\n                                       format(item, description, event_type))"}
{"func_code_string": "def do_command_line(infile: typing.IO[str]) -> int:\n    \"\"\"\n    Currently a small stub to create an instance of Checker for the passed\n    ``infile`` and run its test functions through linting.\n\n    Args:\n        infile\n\n    Returns:\n        int: Number of flake8 errors raised.\n    \"\"\"\n    lines = infile.readlines()\n    tree = ast.parse(''.join(lines))\n    checker = Checker(tree, lines, infile.name)\n    checker.load()\n    errors = []  # type: typing.List[AAAError]\n    for func in checker.all_funcs(skip_noqa=True):\n        try:\n            errors = list(func.check_all())\n        except ValidationError as error:\n            errors = [error.to_aaa()]\n        print(func.__str__(errors), end='')\n    return len(errors)"}
{"func_code_string": "def find_spec(self, fullname, path, target=None):\n        '''finds the appropriate properties (spec) of a module, and sets\n           its loader.'''\n        if not path:\n            path = [os.getcwd()]\n        if \".\" in fullname:\n            name = fullname.split(\".\")[-1]\n        else:\n            name = fullname\n        for entry in path:\n            if os.path.isdir(os.path.join(entry, name)):\n                # this module has child modules\n                filename = os.path.join(entry, name, \"__init__.py\")\n                submodule_locations = [os.path.join(entry, name)]\n            else:\n                filename = os.path.join(entry, name + \".py\")\n                submodule_locations = None\n            if not os.path.exists(filename):\n                continue\n\n            return spec_from_file_location(fullname, filename,\n                                           loader=MyLoader(filename),\n                                           submodule_search_locations=submodule_locations)\n        return None"}
{"func_code_string": "def exec_module(self, module):\n        '''import the source code, transforma it before executing it so that\n           it is known to Python.'''\n        global MAIN_MODULE_NAME\n        if module.__name__ == MAIN_MODULE_NAME:\n            module.__name__ = \"__main__\"\n            MAIN_MODULE_NAME = None\n\n        with open(self.filename) as f:\n            source = f.read()\n\n        if transforms.transformers:\n            source = transforms.transform(source)\n        else:\n            for line in source.split('\\n'):\n                if transforms.FROM_EXPERIMENTAL.match(line):\n                    ## transforms.transform will extract all such relevant\n                    ## lines and add them all relevant transformers\n                    source = transforms.transform(source)\n                    break\n        exec(source, vars(module))"}
{"func_code_string": "def _izip(*iterables):\n    \"\"\" Iterate through multiple lists or arrays of equal size \"\"\"\n    # This izip routine is from itertools\n    # izip('ABCD', 'xy') --> Ax By\n\n    iterators = map(iter, iterables)\n    while iterators:\n        yield tuple(map(next, iterators))"}
{"func_code_string": "def _checkinput(zi, Mi, z=False, verbose=None):\n    \"\"\" Check and convert any input scalar or array to numpy array \"\"\"\n    # How many halo redshifts provided?\n    zi = np.array(zi, ndmin=1, dtype=float)\n\n    # How many halo masses provided?\n    Mi = np.array(Mi, ndmin=1, dtype=float)\n\n    # Check the input sizes for zi and Mi make sense, if not then exit unless\n    # one axis is length one, then replicate values to the size of the other\n    if (zi.size > 1) and (Mi.size > 1):\n        if(zi.size != Mi.size):\n            print(\"Error ambiguous request\")\n            print(\"Need individual redshifts for all haloes provided \")\n            print(\"Or have all haloes at same redshift \")\n            return(-1)\n    elif (zi.size == 1) and (Mi.size > 1):\n        if verbose:\n            print(\"Assume zi is the same for all Mi halo masses provided\")\n        # Replicate redshift for all halo masses\n        zi = np.ones_like(Mi)*zi[0]\n    elif (Mi.size == 1) and (zi.size > 1):\n        if verbose:\n            print(\"Assume Mi halo masses are the same for all zi provided\")\n        # Replicate redshift for all halo masses\n        Mi = np.ones_like(zi)*Mi[0]\n    else:\n        if verbose:\n            print(\"A single Mi and zi provided\")\n\n    # Very simple test for size / type of incoming array\n    # just in case numpy / list given\n    if z is False:\n        # Didn't pass anything, set zi = z\n        lenzout = 1\n    else:\n        # If something was passed, convert to 1D NumPy array\n        z = np.array(z, ndmin=1, dtype=float)\n        lenzout = z.size\n\n    return(zi, Mi, z, zi.size, Mi.size, lenzout)"}
{"func_code_string": "def getcosmo(cosmology):\n    \"\"\" Find cosmological parameters for named cosmo in cosmology.py list \"\"\"\n\n    defaultcosmologies = {'dragons': cg.DRAGONS(), 'wmap1': cg.WMAP1_Mill(),\n                          'wmap3': cg.WMAP3_ML(), 'wmap5': cg.WMAP5_mean(),\n                          'wmap7': cg.WMAP7_ML(), 'wmap9': cg.WMAP9_ML(),\n                          'wmap1_lss': cg.WMAP1_2dF_mean(),\n                          'wmap3_mean': cg.WMAP3_mean(),\n                          'wmap5_ml': cg.WMAP5_ML(),\n                          'wmap5_lss': cg.WMAP5_BAO_SN_mean(),\n                          'wmap7_lss': cg.WMAP7_BAO_H0_mean(),\n                          'planck13': cg.Planck_2013(),\n                          'planck15': cg.Planck_2015()}\n\n    if isinstance(cosmology, dict):\n        # User providing their own variables\n        cosmo = cosmology\n        if 'A_scaling' not in cosmology.keys():\n            A_scaling = getAscaling(cosmology, newcosmo=True)\n            cosmo.update({'A_scaling': A_scaling})\n\n        # Add extra variables by hand that cosmolopy requires\n        # note that they aren't used (set to zero)\n        for paramnames in cg.WMAP5_mean().keys():\n            if paramnames not in cosmology.keys():\n                cosmo.update({paramnames: 0})\n    elif cosmology.lower() in defaultcosmologies.keys():\n        # Load by name of cosmology instead\n        cosmo = defaultcosmologies[cosmology.lower()]\n        A_scaling = getAscaling(cosmology)\n        cosmo.update({'A_scaling': A_scaling})\n    else:\n        print(\"You haven't passed a dict of cosmological parameters \")\n        print(\"OR a recognised cosmology, you gave %s\" % (cosmology))\n    # No idea why this has to be done by hand but should be O_k = 0\n    cosmo = cp.distance.set_omega_k_0(cosmo)\n\n    # Use the cosmology as **cosmo passed to cosmolopy routines\n    return(cosmo)"}
{"func_code_string": "def _getcosmoheader(cosmo):\n    \"\"\" Output the cosmology to a string for writing to file \"\"\"\n\n    cosmoheader = (\"# Cosmology (flat) Om:{0:.3f}, Ol:{1:.3f}, h:{2:.2f}, \"\n                   \"sigma8:{3:.3f}, ns:{4:.2f}\".format(\n                       cosmo['omega_M_0'], cosmo['omega_lambda_0'], cosmo['h'],\n                       cosmo['sigma_8'], cosmo['n']))\n\n    return(cosmoheader)"}
{"func_code_string": "def cduffy(z, M, vir='200crit', relaxed=True):\n    \"\"\" NFW conc from Duffy 08 Table 1 for halo mass and redshift\"\"\"\n\n    if(vir == '200crit'):\n        if relaxed:\n            params = [6.71, -0.091, -0.44]\n        else:\n            params = [5.71, -0.084, -0.47]\n    elif(vir == 'tophat'):\n        if relaxed:\n            params = [9.23, -0.090, -0.69]\n        else:\n            params = [7.85, -0.081, -0.71]\n    elif(vir == '200mean'):\n        if relaxed:\n            params = [11.93, -0.090, -0.99]\n        else:\n            params = [10.14, -0.081, -1.01]\n    else:\n        print(\"Didn't recognise the halo boundary definition provided %s\"\n              % (vir))\n\n    return(params[0] * ((M/(2e12/0.72))**params[1]) * ((1+z)**params[2]))"}
{"func_code_string": "def _delta_sigma(**cosmo):\n    \"\"\" Perturb best-fit constant of proportionality Ascaling for\n        rho_crit - rho_2 relation for unknown cosmology (Correa et al 2015c)\n\n    Parameters\n    ----------\n    cosmo : dict\n        Dictionary of cosmological parameters, similar in format to:\n        {'N_nu': 0,'Y_He': 0.24, 'h': 0.702, 'n': 0.963,'omega_M_0': 0.275,\n         'omega_b_0': 0.0458,'omega_lambda_0': 0.725,'omega_n_0': 0.0,\n         'sigma_8': 0.816, 't_0': 13.76, 'tau': 0.088,'z_reion': 10.6}\n\n    Returns\n    -------\n    float\n        The perturbed 'A' relation between rho_2 and rho_crit for the cosmology\n\n    Raises\n    ------\n\n    \"\"\"\n\n    M8_cosmo = cp.perturbation.radius_to_mass(8, **cosmo)\n    perturbed_A = (0.796/cosmo['sigma_8']) * \\\n                  (M8_cosmo/2.5e14)**((cosmo['n']-0.963)/6)\n    return(perturbed_A)"}
{"func_code_string": "def getAscaling(cosmology, newcosmo=None):\n    \"\"\" Returns the normalisation constant between\n        Rho_-2 and Rho_mean(z_formation) for a given cosmology\n\n    Parameters\n    ----------\n    cosmology : str or dict\n        Can be named cosmology, default WMAP7 (aka DRAGONS), or\n        DRAGONS, WMAP1, WMAP3, WMAP5, WMAP7, WMAP9, Planck13, Planck15\n        or dictionary similar in format to:\n        {'N_nu': 0,'Y_He': 0.24, 'h': 0.702, 'n': 0.963,'omega_M_0': 0.275,\n         'omega_b_0': 0.0458,'omega_lambda_0': 0.725,'omega_n_0': 0.0,\n         'sigma_8': 0.816, 't_0': 13.76, 'tau': 0.088,'z_reion': 10.6}\n    newcosmo : str, optional\n        If cosmology is not from predefined list have to perturbation\n        A_scaling variable. Defaults to None.\n\n    Returns\n    -------\n    float\n        The scaled 'A' relation between rho_2 and rho_crit for the cosmology\n\n    \"\"\"\n    # Values from Correa 15c\n    defaultcosmologies = {'dragons': 887, 'wmap1': 853, 'wmap3': 850,\n                          'wmap5': 887, 'wmap7': 887, 'wmap9': 950,\n                          'wmap1_lss': 853, 'wmap3_mean': 850,\n                          'wmap5_ml': 887, 'wmap5_lss': 887,\n                          'wmap7_lss': 887,\n                          'planck13': 880, 'planck15': 880}\n\n    if newcosmo:\n        # Scale from default WMAP5 cosmology using Correa et al 14b eqn C1\n        A_scaling = defaultcosmologies['wmap5'] * _delta_sigma(**cosmology)\n    else:\n        if cosmology.lower() in defaultcosmologies.keys():\n            A_scaling = defaultcosmologies[cosmology.lower()]\n        else:\n            print(\"Error, don't recognise your cosmology for A_scaling \")\n            print(\"You provided %s\" % (cosmology))\n\n    return(A_scaling)"}
{"func_code_string": "def _int_growth(z, **cosmo):\n    \"\"\" Returns integral of the linear growth factor from z=200 to z=z \"\"\"\n\n    zmax = 200\n\n    if hasattr(z, \"__len__\"):\n        for zval in z:\n            assert(zval < zmax)\n    else:\n        assert(z < zmax)\n\n    y, yerr = scipy.integrate.quad(\n        lambda z: (1 + z)/(cosmo['omega_M_0']*(1 + z)**3 +\n                           cosmo['omega_lambda_0'])**(1.5),\n        z, zmax)\n\n    return(y)"}
{"func_code_string": "def _deriv_growth(z, **cosmo):\n    \"\"\" Returns derivative of the linear growth factor at z\n        for a given cosmology **cosmo \"\"\"\n\n    inv_h = (cosmo['omega_M_0']*(1 + z)**3 + cosmo['omega_lambda_0'])**(-0.5)\n    fz = (1 + z) * inv_h**3\n\n    deriv_g = growthfactor(z, norm=True, **cosmo)*(inv_h**2) *\\\n        1.5 * cosmo['omega_M_0'] * (1 + z)**2 -\\\n        fz * growthfactor(z, norm=True, **cosmo)/_int_growth(z, **cosmo)\n\n    return(deriv_g)"}
{"func_code_string": "def growthfactor(z, norm=True, **cosmo):\n    \"\"\" Returns linear growth factor at a given redshift, normalised to z=0\n        by default, for a given cosmology\n\n    Parameters\n    ----------\n\n    z : float or numpy array\n        The redshift at which the growth factor should be calculated\n    norm : boolean, optional\n        If true then normalise the growth factor to z=0 case defaults True\n    cosmo : dict\n        Dictionary of cosmological parameters, similar in format to:\n        {'N_nu': 0,'Y_He': 0.24, 'h': 0.702, 'n': 0.963,'omega_M_0': 0.275,\n         'omega_b_0': 0.0458,'omega_lambda_0': 0.725,'omega_n_0': 0.0,\n         'sigma_8': 0.816, 't_0': 13.76, 'tau': 0.088,'z_reion': 10.6}\n\n    Returns\n    -------\n    float or numpy array\n        The growth factor at a range of redshifts 'z'\n\n    Raises\n    ------\n\n    \"\"\"\n    H = np.sqrt(cosmo['omega_M_0'] * (1 + z)**3 +\n                cosmo['omega_lambda_0'])\n    growthval = H * _int_growth(z, **cosmo)\n    if norm:\n        growthval /= _int_growth(0, **cosmo)\n\n    return(growthval)"}
{"func_code_string": "def _minimize_c(c, z=0, a_tilde=1, b_tilde=-1,\n                Ascaling=900, omega_M_0=0.25, omega_lambda_0=0.75):\n    \"\"\" Trial function to solve 2 eqns (17 and 18) from Correa et al. (2015c)\n        for 1 unknown, i.e. concentration, returned by a minimisation call \"\"\"\n\n    # Fn 1 (LHS of Eqn 18)\n\n    Y1 = np.log(2) - 0.5\n    Yc = np.log(1+c) - c/(1+c)\n    f1 = Y1/Yc\n\n    # Fn 2 (RHS of Eqn 18)\n\n    # Eqn 14 - Define the mean inner density\n    rho_2 = 200 * c**3 * Y1 / Yc\n\n    # Eqn 17 rearranged to solve for Formation Redshift\n    # essentially when universe had rho_2 density\n    zf = (((1 + z)**3 + omega_lambda_0/omega_M_0) *\n          (rho_2/Ascaling) - omega_lambda_0/omega_M_0)**(1/3) - 1\n\n    # RHS of Eqn 19\n    f2 = ((1 + zf - z)**a_tilde) * np.exp((zf - z) * b_tilde)\n\n    # LHS - RHS should be zero for the correct concentration\n    return(f1-f2)"}
{"func_code_string": "def formationz(c, z, Ascaling=900, omega_M_0=0.25, omega_lambda_0=0.75):\n    \"\"\" Rearrange eqn 18 from Correa et al (2015c) to return\n        formation redshift for a concentration at a given redshift\n\n    Parameters\n    ----------\n    c : float / numpy array\n        Concentration of halo\n    z : float / numpy array\n        Redshift of halo with concentration c\n    Ascaling : float\n        Cosmological dependent scaling between densities, use function\n        getAscaling('WMAP5') if unsure. Default is 900.\n    omega_M_0 : float\n        Mass density of the universe. Default is 0.25\n    omega_lambda_0 : float\n        Dark Energy density of the universe. Default is 0.75\n\n    Returns\n    -------\n    zf : float / numpy array\n        Formation redshift for halo of concentration 'c' at redshift 'z'\n\n    \"\"\"\n    Y1 = np.log(2) - 0.5\n    Yc = np.log(1+c) - c/(1+c)\n    rho_2 = 200*(c**3)*Y1/Yc\n\n    zf = (((1+z)**3 + omega_lambda_0/omega_M_0) *\n          (rho_2/Ascaling) - omega_lambda_0/omega_M_0)**(1/3) - 1\n\n    return(zf)"}
{"func_code_string": "def calc_ab(zi, Mi, **cosmo):\n    \"\"\" Calculate growth rate indices a_tilde and b_tilde\n\n    Parameters\n    ----------\n    zi : float\n        Redshift\n    Mi : float\n        Halo mass at redshift 'zi'\n    cosmo : dict\n        Dictionary of cosmological parameters, similar in format to:\n        {'N_nu': 0,'Y_He': 0.24, 'h': 0.702, 'n': 0.963,'omega_M_0': 0.275,\n         'omega_b_0': 0.0458,'omega_lambda_0': 0.725,'omega_n_0': 0.0,\n         'sigma_8': 0.816, 't_0': 13.76, 'tau': 0.088,'z_reion': 10.6}\n\n    Returns\n    -------\n    (a_tilde, b_tilde) : float\n    \"\"\"\n\n    # When zi = 0, the a_tilde becomes alpha and b_tilde becomes beta\n\n    # Eqn 23 of Correa et al 2015a (analytically solve from Eqn 16 and 17)\n    # Arbitray formation redshift, z_-2 in COM is more physically motivated\n    zf = -0.0064 * (np.log10(Mi))**2 + 0.0237 * (np.log10(Mi)) + 1.8837\n\n    # Eqn 22 of Correa et al 2015a\n    q = 4.137 * zf**(-0.9476)\n\n    # Radius of a mass Mi\n    R_Mass = cp.perturbation.mass_to_radius(Mi, **cosmo)  # [Mpc]\n    # Radius of a mass Mi/q\n    Rq_Mass = cp.perturbation.mass_to_radius(Mi/q, **cosmo)  # [Mpc]\n\n    # Mass variance 'sigma' evaluate at z=0 to a good approximation\n    sig, err_sig = cp.perturbation.sigma_r(R_Mass, 0, **cosmo)  # [Mpc]\n    sigq, err_sigq = cp.perturbation.sigma_r(Rq_Mass, 0, **cosmo)  # [Mpc]\n\n    f = (sigq**2 - sig**2)**(-0.5)\n\n    # Eqn 9 and 10 from Correa et al 2015c\n    # (generalised to zi from Correa et al 2015a's z=0 special case)\n    # a_tilde is power law growth rate\n    a_tilde = (np.sqrt(2/np.pi) * 1.686 * _deriv_growth(zi, **cosmo) /\n               growthfactor(zi, norm=True, **cosmo)**2 + 1)*f\n    # b_tilde is exponential growth rate\n    b_tilde = -f\n\n    return(a_tilde, b_tilde)"}
{"func_code_string": "def acc_rate(z, zi, Mi, **cosmo):\n    \"\"\" Calculate accretion rate and mass history of a halo at any\n        redshift 'z' with mass 'Mi' at a lower redshift 'z'\n\n    Parameters\n    ----------\n    z : float\n        Redshift to solve acc_rate / mass history. Note zi<z\n    zi : float\n        Redshift\n    Mi : float\n        Halo mass at redshift 'zi'\n    cosmo : dict\n        Dictionary of cosmological parameters, similar in format to:\n        {'N_nu': 0,'Y_He': 0.24, 'h': 0.702, 'n': 0.963,'omega_M_0': 0.275,\n         'omega_b_0': 0.0458,'omega_lambda_0': 0.725,'omega_n_0': 0.0,\n         'sigma_8': 0.816, 't_0': 13.76, 'tau': 0.088,'z_reion': 10.6}\n\n    Returns\n    -------\n    (dMdt, Mz) : float\n        Accretion rate [Msol/yr], halo mass [Msol] at redshift 'z'\n\n    \"\"\"\n    # Find parameters a_tilde and b_tilde for initial redshift\n    # use Eqn 9 and 10 of Correa et al. (2015c)\n    a_tilde, b_tilde = calc_ab(zi, Mi, **cosmo)\n\n    # Halo mass at z, in Msol\n    # use Eqn 8 in Correa et al. (2015c)\n    Mz = Mi * ((1 + z - zi)**a_tilde) * (np.exp(b_tilde * (z - zi)))\n\n    # Accretion rate at z, Msol yr^-1\n    # use Eqn 11 from Correa et al. (2015c)\n    dMdt = 71.6 * (Mz/1e12) * (cosmo['h']/0.7) *\\\n        (-a_tilde / (1 + z - zi) - b_tilde) * (1 + z) *\\\n        np.sqrt(cosmo['omega_M_0']*(1 + z)**3+cosmo['omega_lambda_0'])\n\n    return(dMdt, Mz)"}
{"func_code_string": "def MAH(z, zi, Mi, **cosmo):\n    \"\"\" Calculate mass accretion history by looping function acc_rate\n        over redshift steps 'z' for halo of mass 'Mi' at redshift 'zi'\n\n    Parameters\n    ----------\n    z : float / numpy array\n        Redshift to output MAH over. Note zi<z always\n    zi : float\n        Redshift\n    Mi : float\n        Halo mass at redshift 'zi'\n    cosmo : dict\n        Dictionary of cosmological parameters, similar in format to:\n        {'N_nu': 0,'Y_He': 0.24, 'h': 0.702, 'n': 0.963,'omega_M_0': 0.275,\n         'omega_b_0': 0.0458,'omega_lambda_0': 0.725,'omega_n_0': 0.0,\n         'sigma_8': 0.816, 't_0': 13.76, 'tau': 0.088,'z_reion': 10.6}\n\n    Returns\n    -------\n    (dMdt, Mz) : float / numpy arrays of equivalent size to 'z'\n        Accretion rate [Msol/yr], halo mass [Msol] at redshift 'z'\n\n    \"\"\"\n\n    # Ensure that z is a 1D NumPy array\n    z = np.array(z, ndmin=1, dtype=float)\n\n    # Create a full array\n    dMdt_array = np.empty_like(z)\n    Mz_array = np.empty_like(z)\n\n    for i_ind, zval in enumerate(z):\n        # Solve the accretion rate and halo mass at each redshift step\n        dMdt, Mz = acc_rate(zval, zi, Mi, **cosmo)\n\n        dMdt_array[i_ind] = dMdt\n        Mz_array[i_ind] = Mz\n\n    return(dMdt_array, Mz_array)"}
{"func_code_string": "def COM(z, M, **cosmo):\n    \"\"\" Calculate concentration for halo of mass 'M' at redshift 'z'\n\n    Parameters\n    ----------\n    z : float / numpy array\n        Redshift to find concentration of halo\n    M : float / numpy array\n        Halo mass at redshift 'z'. Must be same size as 'z'\n    cosmo : dict\n        Dictionary of cosmological parameters, similar in format to:\n        {'N_nu': 0,'Y_He': 0.24, 'h': 0.702, 'n': 0.963,'omega_M_0': 0.275,\n         'omega_b_0': 0.0458,'omega_lambda_0': 0.725,'omega_n_0': 0.0,\n         'sigma_8': 0.816, 't_0': 13.76, 'tau': 0.088,'z_reion': 10.6}\n\n    Returns\n    -------\n    (c_array, sig_array, nu_array, zf_array) : float / numpy arrays\n        of equivalent size to 'z' and 'M'. Variables are\n        Concentration, Mass Variance 'sigma' this corresponds too,\n        the dimnesionless fluctuation this represents and formation redshift\n\n    \"\"\"\n    # Check that z and M are arrays\n    z = np.array(z, ndmin=1, dtype=float)\n    M = np.array(M, ndmin=1, dtype=float)\n\n    # Create array\n    c_array = np.empty_like(z)\n    sig_array = np.empty_like(z)\n    nu_array = np.empty_like(z)\n    zf_array = np.empty_like(z)\n\n    for i_ind, (zval, Mval) in enumerate(_izip(z, M)):\n        # Evaluate the indices at each redshift and mass combination\n        # that you want a concentration for, different to MAH which\n        # uses one a_tilde and b_tilde at the starting redshift only\n        a_tilde, b_tilde = calc_ab(zval, Mval, **cosmo)\n\n        # Minimize equation to solve for 1 unknown, 'c'\n        c = scipy.optimize.brentq(_minimize_c, 2, 1000,\n                                  args=(zval, a_tilde, b_tilde,\n                                        cosmo['A_scaling'], cosmo['omega_M_0'],\n                                        cosmo['omega_lambda_0']))\n\n        if np.isclose(c, 0):\n            print(\"Error solving for concentration with given redshift and \"\n                  \"(probably) too small a mass\")\n            c = -1\n            sig = -1\n            nu = -1\n            zf = -1\n        else:\n            # Calculate formation redshift for this concentration,\n            # redshift at which the scale radius = virial radius: z_-2\n            zf = formationz(c, zval, Ascaling=cosmo['A_scaling'],\n                            omega_M_0=cosmo['omega_M_0'],\n                            omega_lambda_0=cosmo['omega_lambda_0'])\n\n            R_Mass = cp.perturbation.mass_to_radius(Mval, **cosmo)\n\n            sig, err_sig = cp.perturbation.sigma_r(R_Mass, 0, **cosmo)\n            nu = 1.686/(sig*growthfactor(zval, norm=True, **cosmo))\n\n        c_array[i_ind] = c\n        sig_array[i_ind] = sig\n        nu_array[i_ind] = nu\n        zf_array[i_ind] = zf\n\n    return(c_array, sig_array, nu_array, zf_array)"}
{"func_code_string": "def run(cosmology, zi=0, Mi=1e12, z=False, com=True, mah=True,\n        filename=None, verbose=None, retcosmo=None):\n    \"\"\" Run commah code on halo of mass 'Mi' at redshift 'zi' with\n        accretion and profile history at higher redshifts 'z'\n        This is based on Correa et al. (2015a,b,c)\n\n    Parameters\n    ----------\n    cosmology : str or dict\n        Can be named cosmology, default WMAP7 (aka DRAGONS), or\n        DRAGONS, WMAP1, WMAP3, WMAP5, WMAP7, WMAP9, Planck13, Planck15\n        or dictionary similar in format to:\n        {'N_nu': 0,'Y_He': 0.24, 'h': 0.702, 'n': 0.963,'omega_M_0': 0.275,\n         'omega_b_0': 0.0458,'omega_lambda_0': 0.725,'omega_n_0': 0.0,\n         'sigma_8': 0.816, 't_0': 13.76, 'tau': 0.088,'z_reion': 10.6}\n\n    zi : float / numpy array, optional\n        Redshift at which halo has mass 'Mi'. If float then all\n        halo masses 'Mi' are assumed to be at this redshift.\n        If array but Mi is float, then this halo mass is used across\n        all starting redshifts. If both Mi and zi are arrays then they\n        have to be the same size for one - to - one correspondence between\n        halo mass and the redshift at which it has that mass. Default is 0.\n    Mi : float / numpy array, optional\n        Halo mass 'Mi' at a redshift 'zi'. If float then all redshifts 'zi'\n        are solved for this halo mass. If array but zi is float, then this\n        redshift is applied to all halo masses. If both Mi and zi are\n        arrays then they have to be the same size for one - to - one\n        correspondence between halo mass and the redshift at which it\n        has that mass. Default is 1e12 Msol.\n    z : float / numpy array, optional\n        Redshift to solve commah code at. Must have zi<z else these steps\n        are skipped. Default is False, meaning commah is solved at z=zi\n\n    com : bool, optional\n        If true then solve for concentration-mass,\n        default is True.\n    mah : bool, optional\n        If true then solve for accretion rate and halo mass history,\n        default is True.\n    filename : bool / str, optional\n        If str is passed this is used as a filename for output of commah\n    verbose : bool, optional\n        If true then give comments, default is None.\n    retcosmo : bool, optional\n        Return cosmological parameters used as a dict if retcosmo = True,\n        default is None.\n\n    Returns\n    -------\n    dataset : structured dataset\n        dataset contains structured columns of size\n        (size(Mi) > size(z)) by size(z)\n\n        If mah = True and com = False then columns are\n        ('zi',float),('Mi',float),('z',float),('dMdt',float),('Mz',float)\n        where 'zi' is the starting redshift, 'Mi' is halo mass at zi\n        'z' is output redshift (NB z>zi), 'dMdt' is accretion rate [Msol/yr]\n        and 'Mz' is the halo mass at 'z' for a halo which was 'Mi' massive\n        at starting redshift 'zi'\n\n        If mah = False and com = True then columns are\n        ('zi',float),('Mi',float),('z',float),('c',float),('sig',float),('nu',float),('zf',float)\n        where 'zi' is the starting redshift, 'Mi' is halo mass at zi\n        'z' is output redshift (NB z>zi), 'c' is NFW concentration of halo\n        at the redshift 'z', 'sig' is the mass variance 'sigma',\n        'nu' is the dimensionless fluctuation for halo mass 'Mi' at 'zi',\n        'zf' is the formation redshift for a halo of mass 'Mi' at redshift 'zi'\n\n        If mah = True and com = True then columns are:\n        ('zi',float),('Mi',float),('z',float),('dMdt',float),('Mz',float),\n        ('c',float),('sig',float),('nu',float),('zf',float)\n\n    file : structured dataset with name 'filename' if passed\n\n    Raises\n    ------\n    Output -1\n        If com = False and mah = False as user has to select something.\n    Output -1\n        If 'zi' and 'Mi' are arrays of unequal size. Impossible to match\n        corresponding masses and redshifts of output.\n\n    Examples\n    --------\n    Examples should be written in doctest format, and should illustrate how\n    to use the function.\n\n    >>> import examples\n    >>> examples.runcommands() #  A series of ways to query structured dataset\n    >>> examples.plotcommands() #  Examples to plot data\n\n    \"\"\"\n\n    # Check user choices...\n    if not com and not mah:\n        print(\"User has to choose com=True and / or mah=True \")\n        return(-1)\n\n    # Convert arrays / lists to np.array\n    # and inflate redshift / mass axis\n    # to match each other for later loop\n    results = _checkinput(zi, Mi, z=z, verbose=verbose)\n\n    # Return if results is -1\n    if(results == -1):\n        return(-1)\n    # If not, unpack the returned iterable\n    else:\n        zi, Mi, z, lenz, lenm, lenzout = results\n    # At this point we will have lenm objects to iterate over\n\n    # Get the cosmological parameters for the given cosmology\n    cosmo = getcosmo(cosmology)\n\n    # Create  output file if desired\n    if filename:\n        print(\"Output to file %r\" % (filename))\n        fout = open(filename, 'wb')\n\n    # Create the structured dataset\n    try:\n        if mah and com:\n            if verbose:\n                print(\"Output requested is zi, Mi, z, dMdt, Mz, c, sig, nu, \"\n                      \"zf\")\n            if filename:\n                fout.write(_getcosmoheader(cosmo)+'\\n')\n                fout.write(\"# Initial z - Initial Halo  - Output z - \"\n                           \" Accretion -  Final Halo  - concentration - \"\n                           \"    Mass   -    Peak    -  Formation z \"+'\\n')\n                fout.write(\"#           -     mass      -          -\"\n                           \"    rate    -     mass     -               - \"\n                           \" Variance  -   Height   -              \"+'\\n')\n                fout.write(\"#           -    (M200)     -          - \"\n                           \"  (dM/dt)  -    (M200)    -               - \"\n                           \"  (sigma)  -    (nu)    -              \"+'\\n')\n                fout.write(\"#           -    [Msol]     -          - \"\n                           \" [Msol/yr] -    [Msol]    -               - \"\n                           \"           -            -              \"+'\\n')\n            dataset = np.zeros((lenm, lenzout), dtype=[('zi', float),\n                               ('Mi', float), ('z', float), ('dMdt', float),\n                               ('Mz', float), ('c', float), ('sig', float),\n                               ('nu', float), ('zf', float)])\n        elif mah:\n            if verbose:\n                print(\"Output requested is zi, Mi, z, dMdt, Mz\")\n            if filename:\n                fout.write(_getcosmoheader(cosmo)+'\\n')\n                fout.write(\"# Initial z - Initial Halo  - Output z -\"\n                           \"   Accretion - Final Halo \"+'\\n')\n                fout.write(\"#           -     mass      -          -\"\n                           \"     rate    -   mass     \"+'\\n')\n                fout.write(\"#           -    (M200)     -          -\"\n                           \"    (dm/dt)  -  (M200)    \"+'\\n')\n                fout.write(\"#           -    [Msol]     -          -\"\n                           \"   [Msol/yr] -  [Msol]    \"+'\\n')\n            dataset = np.zeros((lenm, lenzout), dtype=[('zi', float),\n                               ('Mi', float), ('z', float),\n                               ('dMdt', float), ('Mz', float)])\n        else:\n            if verbose:\n                print(\"Output requested is zi, Mi, z, c, sig, nu, zf\")\n            if filename:\n                fout.write(_getcosmoheader(cosmo)+'\\n')\n                fout.write(\"# Initial z - Initial Halo  - Output z - \"\n                           \" concentration - \"\n                           \"  Mass    -    Peak    -  Formation z \"+'\\n')\n                fout.write(\"#           -     mass      -          -\"\n                           \"                -\"\n                           \" Variance  -   Height   -              \"+'\\n')\n                fout.write(\"#           -   (M200)      -          - \"\n                           \"               - \"\n                           \" (sigma)  -    (nu)    -              \"+'\\n')\n                fout.write(\"#           -   [Msol]      -          - \"\n                           \"               - \"\n                           \"          -            -            \"+'\\n')\n            dataset = np.zeros((lenm, lenzout), dtype=[('zi', float),\n                               ('Mi', float), ('z', float), ('c', float),\n                               ('sig', float), ('nu', float), ('zf', float)])\n\n        # Now loop over the combination of initial redshift and halo mamss\n        for i_ind, (zval, Mval) in enumerate(_izip(zi, Mi)):\n            if verbose:\n                print(\"Output Halo of Mass Mi=%s at zi=%s\" % (Mval, zval))\n            # For a given halo mass Mi at redshift zi need to know\n            # output redshifts 'z'\n            # Check that all requested redshifts are greater than\n            # input redshift, except if z is False, in which case\n            # only solve z at zi, i.e. remove a loop\n            if z is False:\n                ztemp = np.array(zval, ndmin=1, dtype=float)\n            else:\n                ztemp = np.array(z[z >= zval], dtype=float)\n\n            # Loop over the output redshifts\n            if ztemp.size:\n                # Return accretion rates and halo mass progenitors at\n                # redshifts 'z' for object of mass Mi at zi\n                dMdt, Mz = MAH(ztemp, zval, Mval, **cosmo)\n                if mah and com:\n                    # More expensive to return concentrations\n                    c, sig, nu, zf = COM(ztemp, Mz, **cosmo)\n                    # Save all arrays\n                    for j_ind, j_val in enumerate(ztemp):\n                        dataset[i_ind, j_ind] =\\\n                            (zval, Mval, ztemp[j_ind], dMdt[j_ind], Mz[j_ind],\n                             c[j_ind], sig[j_ind], nu[j_ind], zf[j_ind])\n                        if filename:\n                            fout.write(\n                                \"{}, {}, {}, {}, {}, {}, {}, {}, {} \\n\".format(\n                                    zval, Mval, ztemp[j_ind], dMdt[j_ind],\n                                    Mz[j_ind], c[j_ind], sig[j_ind], nu[j_ind],\n                                    zf[j_ind]))\n                elif mah:\n                    # Save only MAH arrays\n                    for j_ind, j_val in enumerate(ztemp):\n                        dataset[i_ind, j_ind] =\\\n                            (zval, Mval, ztemp[j_ind], dMdt[j_ind], Mz[j_ind])\n                        if filename:\n                            fout.write(\"{}, {}, {}, {}, {} \\n\".format(\n                                zval, Mval, ztemp[j_ind], dMdt[j_ind],\n                                Mz[j_ind]))\n                else:\n                    # Output only COM arrays\n                    c, sig, nu, zf = COM(ztemp, Mz, **cosmo)\n                    # For any halo mass Mi at redshift zi\n                    # solve for c, sig, nu and zf\n                    for j_ind, j_val in enumerate(ztemp):\n                        dataset[i_ind, j_ind] =\\\n                            (zval, Mval, ztemp[j_ind], c[j_ind], sig[j_ind],\n                             nu[j_ind], zf[j_ind])\n                        if filename:\n                            fout.write(\"{}, {}, {}, {}, {}, {}, {} \\n\".format(\n                                zval, Mval, ztemp[j_ind], c[j_ind], sig[j_ind],\n                                nu[j_ind], zf[j_ind]))\n\n    # Make sure to close the file if it was opened\n    finally:\n        fout.close() if filename else None\n\n    if retcosmo:\n        return(dataset, cosmo)\n    else:\n        return(dataset)"}
{"func_code_string": "def load(config_path: str):\n    \"\"\"\n    Load a configuration and keep it alive for the given context\n\n    :param config_path: path to a configuration file\n    \"\"\"\n    # we bind the config to _ to keep it alive\n    if os.path.splitext(config_path)[1] in ('.yaml', '.yml'):\n        _ = load_yaml_configuration(config_path, translator=PipelineTranslator())\n    elif os.path.splitext(config_path)[1] == '.py':\n        _ = load_python_configuration(config_path)\n    else:\n        raise ValueError('Unknown configuration extension: %r' % os.path.splitext(config_path)[1])\n    yield"}
{"func_code_string": "def transform_source(text):\n    '''Replaces instances of\n\n        repeat n:\n    by\n\n        for __VAR_i in range(n):\n\n    where __VAR_i is a string that does not appear elsewhere\n    in the code sample.\n    '''\n\n    loop_keyword = 'repeat'\n\n    nb = text.count(loop_keyword)\n    if nb == 0:\n        return text\n\n    var_names = get_unique_variable_names(text, nb)\n\n    toks = tokenize.generate_tokens(StringIO(text).readline)\n    result = []\n    replacing_keyword = False\n    for toktype, tokvalue, _, _, _ in toks:\n        if toktype == tokenize.NAME and tokvalue == loop_keyword:\n            result.extend([\n                (tokenize.NAME, 'for'),\n                (tokenize.NAME, var_names.pop()),\n                (tokenize.NAME, 'in'),\n                (tokenize.NAME, 'range'),\n                (tokenize.OP, '(')\n            ])\n            replacing_keyword = True\n        elif replacing_keyword and tokvalue == ':':\n            result.extend([\n                (tokenize.OP, ')'),\n                (tokenize.OP, ':')\n            ])\n            replacing_keyword = False\n        else:\n            result.append((toktype, tokvalue))\n    return tokenize.untokenize(result)"}
{"func_code_string": "def get_unique_variable_names(text, nb):\n    '''returns a list of possible variables names that\n       are not found in the original text.'''\n    base_name = '__VAR_'\n    var_names = []\n    i = 0\n    j = 0\n    while j < nb:\n        tentative_name = base_name + str(i)\n        if text.count(tentative_name) == 0 and tentative_name not in ALL_NAMES:\n            var_names.append(tentative_name)\n            ALL_NAMES.append(tentative_name)\n            j += 1\n        i += 1\n    return var_names"}
{"func_code_string": "def tag(self, tag):\n        \"\"\"Get a release by tag\n        \"\"\"\n        url = '%s/tags/%s' % (self, tag)\n        response = self.http.get(url, auth=self.auth)\n        response.raise_for_status()\n        return response.json()"}
{"func_code_string": "def release_assets(self, release):\n        \"\"\"Assets for a given release\n        \"\"\"\n        release = self.as_id(release)\n        return self.get_list(url='%s/%s/assets' % (self, release))"}
{"func_code_string": "def upload(self, release, filename, content_type=None):\n        \"\"\"Upload a file to a release\n\n        :param filename: filename to upload\n        :param content_type: optional content type\n        :return: json object from github\n        \"\"\"\n        release = self.as_id(release)\n        name = os.path.basename(filename)\n        if not content_type:\n            content_type, _ = mimetypes.guess_type(name)\n        if not content_type:\n            raise ValueError('content_type not known')\n        inputs = {'name': name}\n        url = '%s%s/%s/assets' % (self.uploads_url,\n                                  urlsplit(self.api_url).path,\n                                  release)\n        info = os.stat(filename)\n        size = info[stat.ST_SIZE]\n        response = self.http.post(\n            url, data=stream_upload(filename), auth=self.auth,\n            params=inputs,\n            headers={'content-type': content_type,\n                     'content-length': str(size)})\n        response.raise_for_status()\n        return response.json()"}
{"func_code_string": "def validate_tag(self, tag_name, prefix=None):\n        \"\"\"Validate ``tag_name`` with the latest tag from github\n\n        If ``tag_name`` is a valid candidate, return the latest tag from github\n        \"\"\"\n        new_version = semantic_version(tag_name)\n        current = self.latest()\n        if current:\n            tag_name = current['tag_name']\n            if prefix:\n                tag_name = tag_name[len(prefix):]\n            tag_name = semantic_version(tag_name)\n            if tag_name >= new_version:\n                what = 'equal to' if tag_name == new_version else 'older than'\n                raise GithubException(\n                    'Your local version \"%s\" is %s '\n                    'the current github version \"%s\".\\n'\n                    'Bump the local version to '\n                    'continue.' %\n                    (\n                        str(new_version),\n                        what,\n                        str(tag_name)\n                    )\n                )\n        return current"}
{"func_code_string": "def full_url(self):\n        \"\"\"Return the full reddit URL associated with the usernote.\n\n        Arguments:\n            subreddit: the subreddit name for the note (PRAW Subreddit object)\n        \"\"\"\n        if self.link == '':\n            return None\n        else:\n            return Note._expand_url(self.link, self.subreddit)"}
{"func_code_string": "def _compress_url(link):\n        \"\"\"Convert a reddit URL into the short-hand used by usernotes.\n\n        Arguments:\n            link: a link to a comment, submission, or message (str)\n\n        Returns a String of the shorthand URL\n        \"\"\"\n        comment_re = re.compile(r'/comments/([A-Za-z\\d]{2,})(?:/[^\\s]+/([A-Za-z\\d]+))?')\n        message_re = re.compile(r'/message/messages/([A-Za-z\\d]+)')\n        matches = re.findall(comment_re, link)\n\n        if len(matches) == 0:\n            matches = re.findall(message_re, link)\n\n            if len(matches) == 0:\n                return None\n            else:\n                return 'm,' + matches[0]\n        else:\n            if matches[0][1] == '':\n                return 'l,' + matches[0][0]\n            else:\n                return 'l,' + matches[0][0] + ',' + matches[0][1]"}
{"func_code_string": "def _expand_url(short_link, subreddit=None):\n        \"\"\"Convert a usernote's URL short-hand into a full reddit URL.\n\n        Arguments:\n            subreddit: the subreddit the URL is for (PRAW Subreddit object or str)\n            short_link: the compressed link from a usernote (str)\n\n        Returns a String of the full URL.\n        \"\"\"\n        # Some URL structures for notes\n        message_scheme = 'https://reddit.com/message/messages/{}'\n        comment_scheme = 'https://reddit.com/r/{}/comments/{}/-/{}'\n        post_scheme = 'https://reddit.com/r/{}/comments/{}/'\n\n        if short_link == '':\n            return None\n        else:\n            parts = short_link.split(',')\n\n            if parts[0] == 'm':\n                return message_scheme.format(parts[1])\n            if parts[0] == 'l' and subreddit:\n                if len(parts) > 2:\n                    return comment_scheme.format(subreddit, parts[1], parts[2])\n                else:\n                    return post_scheme.format(subreddit, parts[1])\n            elif not subreddit:\n                raise ValueError('Subreddit name must be provided')\n            else:\n                return None"}
{"func_code_string": "def get_json(self):\n        \"\"\"Get the JSON stored on the usernotes wiki page.\n\n        Returns a dict representation of the usernotes (with the notes BLOB\n        decoded).\n\n        Raises:\n            RuntimeError if the usernotes version is incompatible with this\n                version of puni.\n        \"\"\"\n        try:\n            usernotes = self.subreddit.wiki[self.page_name].content_md\n            notes = json.loads(usernotes)\n        except NotFound:\n            self._init_notes()\n        else:\n            if notes['ver'] != self.schema:\n                raise RuntimeError(\n                    'Usernotes schema is v{0}, puni requires v{1}'.\n                    format(notes['ver'], self.schema)\n                )\n\n            self.cached_json = self._expand_json(notes)\n\n        return self.cached_json"}
{"func_code_string": "def _init_notes(self):\n        \"\"\"Set up the UserNotes page with the initial JSON schema.\"\"\"\n        self.cached_json = {\n            'ver': self.schema,\n            'users': {},\n            'constants': {\n                'users': [x.name for x in self.subreddit.moderator()],\n                'warnings': Note.warnings\n            }\n        }\n\n        self.set_json('Initializing JSON via puni', True)"}
{"func_code_string": "def set_json(self, reason='', new_page=False):\n        \"\"\"Send the JSON from the cache to the usernotes wiki page.\n\n        Arguments:\n            reason: the change reason that will be posted to the wiki changelog\n                (str)\n        Raises:\n            OverflowError if the new JSON data is greater than max_page_size\n        \"\"\"\n        compressed_json = json.dumps(self._compress_json(self.cached_json))\n\n        if len(compressed_json) > self.max_page_size:\n            raise OverflowError(\n                'Usernotes page is too large (>{0} characters)'.\n                format(self.max_page_size)\n            )\n\n        if new_page:\n            self.subreddit.wiki.create(\n                self.page_name,\n                compressed_json,\n                reason\n            )\n            # Set the page as hidden and available to moderators only\n            self.subreddit.wiki[self.page_name].mod.update(False, permlevel=2)\n        else:\n            self.subreddit.wiki[self.page_name].edit(\n                compressed_json,\n                reason\n            )"}
{"func_code_string": "def get_notes(self, user):\n        \"\"\"Return a list of Note objects for the given user.\n\n        Return an empty list if no notes are found.\n\n        Arguments:\n            user: the user to search for in the usernotes (str)\n        \"\"\"\n        # Try to search for all notes on a user, return an empty list if none\n        # are found.\n        try:\n            users_notes = []\n\n            for note in self.cached_json['users'][user]['ns']:\n                users_notes.append(Note(\n                    user=user,\n                    note=note['n'],\n                    subreddit=self.subreddit,\n                    mod=self._mod_from_index(note['m']),\n                    link=note['l'],\n                    warning=self._warning_from_index(note['w']),\n                    note_time=note['t']\n                ))\n\n            return users_notes\n        except KeyError:\n            # User not found\n            return []"}
{"func_code_string": "def _expand_json(self, j):\n        \"\"\"Decompress the BLOB portion of the usernotes.\n\n        Arguments:\n            j: the JSON returned from the wiki page (dict)\n\n        Returns a Dict with the 'blob' key removed and a 'users' key added\n        \"\"\"\n        decompressed_json = copy.copy(j)\n        decompressed_json.pop('blob', None)  # Remove BLOB portion of JSON\n\n        # Decode and decompress JSON\n        compressed_data = base64.b64decode(j['blob'])\n        original_json = zlib.decompress(compressed_data).decode('utf-8')\n\n        decompressed_json['users'] = json.loads(original_json)  # Insert users\n\n        return decompressed_json"}
{"func_code_string": "def _compress_json(self, j):\n        \"\"\"Compress the BLOB data portion of the usernotes.\n\n        Arguments:\n            j: the JSON in Schema v5 format (dict)\n\n        Returns a dict with the 'users' key removed and 'blob' key added\n        \"\"\"\n        compressed_json = copy.copy(j)\n        compressed_json.pop('users', None)\n\n        compressed_data = zlib.compress(\n            json.dumps(j['users']).encode('utf-8'),\n            self.zlib_compression_strength\n        )\n        b64_data = base64.b64encode(compressed_data).decode('utf-8')\n\n        compressed_json['blob'] = b64_data\n\n        return compressed_json"}
{"func_code_string": "def add_note(self, note):\n        \"\"\"Add a note to the usernotes wiki page.\n\n        Arguments:\n            note: the note to be added (Note)\n\n        Returns the update message for the usernotes wiki\n\n        Raises:\n            ValueError when the warning type of the note can not be found in the\n                stored list of warnings.\n        \"\"\"\n        notes = self.cached_json\n\n        if not note.moderator:\n            note.moderator = self.r.user.me().name\n\n        # Get index of moderator in mod list from usernotes\n        # Add moderator to list if not already there\n        try:\n            mod_index = notes['constants']['users'].index(note.moderator)\n        except ValueError:\n            notes['constants']['users'].append(note.moderator)\n            mod_index = notes['constants']['users'].index(note.moderator)\n\n        # Get index of warning type from warnings list\n        # Add warning type to list if not already there\n        try:\n            warn_index = notes['constants']['warnings'].index(note.warning)\n        except ValueError:\n            if note.warning in Note.warnings:\n                notes['constants']['warnings'].append(note.warning)\n                warn_index = notes['constants']['warnings'].index(note.warning)\n            else:\n                raise ValueError('Warning type not valid: ' + note.warning)\n\n        new_note = {\n            'n': note.note,\n            't': note.time,\n            'm': mod_index,\n            'l': note.link,\n            'w': warn_index\n        }\n\n        try:\n            notes['users'][note.username]['ns'].insert(0, new_note)\n        except KeyError:\n            notes['users'][note.username] = {'ns': [new_note]}\n\n        return '\"create new note on user {}\" via puni'.format(note.username)"}
{"func_code_string": "def remove_note(self, username, index):\n        \"\"\"Remove a single usernote from the usernotes.\n\n        Arguments:\n            username: the user that for whom you're removing a note (str)\n            index: the index of the note which is to be removed (int)\n\n        Returns the update message for the usernotes wiki\n        \"\"\"\n        self.cached_json['users'][username]['ns'].pop(index)\n\n        # Go ahead and remove the user's entry if they have no more notes left\n        if len(self.cached_json['users'][username]['ns']) == 0:\n            del self.cached_json['users'][username]\n\n        return '\"delete note #{} on user {}\" via puni'.format(index, username)"}
{"func_code_string": "def load(self):\n        \"\"\" Function load\n        Get the list of all objects\n\n        @return RETURN: A ForemanItem list\n        \"\"\"\n        cl_tmp = self.api.list(self.objName, limit=self.searchLimit).values()\n        cl = []\n        for i in cl_tmp:\n            cl.extend(i)\n        return {x[self.index]: ItemPuppetClass(self.api, x['id'],\n                                               self.objName, self.payloadObj,\n                                               x)\n                for x in cl}"}
{"func_code_string": "def is_related_to(item, app_id, app_ver=None):\n    \"\"\"Return True if the item relates to the given app_id (and app_ver, if passed).\"\"\"\n    versionRange = item.get('versionRange')\n    if not versionRange:\n        return True\n\n    for vR in versionRange:\n        if not vR.get('targetApplication'):\n            return True\n        if get_related_targetApplication(vR, app_id, app_ver) is not None:\n            return True\n    return False"}
{"func_code_string": "def get_related_targetApplication(vR, app_id, app_ver):\n    \"\"\"Return the first matching target application in this version range.\n    Returns None if there are no target applications or no matching ones.\"\"\"\n    targetApplication = vR.get('targetApplication')\n    if not targetApplication:\n        return None\n\n    for tA in targetApplication:\n        guid = tA.get('guid')\n        if not guid or guid == app_id:\n            if not app_ver:\n                return tA\n            # We purposefully use maxVersion only, so that the blocklist contains items\n            # whose minimum version is ahead of the version we get passed. This means\n            # the blocklist we serve is \"future-proof\" for app upgrades.\n            if between(version_int(app_ver), '0', tA.get('maxVersion', '*')):\n                return tA\n\n    return None"}
{"func_code_string": "def write_addons_items(xml_tree, records, app_id, api_ver=3, app_ver=None):\n    \"\"\"Generate the addons blocklists.\n\n    <emItem blockID=\"i372\" id=\"5nc3QHFgcb@r06Ws9gvNNVRfH.com\">\n      <versionRange minVersion=\"0\" maxVersion=\"*\" severity=\"3\">\n        <targetApplication id=\"{ec8030f7-c20a-464f-9b0e-13a3a9e97384}\">\n          <versionRange minVersion=\"39.0a1\" maxVersion=\"*\"/>\n        </targetApplication>\n      </versionRange>\n      <prefs>\n        <pref>browser.startup.homepage</pref>\n        <pref>browser.search.defaultenginename</pref>\n      </prefs>\n    </emItem>\n    \"\"\"\n    if not records:\n        return\n\n    emItems = etree.SubElement(xml_tree, 'emItems')\n    groupby = {}\n    for item in records:\n        if is_related_to(item, app_id, app_ver):\n            if item['guid'] in groupby:\n                emItem = groupby[item['guid']]\n                # When creating new records from the Kinto Admin we don't have proper blockID.\n                if 'blockID' in item:\n                    # Remove the first caracter which is the letter i to\n                    # compare the numeric value i45 < i356.\n                    current_blockID = int(item['blockID'][1:])\n                    previous_blockID = int(emItem.attrib['blockID'][1:])\n                    # Group by and keep the biggest blockID in the XML file.\n                    if current_blockID > previous_blockID:\n                        emItem.attrib['blockID'] = item['blockID']\n                else:\n                    # If the latest entry does not have any blockID attribute, its\n                    # ID should be used. (the list of records is sorted by ascending\n                    # last_modified).\n                    # See https://bugzilla.mozilla.org/show_bug.cgi?id=1473194\n                    emItem.attrib['blockID'] = item['id']\n            else:\n                emItem = etree.SubElement(emItems, 'emItem',\n                                          blockID=item.get('blockID', item['id']))\n                groupby[item['guid']] = emItem\n                prefs = etree.SubElement(emItem, 'prefs')\n                for p in item['prefs']:\n                    pref = etree.SubElement(prefs, 'pref')\n                    pref.text = p\n\n            # Set the add-on ID\n            emItem.set('id', item['guid'])\n\n            for field in ['name', 'os']:\n                if field in item:\n                    emItem.set(field, item[field])\n\n            build_version_range(emItem, item, app_id)"}
{"func_code_string": "def write_plugin_items(xml_tree, records, app_id, api_ver=3, app_ver=None):\n    \"\"\"Generate the plugin blocklists.\n\n    <pluginItem blockID=\"p422\">\n        <match name=\"filename\" exp=\"JavaAppletPlugin\\\\.plugin\"/>\n        <versionRange minVersion=\"Java 7 Update 16\"\n                      maxVersion=\"Java 7 Update 24\"\n                      severity=\"0\" vulnerabilitystatus=\"1\">\n            <targetApplication id=\"{ec8030f7-c20a-464f-9b0e-13a3a9e97384}\">\n                <versionRange minVersion=\"17.0\" maxVersion=\"*\"/>\n            </targetApplication>\n        </versionRange>\n    </pluginItem>\n    \"\"\"\n\n    if not records:\n        return\n\n    pluginItems = etree.SubElement(xml_tree, 'pluginItems')\n    for item in records:\n        for versionRange in item.get('versionRange', []):\n            if not versionRange.get('targetApplication'):\n                add_plugin_item(pluginItems, item, versionRange,\n                                app_id=app_id, api_ver=api_ver,\n                                app_ver=app_ver)\n            else:\n                targetApplication = get_related_targetApplication(versionRange, app_id, app_ver)\n                if targetApplication is not None:\n                    add_plugin_item(pluginItems, item, versionRange, targetApplication,\n                                    app_id=app_id, api_ver=api_ver,\n                                    app_ver=app_ver)"}
{"func_code_string": "def write_gfx_items(xml_tree, records, app_id, api_ver=3):\n    \"\"\"Generate the gfxBlacklistEntry.\n\n    <gfxBlacklistEntry blockID=\"g35\">\n        <os>WINNT 6.1</os>\n        <vendor>0x10de</vendor>\n        <devices>\n            <device>0x0a6c</device>\n        </devices>\n        <feature>DIRECT2D</feature>\n        <featureStatus>BLOCKED_DRIVER_VERSION</featureStatus>\n        <driverVersion>8.17.12.5896</driverVersion>\n        <driverVersionComparator>LESS_THAN_OR_EQUAL</driverVersionComparator>\n        <versionRange minVersion=\"3.2\" maxVersion=\"3.4\" />\n    </gfxBlacklistEntry>\n    \"\"\"\n    if not records:\n        return\n\n    gfxItems = etree.SubElement(xml_tree, 'gfxItems')\n    for item in records:\n        is_record_related = ('guid' not in item or item['guid'] == app_id)\n\n        if is_record_related:\n            entry = etree.SubElement(gfxItems, 'gfxBlacklistEntry',\n                                     blockID=item.get('blockID', item['id']))\n            fields = ['os', 'vendor', 'feature', 'featureStatus',\n                      'driverVersion', 'driverVersionComparator']\n            for field in fields:\n                if field in item:\n                    node = etree.SubElement(entry, field)\n                    node.text = item[field]\n\n            # Devices\n            if item['devices']:\n                devices = etree.SubElement(entry, 'devices')\n                for d in item['devices']:\n                    device = etree.SubElement(devices, 'device')\n                    device.text = d\n\n            if 'versionRange' in item:\n                version = item['versionRange']\n                versionRange = etree.SubElement(entry, 'versionRange')\n\n                for field in ['minVersion', 'maxVersion']:\n                    value = version.get(field)\n                    if value:\n                        versionRange.set(field, str(value))"}
{"func_code_string": "def write_cert_items(xml_tree, records, api_ver=3, app_id=None, app_ver=None):\n    \"\"\"Generate the certificate blocklists.\n\n    <certItem issuerName=\"MIGQMQswCQYD...IENB\">\n      <serialNumber>UoRGnb96CUDTxIqVry6LBg==</serialNumber>\n    </certItem>\n\n    or\n\n    <certItem subject='MCIxIDAeBgNVBAMMF0Fub3RoZXIgVGVzdCBFbmQtZW50aXR5'\n              pubKeyHash='VCIlmPM9NkgFQtrs4Oa5TeFcDu6MWRTKSNdePEhOgD8='>\n    </certItem>\n    \"\"\"\n    if not records or not should_include_certs(app_id, app_ver):\n        return\n\n    certItems = etree.SubElement(xml_tree, 'certItems')\n    for item in records:\n        if item.get('subject') and item.get('pubKeyHash'):\n            cert = etree.SubElement(certItems, 'certItem',\n                                    subject=item['subject'],\n                                    pubKeyHash=item['pubKeyHash'])\n        else:\n            cert = etree.SubElement(certItems, 'certItem',\n                                    issuerName=item['issuerName'])\n            serialNumber = etree.SubElement(cert, 'serialNumber')\n            serialNumber.text = item['serialNumber']"}
{"func_code_string": "def label(self, name, color, update=True):\n        \"\"\"Create or update a label\n        \"\"\"\n        url = '%s/labels' % self\n        data = dict(name=name, color=color)\n        response = self.http.post(\n            url, json=data, auth=self.auth, headers=self.headers\n        )\n        if response.status_code == 201:\n            return True\n        elif response.status_code == 422 and update:\n            url = '%s/%s' % (url, name)\n            response = self.http.patch(\n                url, json=data, auth=self.auth, headers=self.headers\n            )\n        response.raise_for_status()\n        return False"}
{"func_code_string": "def translate(source, dictionary):\n    '''A dictionary with a one-to-one translation of keywords is used\n    to provide the transformation.\n    '''\n    toks = tokenize.generate_tokens(StringIO(source).readline)\n    result = []\n    for toktype, tokvalue, _, _, _ in toks:\n        if toktype == tokenize.NAME and tokvalue in dictionary:\n            result.append((toktype, dictionary[tokvalue]))\n        else:\n            result.append((toktype, tokvalue))\n    return tokenize.untokenize(result)"}
{"func_code_string": "def enhance(self):\n        \"\"\" Function enhance\n        Enhance the object with new item or enhanced items\n        \"\"\"\n        self.update({'images':\n                     SubDict(self.api, self.objName,\n                             self.payloadObj, self.key,\n                             SubItemImages)})"}
{"func_code_string": "async def _run_payloads(self):\n        \"\"\"Async component of _run\"\"\"\n        delay = 0.0\n        try:\n            while self.running.is_set():\n                await self._start_payloads()\n                await self._reap_payloads()\n                await asyncio.sleep(delay)\n                delay = min(delay + 0.1, 1.0)\n        except Exception:\n            await self._cancel_payloads()\n            raise"}
{"func_code_string": "async def _start_payloads(self):\n        \"\"\"Start all queued payloads\"\"\"\n        with self._lock:\n            for coroutine in self._payloads:\n                task = self.event_loop.create_task(coroutine())\n                self._tasks.add(task)\n            self._payloads.clear()\n        await asyncio.sleep(0)"}
{"func_code_string": "async def _reap_payloads(self):\n        \"\"\"Clean up all finished payloads\"\"\"\n        for task in self._tasks.copy():\n            if task.done():\n                self._tasks.remove(task)\n                if task.exception() is not None:\n                    raise task.exception()\n        await asyncio.sleep(0)"}
{"func_code_string": "async def _cancel_payloads(self):\n        \"\"\"Cancel all remaining payloads\"\"\"\n        for task in self._tasks:\n            task.cancel()\n            await asyncio.sleep(0)\n        for task in self._tasks:\n            while not task.done():\n                await asyncio.sleep(0.1)\n                task.cancel()"}
{"func_code_string": "def check_password(password: str, encrypted: str) -> bool:\n    \"\"\" Check a plaintext password against a hashed password. \"\"\"\n    # some old passwords have {crypt} in lower case, and passlib wants it to be\n    # in upper case.\n    if encrypted.startswith(\"{crypt}\"):\n        encrypted = \"{CRYPT}\" + encrypted[7:]\n    return pwd_context.verify(password, encrypted)"}
{"func_code_string": "def validate(ctx, sandbox):\n    \"\"\"Check if version of repository is semantic\n    \"\"\"\n    m = RepoManager(ctx.obj['agile'])\n    if not sandbox or m.can_release('sandbox'):\n        click.echo(m.validate_version())"}
{"func_code_string": "def reset(self, force_flush_cache: bool = False) -> None:\n        \"\"\"\n        Reset transaction back to original state, discarding all\n        uncompleted transactions.\n        \"\"\"\n        super(LDAPwrapper, self).reset()\n        if len(self._transactions) == 0:\n            raise RuntimeError(\"reset called outside a transaction.\")\n        self._transactions[-1] = []"}
{"func_code_string": "def _cache_get_for_dn(self, dn: str) -> Dict[str, bytes]:\n        \"\"\"\n        Object state is cached. When an update is required the update will be\n        simulated on this cache, so that rollback information can be correct.\n        This function retrieves the cached data.\n        \"\"\"\n\n        # no cached item, retrieve from ldap\n        self._do_with_retry(\n            lambda obj: obj.search(\n                dn,\n                '(objectclass=*)',\n                ldap3.BASE,\n                attributes=['*', '+']))\n        results = self._obj.response\n        if len(results) < 1:\n            raise NoSuchObject(\"No results finding current value\")\n        if len(results) > 1:\n            raise RuntimeError(\"Too many results finding current value\")\n\n        return results[0]['raw_attributes']"}
{"func_code_string": "def is_dirty(self) -> bool:\n        \"\"\" Are there uncommitted changes? \"\"\"\n        if len(self._transactions) == 0:\n            raise RuntimeError(\"is_dirty called outside a transaction.\")\n        if len(self._transactions[-1]) > 0:\n            return True\n        return False"}
{"func_code_string": "def leave_transaction_management(self) -> None:\n        \"\"\"\n        End a transaction. Must not be dirty when doing so. ie. commit() or\n        rollback() must be called if changes made. If dirty, changes will be\n        discarded.\n        \"\"\"\n        if len(self._transactions) == 0:\n            raise RuntimeError(\"leave_transaction_management called outside transaction\")\n        elif len(self._transactions[-1]) > 0:\n            raise RuntimeError(\"leave_transaction_management called with uncommited rollbacks\")\n        else:\n            self._transactions.pop()"}
{"func_code_string": "def commit(self) -> None:\n        \"\"\"\n        Attempt to commit all changes to LDAP database. i.e. forget all\n        rollbacks.  However stay inside transaction management.\n        \"\"\"\n        if len(self._transactions) == 0:\n            raise RuntimeError(\"commit called outside transaction\")\n\n        # If we have nested transactions, we don't actually commit, but push\n        # rollbacks up to previous transaction.\n        if len(self._transactions) > 1:\n            for on_rollback in reversed(self._transactions[-1]):\n                self._transactions[-2].insert(0, on_rollback)\n\n        _debug(\"commit\")\n        self.reset()"}
{"func_code_string": "def rollback(self) -> None:\n        \"\"\"\n        Roll back to previous database state. However stay inside transaction\n        management.\n        \"\"\"\n        if len(self._transactions) == 0:\n            raise RuntimeError(\"rollback called outside transaction\")\n\n        _debug(\"rollback:\", self._transactions[-1])\n        # if something goes wrong here, nothing we can do about it, leave\n        # database as is.\n        try:\n            # for every rollback action ...\n            for on_rollback in self._transactions[-1]:\n                # execute it\n                _debug(\"--> rolling back\", on_rollback)\n                self._do_with_retry(on_rollback)\n        except:  # noqa: E722\n            _debug(\"--> rollback failed\")\n            exc_class, exc, tb = sys.exc_info()\n            raise tldap.exceptions.RollbackError(\n                \"FATAL Unrecoverable rollback error: %r\" % exc)\n        finally:\n            # reset everything to clean state\n            _debug(\"--> rollback success\")\n            self.reset()"}
{"func_code_string": "def _process(self, on_commit: UpdateCallable, on_rollback: UpdateCallable) -> Any:\n        \"\"\"\n        Process action. oncommit is a callback to execute action, onrollback is\n        a callback to execute if the oncommit() has been called and a rollback\n        is required\n        \"\"\"\n\n        _debug(\"---> commiting\", on_commit)\n        result = self._do_with_retry(on_commit)\n\n        if len(self._transactions) > 0:\n            # add statement to rollback log in case something goes wrong\n            self._transactions[-1].insert(0, on_rollback)\n\n        return result"}
{"func_code_string": "def add(self, dn: str, mod_list: dict) -> None:\n        \"\"\"\n        Add a DN to the LDAP database; See ldap module. Doesn't return a result\n        if transactions enabled.\n        \"\"\"\n\n        _debug(\"add\", self, dn, mod_list)\n\n        # if rollback of add required, delete it\n        def on_commit(obj):\n            obj.add(dn, None, mod_list)\n\n        def on_rollback(obj):\n            obj.delete(dn)\n\n        # process this action\n        return self._process(on_commit, on_rollback)"}
{"func_code_string": "def modify(self, dn: str, mod_list: dict) -> None:\n        \"\"\"\n        Modify a DN in the LDAP database; See ldap module. Doesn't return a\n        result if transactions enabled.\n        \"\"\"\n\n        _debug(\"modify\", self, dn, mod_list)\n\n        # need to work out how to reverse changes in mod_list; result in revlist\n        revlist = {}\n\n        # get the current cached attributes\n        result = self._cache_get_for_dn(dn)\n\n        # find the how to reverse mod_list (for rollback) and put result in\n        # revlist. Also simulate actions on cache.\n        for mod_type, l in six.iteritems(mod_list):\n            for mod_op, mod_vals in l:\n\n                _debug(\"attribute:\", mod_type)\n                if mod_type in result:\n                    _debug(\"attribute cache:\", result[mod_type])\n                else:\n                    _debug(\"attribute cache is empty\")\n                _debug(\"attribute modify:\", (mod_op, mod_vals))\n\n                if mod_vals is not None:\n                    if not isinstance(mod_vals, list):\n                        mod_vals = [mod_vals]\n\n                if mod_op == ldap3.MODIFY_ADD:\n                    # reverse of MODIFY_ADD is MODIFY_DELETE\n                    reverse = (ldap3.MODIFY_DELETE, mod_vals)\n\n                elif mod_op == ldap3.MODIFY_DELETE and len(mod_vals) > 0:\n                    # Reverse of MODIFY_DELETE is MODIFY_ADD, but only if value\n                    # is given if mod_vals is None, this means all values where\n                    # deleted.\n                    reverse = (ldap3.MODIFY_ADD, mod_vals)\n\n                elif mod_op == ldap3.MODIFY_DELETE \\\n                        or mod_op == ldap3.MODIFY_REPLACE:\n                    if mod_type in result:\n                        # If MODIFY_DELETE with no values or MODIFY_REPLACE\n                        # then we have to replace all attributes with cached\n                        # state\n                        reverse = (\n                            ldap3.MODIFY_REPLACE,\n                            tldap.modlist.escape_list(result[mod_type])\n                        )\n                    else:\n                        # except if we have no cached state for this DN, in\n                        # which case we delete it.\n                        reverse = (ldap3.MODIFY_DELETE, [])\n\n                else:\n                    raise RuntimeError(\"mod_op of %d not supported\" % mod_op)\n\n                reverse = [reverse]\n                _debug(\"attribute reverse:\", reverse)\n                if mod_type in result:\n                    _debug(\"attribute cache:\", result[mod_type])\n                else:\n                    _debug(\"attribute cache is empty\")\n\n                revlist[mod_type] = reverse\n\n        _debug(\"--\")\n        _debug(\"mod_list:\", mod_list)\n        _debug(\"revlist:\", revlist)\n        _debug(\"--\")\n\n        # now the hard stuff is over, we get to the easy stuff\n        def on_commit(obj):\n            obj.modify(dn, mod_list)\n\n        def on_rollback(obj):\n            obj.modify(dn, revlist)\n\n        return self._process(on_commit, on_rollback)"}
{"func_code_string": "def modify_no_rollback(self, dn: str, mod_list: dict):\n        \"\"\"\n        Modify a DN in the LDAP database; See ldap module. Doesn't return a\n        result if transactions enabled.\n        \"\"\"\n\n        _debug(\"modify_no_rollback\", self, dn, mod_list)\n        result = self._do_with_retry(lambda obj: obj.modify_s(dn, mod_list))\n        _debug(\"--\")\n\n        return result"}
{"func_code_string": "def delete(self, dn: str) -> None:\n        \"\"\"\n        delete a dn in the ldap database; see ldap module. doesn't return a\n        result if transactions enabled.\n        \"\"\"\n\n        _debug(\"delete\", self)\n\n        # get copy of cache\n        result = self._cache_get_for_dn(dn)\n\n        # remove special values that can't be added\n        def delete_attribute(name):\n            if name in result:\n                del result[name]\n        delete_attribute('entryUUID')\n        delete_attribute('structuralObjectClass')\n        delete_attribute('modifiersName')\n        delete_attribute('subschemaSubentry')\n        delete_attribute('entryDN')\n        delete_attribute('modifyTimestamp')\n        delete_attribute('entryCSN')\n        delete_attribute('createTimestamp')\n        delete_attribute('creatorsName')\n        delete_attribute('hasSubordinates')\n        delete_attribute('pwdFailureTime')\n        delete_attribute('pwdChangedTime')\n        # turn into mod_list list.\n        mod_list = tldap.modlist.addModlist(result)\n\n        _debug(\"revlist:\", mod_list)\n\n        # on commit carry out action; on rollback restore cached state\n        def on_commit(obj):\n            obj.delete(dn)\n\n        def on_rollback(obj):\n            obj.add(dn, None, mod_list)\n\n        return self._process(on_commit, on_rollback)"}
{"func_code_string": "def rename(self, dn: str, new_rdn: str, new_base_dn: Optional[str] = None) -> None:\n        \"\"\"\n        rename a dn in the ldap database; see ldap module. doesn't return a\n        result if transactions enabled.\n        \"\"\"\n\n        _debug(\"rename\", self, dn, new_rdn, new_base_dn)\n\n        # split up the parameters\n        split_dn = tldap.dn.str2dn(dn)\n        split_newrdn = tldap.dn.str2dn(new_rdn)\n        assert(len(split_newrdn) == 1)\n\n        # make dn unqualified\n        rdn = tldap.dn.dn2str(split_dn[0:1])\n\n        # make newrdn fully qualified dn\n        tmplist = [split_newrdn[0]]\n        if new_base_dn is not None:\n            tmplist.extend(tldap.dn.str2dn(new_base_dn))\n            old_base_dn = tldap.dn.dn2str(split_dn[1:])\n        else:\n            tmplist.extend(split_dn[1:])\n            old_base_dn = None\n        newdn = tldap.dn.dn2str(tmplist)\n\n        _debug(\"--> commit  \", self, dn, new_rdn, new_base_dn)\n        _debug(\"--> rollback\", self, newdn, rdn, old_base_dn)\n\n        # on commit carry out action; on rollback reverse rename\n        def on_commit(obj):\n            obj.modify_dn(dn, new_rdn, new_superior=new_base_dn)\n\n        def on_rollback(obj):\n            obj.modify_dn(newdn, rdn, new_superior=old_base_dn)\n\n        return self._process(on_commit, on_rollback)"}
{"func_code_string": "def fail(self) -> None:\n        \"\"\" for testing purposes only. always fail in commit \"\"\"\n\n        _debug(\"fail\")\n\n        # on commit carry out action; on rollback reverse rename\n        def on_commit(_obj):\n            raise_testfailure(\"commit\")\n\n        def on_rollback(_obj):\n            raise_testfailure(\"rollback\")\n\n        return self._process(on_commit, on_rollback)"}
{"func_code_string": "def __experimental_range(start, stop, var, cond, loc={}):\n    '''Utility function made to reproduce range() with unit integer step\n       but with the added possibility of specifying a condition\n       on the looping variable  (e.g. var % 2  == 0)\n    '''\n    locals().update(loc)\n    if start < stop:\n        for __ in range(start, stop):\n            locals()[var] = __\n            if eval(cond, globals(), locals()):\n                yield __\n    else:\n        for __ in range(start, stop, -1):\n            locals()[var] = __\n            if eval(cond, globals(), locals()):\n                yield __"}
{"func_code_string": "def create_for(line, search_result):\n    '''Create a new \"for loop\" line as a replacement for the original code.\n    '''\n    try:\n        return line.format(search_result.group(\"indented_for\"),\n                           search_result.group(\"var\"),\n                           search_result.group(\"start\"),\n                           search_result.group(\"stop\"),\n                           search_result.group(\"cond\"))\n    except IndexError:\n        return line.format(search_result.group(\"indented_for\"),\n                           search_result.group(\"var\"),\n                           search_result.group(\"start\"),\n                           search_result.group(\"stop\"))"}
{"func_code_string": "def setOverrideValue(self, attributes, hostName):\n        \"\"\" Function __setitem__\n        Set a parameter of a foreman object as a dict\n\n        @param key: The key to modify\n        @param attribute: The data\n        @return RETURN: The API result\n        \"\"\"\n        self['override'] = True\n        attrType = type(attributes)\n        if attrType is dict:\n            self['parameter_type'] = 'hash'\n        elif attrType is list:\n            self['parameter_type'] = 'array'\n        else:\n            self['parameter_type'] = 'string'\n        orv = self.getOverrideValueForHost(hostName)\n        if orv:\n            orv['value'] = attributes\n            return True\n        else:\n            return self.api.create('{}/{}/{}'.format(self.objName,\n                                                     self.key,\n                                                     'override_values'),\n                                   {\"override_value\":\n                                       {\"match\": \"fqdn={}\".format(hostName),\n                                        \"value\": attributes}})"}
{"func_code_string": "def get_interval_timedelta(self):\n        \"\"\" Spits out the timedelta in days. \"\"\"\n\n        now_datetime = timezone.now()\n        current_month_days = monthrange(now_datetime.year, now_datetime.month)[1]\n\n        # Two weeks\n        if self.interval == reminders_choices.INTERVAL_2_WEEKS:\n            interval_timedelta = datetime.timedelta(days=14)\n\n        # One month\n        elif self.interval == reminders_choices.INTERVAL_ONE_MONTH:\n            interval_timedelta = datetime.timedelta(days=current_month_days)\n\n        # Three months\n        elif self.interval == reminders_choices.INTERVAL_THREE_MONTHS:\n            three_months = now_datetime + relativedelta(months=+3)\n            interval_timedelta = three_months - now_datetime\n\n        # Six months\n        elif self.interval == reminders_choices.INTERVAL_SIX_MONTHS:\n            six_months = now_datetime + relativedelta(months=+6)\n            interval_timedelta = six_months - now_datetime\n\n        # One year\n        elif self.interval == reminders_choices.INTERVAL_ONE_YEAR:\n            one_year = now_datetime + relativedelta(years=+1)\n            interval_timedelta = one_year - now_datetime\n\n        return interval_timedelta"}
{"func_code_string": "async def awaitable_runner(runner: BaseRunner):\n    \"\"\"Execute a runner without blocking the event loop\"\"\"\n    runner_thread = CapturingThread(target=runner.run)\n    runner_thread.start()\n    delay = 0.0\n    while not runner_thread.join(timeout=0):\n        await asyncio.sleep(delay)\n        delay = min(delay + 0.1, 1.0)"}
{"func_code_string": "def asyncio_main_run(root_runner: BaseRunner):\n    \"\"\"\n    Create an ``asyncio`` event loop running in the main thread and watching runners\n\n    Using ``asyncio`` to handle suprocesses requires a specific loop type to run in the main thread.\n    This function sets up and runs the correct loop in a portable way.\n    In addition, it runs a single :py:class:`~.BaseRunner` until completion or failure.\n\n    .. seealso:: The `issue #8 <https://github.com/MatterMiners/cobald/issues/8>`_ for details.\n    \"\"\"\n    assert threading.current_thread() == threading.main_thread(), 'only main thread can accept asyncio subprocesses'\n    if sys.platform == 'win32':\n        event_loop = asyncio.ProactorEventLoop()\n        asyncio.set_event_loop(event_loop)\n    else:\n        event_loop = asyncio.get_event_loop()\n        asyncio.get_child_watcher().attach_loop(event_loop)\n    event_loop.run_until_complete(awaitable_runner(root_runner))"}
{"func_code_string": "def enhance(self):\n        \"\"\" Function enhance\n        Enhance the object with new item or enhanced items\n        \"\"\"\n        self.update({'os_default_templates':\n                     SubDict(self.api, self.objName,\n                             self.payloadObj, self.key,\n                             SubItemOsDefaultTemplate)})\n        self.update({'operatingsystems':\n                     SubDict(self.api, self.objName,\n                             self.payloadObj, self.key,\n                             SubItemOperatingSystem)})"}
{"func_code_string": "def retry(tries=10, delay=1, backoff=2, retry_exception=None):\n    \"\"\"\n    Retry \"tries\" times, with initial \"delay\", increasing delay \"delay*backoff\" each time.\n    Without exception success means when function returns valid object.\n    With exception success when no exceptions\n    \"\"\"\n    assert tries > 0, \"tries must be 1 or greater\"\n    catching_mode = bool(retry_exception)\n\n    def deco_retry(f):\n        @functools.wraps(f)\n        def f_retry(*args, **kwargs):\n            mtries, mdelay = tries, delay\n\n            while mtries > 0:\n                time.sleep(mdelay)\n                mdelay *= backoff\n                try:\n                    rv = f(*args, **kwargs)\n                    if not catching_mode and rv:\n                        return rv\n                except retry_exception:\n                    pass\n                else:\n                    if catching_mode:\n                        return rv\n                mtries -= 1\n                if mtries is 0 and not catching_mode:\n                    return False\n                if mtries is 0 and catching_mode:\n                    return f(*args, **kwargs)  # extra try, to avoid except-raise syntax\n                log.debug(\"{0} try, sleeping for {1} sec\".format(tries-mtries, mdelay))\n            raise Exception(\"unreachable code\")\n        return f_retry\n    return deco_retry"}
{"func_code_string": "def dump(node):\n    \"\"\" Dump initialized object structure to yaml\n    \"\"\"\n\n    from qubell.api.private.platform import Auth, QubellPlatform\n    from qubell.api.private.organization import Organization\n    from qubell.api.private.application import Application\n    from qubell.api.private.instance import Instance\n    from qubell.api.private.revision import Revision\n    from qubell.api.private.environment import Environment\n    from qubell.api.private.zone import Zone\n    from qubell.api.private.manifest import Manifest\n\n    # Exclude keys from dump\n    # Format: { 'ClassName': ['fields', 'to', 'exclude']}\n    exclusion_list = {\n        Auth: ['cookies'],\n        QubellPlatform:['auth', ],\n        Organization: ['auth', 'organizationId', 'zone'],\n        Application: ['auth', 'applicationId', 'organization'],\n        Instance: ['auth', 'instanceId', 'application'],\n        Manifest: ['name', 'content'],\n        Revision: ['auth', 'revisionId'],\n        Environment: ['auth', 'environmentId', 'organization'],\n        Zone: ['auth', 'zoneId', 'organization'],\n    }\n\n    def obj_presenter(dumper, obj):\n        for x in exclusion_list.keys():\n            if isinstance(obj, x): # Find class\n                fields = obj.__dict__.copy()\n                for excl_item in exclusion_list[x]:\n                    try:\n                        fields.pop(excl_item)\n                    except:\n                        log.warn('No item %s in object %s' % (excl_item, x))\n                return dumper.represent_mapping('tag:yaml.org,2002:map', fields)\n        return dumper.represent_mapping('tag:yaml.org,2002:map', obj.__dict__)\n\n\n    noalias_dumper = yaml.dumper.Dumper\n    noalias_dumper.ignore_aliases = lambda self, data: True\n\n    yaml.add_representer(unicode, lambda dumper, value: dumper.represent_scalar(u'tag:yaml.org,2002:str', value))\n    yaml.add_multi_representer(object, obj_presenter)\n    serialized = yaml.dump(node, default_flow_style=False, Dumper=noalias_dumper)\n    return serialized"}
{"func_code_string": "def load_env(file):\n    \"\"\"\n    Generate environment used for 'org.restore' method\n    :param file: env file\n    :return: env\n    \"\"\"\n\n    env = yaml.load(open(file))\n\n    for org in env.get('organizations', []):\n        if not org.get('applications'):\n            org['applications'] = []\n\n        if org.get('starter-kit'):\n            kit_meta = get_starter_kit_meta(org.get('starter-kit'))\n            for meta_app in get_applications_from_metadata(kit_meta):\n                org['applications'].append(meta_app)\n\n        if org.get('meta'):\n            for meta_app in get_applications_from_metadata(org.get('meta')):\n                org['applications'].append(meta_app)\n\n        for app in org.get('applications', []):\n            if app.get('file'):\n                app['file'] = os.path.realpath(os.path.join(os.path.dirname(file), app['file']))\n    return env"}
{"func_code_string": "def patch_env(env, path, value):\n        \"\"\" Set specified value to yaml path.\n        Example:\n        patch('application/components/child/configuration/__locator.application-id','777')\n        Will change child app ID to 777\n        \"\"\"\n        def pathGet(dictionary, path):\n            for item in path.split(\"/\"):\n                dictionary = dictionary[item]\n            return dictionary\n\n        def pathSet(dictionary, path, value):\n            path = path.split(\"/\")\n            key = path[-1]\n            dictionary = pathGet(dictionary, \"/\".join(path[:-1]))\n            dictionary[key] = value\n\n        pathSet(env, path, value)\n        return True"}
{"func_code_string": "def get_starter_kit_meta(name):\n    \"\"\"\n    Extract metadata link for starter kit from platform configs. Starter kit available on add component - starter kit menu.\n    Beware, config could be changed by deploy scripts during deploy.\n    :param name: Name of starter kit\n    :return: Link to metadata\n    \"\"\"\n    kits = yaml.safe_load(requests.get(url=starter_kits_url).content)['kits']\n    kits_meta_url = [x['metaUrl'] for x in kits if x['name'] == name]\n\n    assert len(kits_meta_url)==1, \"No component %s found in meta:\\n %s\" % (name, kits)\n    meta = yaml.safe_load(requests.get(url=kits_meta_url[0]).content)['download_url']\n    return meta"}
{"func_code_string": "def get_manifest_from_meta(metaurl, name):\n    \"\"\"\n    Extact manifest url from metadata url\n    :param metaurl: Url to metadata\n    :param name: Name of application to extract\n    :return:\n    \"\"\"\n    if 'http' in metaurl:\n        kit = yaml.safe_load(requests.get(url=metaurl).content)['kit']['applications']\n    else:\n        kit = yaml.safe_load(open(metaurl).read())['kit']['applications']\n    app_urls = [x['manifest'] for x in kit if x['name'] == name]\n    assert len(app_urls) == 1\n    return app_urls[0]"}
{"func_code_string": "def getPayloadStruct(self, attributes, objType=None):\n        \"\"\" Function getPayloadStruct\n        Get the payload structure to do a creation or a modification\n\n        @param key: The key to modify\n        @param attribute: The data\n        @param objType: NOT USED in this class\n        @return RETURN: The API result\n        \"\"\"\n        if self.setInParentPayload:\n            return {self.parentPayloadObject:\n                    {self.payloadObj: attributes}}\n        else:\n            return {self.payloadObj: attributes}"}
{"func_code_string": "def log(function):\n        \"\"\" Function log\n        Decorator to log lasts request before sending a new one\n\n        @return RETURN: None\n        \"\"\"\n        def _log(self, *args, **kwargs):\n            ret = function(self, *args, **kwargs)\n            if len(self.history) > self.maxHistory:\n                self.history = self.history[1:self.maxHistory]\n            self.history.append({'errorMsg': self.errorMsg,\n                                 'payload': self.payload,\n                                 'url': self.url,\n                                 'resp': self.resp,\n                                 'res': self.res,\n                                 'printErrors': self.printErrors,\n                                 'method': self.method})\n            self.clearReqVars()\n            return ret\n        return _log"}
{"func_code_string": "def clearReqVars(self):\n        \"\"\" Function clearHistVars\n        Clear the variables used to get history of all vars\n\n        @return RETURN: None\n        \"\"\"\n        self.errorMsg = None\n        self.payload = None\n        self.url = None\n        self.resp = None\n        self.res = None\n        self.method = None\n        self.printErrors = None"}
{"func_code_string": "def list(self, obj, filter=False, only_id=False, limit=20):\n        \"\"\" Function list\n        Get the list of an object\n\n        @param obj: object name ('hosts', 'puppetclasses'...)\n        @param filter: filter for objects\n        @param only_id: boolean to only return dict with name/id\n        @return RETURN: the list of the object\n        \"\"\"\n        self.url = '{}{}/?per_page={}'.format(self.base_url, obj, limit)\n        self.method = 'GET'\n        if filter:\n            self.url += '&search={}'.format(filter)\n        self.resp = requests.get(url=self.url, auth=self.auth,\n                                 headers=self.headers, cert=self.ca_cert)\n        if only_id:\n            if self.__process_resp__(obj) is False:\n                return False\n            if type(self.res['results']) is list:\n                return dict((x['name'], x['id']) for x in self.res['results'])\n            elif type(self.res['results']) is dict:\n                r = {}\n                for v in self.res['results'].values():\n                    for vv in v:\n                        r[vv['name']] = vv['id']\n                return r\n            else:\n                return False\n        else:\n            return self.__process_resp__(obj)"}
{"func_code_string": "def get(self, obj, id, sub_object=None):\n        \"\"\" Function get\n        Get an object by id\n\n        @param obj: object name ('hosts', 'puppetclasses'...)\n        @param id: the id of the object (name or id)\n        @return RETURN: the targeted object\n        \"\"\"\n        self.url = '{}{}/{}'.format(self.base_url, obj, id)\n        self.method = 'GET'\n        if sub_object:\n            self.url += '/' + sub_object\n        self.resp = requests.get(url=self.url, auth=self.auth,\n                                 headers=self.headers, cert=self.ca_cert)\n        if self.__process_resp__(obj):\n            return self.res\n        return False"}
{"func_code_string": "def get_id_by_name(self, obj, name):\n        \"\"\" Function get_id_by_name\n        Get the id of an object\n\n        @param obj: object name ('hosts', 'puppetclasses'...)\n        @param id: the id of the object (name or id)\n        @return RETURN: the targeted object\n        \"\"\"\n        list = self.list(obj, filter='name = \"{}\"'.format(name),\n                         only_id=True, limit=1)\n        return list[name] if name in list.keys() else False"}
{"func_code_string": "def set(self, obj, id, payload, action='', async=False):\n        \"\"\" Function set\n        Set an object by id\n\n        @param obj: object name ('hosts', 'puppetclasses'...)\n        @param id: the id of the object (name or id)\n        @param action: specific action of an object ('power'...)\n        @param payload: the dict of the payload\n        @param async: should this request be async, if true use\n                        return.result() to get the response\n        @return RETURN: the server response\n        \"\"\"\n        self.url = '{}{}/{}'.format(self.base_url, obj, id)\n        self.method = 'PUT'\n        if action:\n            self.url += '/{}'.format(action)\n        self.payload = json.dumps(payload)\n        if async:\n            session = FuturesSession()\n            return session.put(url=self.url, auth=self.auth,\n                               headers=self.headers, data=self.payload,\n                               cert=self.ca_cert)\n        else:\n            self.resp = requests.put(url=self.url, auth=self.auth,\n                                     headers=self.headers, data=self.payload,\n                                     cert=self.ca_cert)\n            if self.__process_resp__(obj):\n                return self.res\n            return False"}
{"func_code_string": "def create(self, obj, payload, async=False):\n        \"\"\" Function create\n        Create an new object\n\n        @param obj: object name ('hosts', 'puppetclasses'...)\n        @param payload: the dict of the payload\n        @param async: should this request be async, if true use\n                        return.result() to get the response\n        @return RETURN: the server response\n        \"\"\"\n        self.url = self.base_url + obj\n        self.method = 'POST'\n        self.payload = json.dumps(payload)\n        if async:\n            self.method = 'POST(Async)'\n            session = FuturesSession()\n            self.resp = session.post(url=self.url, auth=self.auth,\n                                     headers=self.headers, data=self.payload,\n                                     cert=self.ca_cert)\n            return self.resp\n        else:\n            self.resp = requests.post(url=self.url, auth=self.auth,\n                                      headers=self.headers,\n                                      data=self.payload, cert=self.ca_cert)\n            return self.__process_resp__(obj)"}
{"func_code_string": "def delete(self, obj, id):\n        \"\"\" Function delete\n        Delete an object by id\n\n        @param obj: object name ('hosts', 'puppetclasses'...)\n        @param id: the id of the object (name or id)\n        @return RETURN: the server response\n        \"\"\"\n        self.url = '{}{}/{}'.format(self.base_url, obj, id)\n        self.method = 'DELETE'\n        self.resp = requests.delete(url=self.url,\n                                    auth=self.auth,\n                                    headers=self.headers, cert=self.ca_cert)\n        return self.__process_resp__(obj)"}
{"func_code_string": "def run(self):\n        \"\"\"Modified ``run`` that captures return value and exceptions from ``target``\"\"\"\n        try:\n            if self._target:\n                return_value = self._target(*self._args, **self._kwargs)\n                if return_value is not None:\n                    self._exception = OrphanedReturn(self, return_value)\n        except BaseException as err:\n            self._exception = err\n        finally:\n            # Avoid a refcycle if the thread is running a function with\n            # an argument that has a member that points to the thread.\n            del self._target, self._args, self._kwargs"}
{"func_code_string": "def _start_payloads(self):\n        \"\"\"Start all queued payloads\"\"\"\n        with self._lock:\n            payloads = self._payloads.copy()\n            self._payloads.clear()\n        for subroutine in payloads:\n            thread = CapturingThread(target=subroutine)\n            thread.start()\n            self._threads.add(thread)\n            self._logger.debug('booted thread %s', thread)\n        time.sleep(0)"}
{"func_code_string": "def _reap_payloads(self):\n        \"\"\"Clean up all finished payloads\"\"\"\n        for thread in self._threads.copy():\n            # CapturingThread.join will throw\n            if thread.join(timeout=0):\n                self._threads.remove(thread)\n                self._logger.debug('reaped thread %s', thread)"}
{"func_code_string": "def update_cache(func):\n    \"\"\"Decorate functions that modify the internally stored usernotes JSON.\n\n    Ensures that updates are mirrored onto reddit.\n\n    Arguments:\n        func: the function being decorated\n    \"\"\"\n    @wraps(func)\n    def wrapper(self, *args, **kwargs):\n        \"\"\"The wrapper function.\"\"\"\n        lazy = kwargs.get('lazy', False)\n        kwargs.pop('lazy', None)\n\n        if not lazy:\n            self.get_json()\n\n        ret = func(self, *args, **kwargs)\n\n        # If returning a string assume it is an update message\n        if isinstance(ret, str) and not lazy:\n            self.set_json(ret)\n        else:\n            return ret\n\n    return wrapper"}
{"func_code_string": "def s(cls: Type[C], *args, **kwargs) -> Partial[C]:\n        \"\"\"\n        Create an unbound prototype of this class, partially applying arguments\n\n        .. code:: python\n\n            controller = Controller.s(interval=20)\n\n            pipeline = controller(rate=10) >> pool\n        \"\"\"\n        return Partial(cls, *args, **kwargs)"}
{"func_code_string": "def _build_mappings(\n        self, classes: Sequence[type]\n    ) -> Tuple[Mapping[type, Sequence[type]], Mapping[type, Sequence[type]]]:\n        \"\"\"\n        Collect all bases and organize into parent/child mappings.\n        \"\"\"\n        parents_to_children: MutableMapping[type, Set[type]] = {}\n        children_to_parents: MutableMapping[type, Set[type]] = {}\n        visited_classes: Set[type] = set()\n        class_stack = list(classes)\n        while class_stack:\n            class_ = class_stack.pop()\n            if class_ in visited_classes:\n                continue\n            visited_classes.add(class_)\n            for base in class_.__bases__:\n                if base not in visited_classes:\n                    class_stack.append(base)\n                parents_to_children.setdefault(base, set()).add(class_)\n                children_to_parents.setdefault(class_, set()).add(base)\n        sorted_parents_to_children: MutableMapping[\n            type, List[type]\n        ] = collections.OrderedDict()\n        for parent, children in sorted(\n            parents_to_children.items(), key=lambda x: (x[0].__module__, x[0].__name__)\n        ):\n            sorted_parents_to_children[parent] = sorted(\n                children, key=lambda x: (x.__module__, x.__name__)\n            )\n        sorted_children_to_parents: MutableMapping[\n            type, List[type]\n        ] = collections.OrderedDict()\n        for child, parents in sorted(\n            children_to_parents.items(), key=lambda x: (x[0].__module__, x[0].__name__)\n        ):\n            sorted_children_to_parents[child] = sorted(\n                parents, key=lambda x: (x.__module__, x.__name__)\n            )\n        return sorted_parents_to_children, sorted_children_to_parents"}
{"func_code_string": "def _collect_classes(\n        self, package_paths: Sequence[str], recurse_subpackages: bool = True\n    ) -> Sequence[type]:\n        \"\"\"\n        Collect all classes defined in/under ``package_paths``.\n        \"\"\"\n        import uqbar.apis\n\n        classes = []\n        initial_source_paths: Set[str] = set()\n        # Graph source paths and classes\n        for path in package_paths:\n            try:\n                module = importlib.import_module(path)\n                if hasattr(module, \"__path__\"):\n                    initial_source_paths.update(getattr(module, \"__path__\"))\n                else:\n                    initial_source_paths.add(module.__file__)\n            except ModuleNotFoundError:\n                path, _, class_name = path.rpartition(\".\")\n                module = importlib.import_module(path)\n                classes.append(getattr(module, class_name))\n        # Iterate source paths\n        for source_path in uqbar.apis.collect_source_paths(\n            initial_source_paths, recurse_subpackages=recurse_subpackages\n        ):\n            package_path = uqbar.apis.source_path_to_package_path(source_path)\n            module = importlib.import_module(package_path)\n            # Grab any defined classes\n            for name in dir(module):\n                if name.startswith(\"_\"):\n                    continue\n                object_ = getattr(module, name)\n                if isinstance(object_, type) and object_.__module__ == module.__name__:\n                    classes.append(object_)\n        return sorted(classes, key=lambda x: (x.__module__, x.__name__))"}
{"func_code_string": "def get_auth():\n    \"\"\"Return a tuple for authenticating a user\n\n    If not successful raise ``AgileError``.\n    \"\"\"\n    auth = get_auth_from_env()\n    if auth[0] and auth[1]:\n        return auth\n\n    home = os.path.expanduser(\"~\")\n    config = os.path.join(home, '.gitconfig')\n    if not os.path.isfile(config):\n        raise GithubException('No .gitconfig available')\n\n    parser = configparser.ConfigParser()\n    parser.read(config)\n    if 'user' in parser:\n        user = parser['user']\n        if 'username' not in user:\n            raise GithubException('Specify username in %s user '\n                                  'section' % config)\n        if 'token' not in user:\n            raise GithubException('Specify token in %s user section'\n                                  % config)\n        return user['username'], user['token']\n    else:\n        raise GithubException('No user section in %s' % config)"}
{"func_code_string": "def checkAndCreate(self, key, payload, osIds):\n        \"\"\" Function checkAndCreate\n        Check if an architectures exists and create it if not\n\n        @param key: The targeted architectures\n        @param payload: The targeted architectures description\n        @param osIds: The list of os ids liked with this architecture\n        @return RETURN: The id of the object\n        \"\"\"\n        if key not in self:\n            self[key] = payload\n        oid = self[key]['id']\n        if not oid:\n            return False\n        #~ To be sure the OS list is good, we ensure our os are in the list\n        for os in self[key]['operatingsystems']:\n            osIds.add(os['id'])\n        self[key][\"operatingsystem_ids\"] = list(osIds)\n        if (len(self[key]['operatingsystems']) is not len(osIds)):\n            return False\n        return oid"}
{"func_code_string": "def pip_command_output(pip_args):\n    \"\"\"\n    Get output (as a string) from pip command\n    :param pip_args: list o pip switches to pass\n    :return: string with results\n    \"\"\"\n    import sys\n    import pip\n    from io import StringIO\n    # as pip will write to stdout we use some nasty hacks\n    # to substitute system stdout with our own\n    old_stdout = sys.stdout\n    sys.stdout = mystdout = StringIO()\n    pip.main(pip_args)\n    output = mystdout.getvalue()\n    mystdout.truncate(0)\n    sys.stdout = old_stdout\n    return output"}
{"func_code_string": "def setup_versioneer():\n    \"\"\"\n    Generate (temporarily) versioneer.py file in project root directory\n    :return:\n    \"\"\"\n    try:\n        # assume versioneer.py was generated using \"versioneer install\" command\n        import versioneer\n        versioneer.get_version()\n    except ImportError:\n        # it looks versioneer.py is missing\n        # lets assume that versioneer package is installed\n        # and versioneer binary is present in $PATH\n        import subprocess\n        try:\n            # call versioneer install to generate versioneer.py\n            subprocess.check_output([\"versioneer\", \"install\"])\n        except OSError:\n            # it looks versioneer is missing from $PATH\n            # probably versioneer is installed in some user directory\n\n            # query pip for list of files in versioneer package\n            # line below is equivalen to putting result of\n            #  \"pip show -f versioneer\" command to string output\n            output = pip_command_output([\"show\", \"-f\", \"versioneer\"])\n\n            # now we parse the results\n            import os\n            # find absolute path where *versioneer package* was installed\n            # and store it in main_path\n            main_path = [x[len(\"Location: \"):] for x in output.splitlines()\n                         if x.startswith(\"Location\")][0]\n            # find path relative to main_path where\n            # *versioneer binary* was installed\n            bin_path = [x[len(\"  \"):] for x in output.splitlines()\n                        if x.endswith(os.path.sep + \"versioneer\")][0]\n\n            # exe_path is absolute path to *versioneer binary*\n            exe_path = os.path.join(main_path, bin_path)\n            # call versioneer install to generate versioneer.py\n            # line below is equivalent to running in terminal\n            # \"python versioneer install\"\n            subprocess.check_output([\"python\", exe_path, \"install\"])"}
{"func_code_string": "def clean_cache():\n    \"\"\"\n    Python won't realise that new module has appeared in the runtime\n    We need to clean the cache of module finders. Hacking again\n    :return:\n    \"\"\"\n    import importlib\n    try:  # Python ver < 3.3\n        vermod = importlib.import_module(\"versioneer\")\n        globals()[\"versioneer\"] = vermod\n    except ImportError:\n        importlib.invalidate_caches()"}
{"func_code_string": "def get_version():\n    \"\"\"\n    Get project version (using versioneer)\n    :return: string containing version\n    \"\"\"\n    setup_versioneer()\n    clean_cache()\n    import versioneer\n    version = versioneer.get_version()\n    parsed_version = parse_version(version)\n    if '*@' in str(parsed_version):\n        import time\n        version += str(int(time.time()))\n    return version"}
{"func_code_string": "def find_common_prefix(\n    paths: Sequence[Union[str, pathlib.Path]]\n) -> Optional[pathlib.Path]:\n    \"\"\"\n    Find the common prefix of two or more paths.\n\n    ::\n\n        >>> import pathlib\n        >>> one = pathlib.Path('foo/bar/baz')\n        >>> two = pathlib.Path('foo/quux/biz')\n        >>> three = pathlib.Path('foo/quux/wuux')\n\n    ::\n\n        >>> import uqbar.io\n        >>> str(uqbar.io.find_common_prefix([one, two, three]))\n        'foo'\n\n    :param paths: paths to inspect\n    \"\"\"\n    counter: collections.Counter = collections.Counter()\n    for path in paths:\n        path = pathlib.Path(path)\n        counter.update([path])\n        counter.update(path.parents)\n    valid_paths = sorted(\n        [path for path, count in counter.items() if count >= len(paths)],\n        key=lambda x: len(x.parts),\n    )\n    if valid_paths:\n        return valid_paths[-1]\n    return None"}
{"func_code_string": "def find_executable(name: str, flags=os.X_OK) -> List[str]:\n    r\"\"\"Finds executable `name`.\n\n    Similar to Unix ``which`` command.\n\n    Returns list of zero or more full paths to `name`.\n    \"\"\"\n    result = []\n    extensions = [x for x in os.environ.get(\"PATHEXT\", \"\").split(os.pathsep) if x]\n    path = os.environ.get(\"PATH\", None)\n    if path is None:\n        return []\n    for path in os.environ.get(\"PATH\", \"\").split(os.pathsep):\n        path = os.path.join(path, name)\n        if os.access(path, flags):\n            result.append(path)\n        for extension in extensions:\n            path_extension = path + extension\n            if os.access(path_extension, flags):\n                result.append(path_extension)\n    return result"}
{"func_code_string": "def relative_to(\n    source_path: Union[str, pathlib.Path], target_path: Union[str, pathlib.Path]\n) -> pathlib.Path:\n    \"\"\"\n    Generates relative path from ``source_path`` to ``target_path``.\n\n    Handles the case of paths without a common prefix.\n\n    ::\n\n        >>> import pathlib\n        >>> source = pathlib.Path('foo/bar/baz')\n        >>> target = pathlib.Path('foo/quux/biz')\n\n    ::\n\n        >>> target.relative_to(source)\n        Traceback (most recent call last):\n          ...\n        ValueError: 'foo/quux/biz' does not start with 'foo/bar/baz'\n\n    ::\n\n        >>> import uqbar.io\n        >>> str(uqbar.io.relative_to(source, target))\n        '../../quux/biz'\n\n    :param source_path: the source path\n    :param target_path: the target path\n    \"\"\"\n    source_path = pathlib.Path(source_path).absolute()\n    if source_path.is_file():\n        source_path = source_path.parent\n    target_path = pathlib.Path(target_path).absolute()\n    common_prefix = find_common_prefix([source_path, target_path])\n    if not common_prefix:\n        raise ValueError(\"No common prefix\")\n    source_path = source_path.relative_to(common_prefix)\n    target_path = target_path.relative_to(common_prefix)\n    result = pathlib.Path(*[\"..\"] * len(source_path.parts))\n    return result / target_path"}
{"func_code_string": "def walk(\n    root_path: Union[str, pathlib.Path], top_down: bool = True\n) -> Generator[\n    Tuple[pathlib.Path, Sequence[pathlib.Path], Sequence[pathlib.Path]], None, None\n]:\n    \"\"\"\n    Walks a directory tree.\n\n    Like :py:func:`os.walk` but yielding instances of :py:class:`pathlib.Path`\n    instead of strings.\n\n    :param root_path: foo\n    :param top_down: bar\n    \"\"\"\n    root_path = pathlib.Path(root_path)\n    directory_paths, file_paths = [], []\n    for path in sorted(root_path.iterdir()):\n        if path.is_dir():\n            directory_paths.append(path)\n        else:\n            file_paths.append(path)\n    if top_down:\n        yield root_path, directory_paths, file_paths\n    for directory_path in directory_paths:\n        yield from walk(directory_path, top_down=top_down)\n    if not top_down:\n        yield root_path, directory_paths, file_paths"}
{"func_code_string": "def write(\n    contents: str,\n    path: Union[str, pathlib.Path],\n    verbose: bool = False,\n    logger_func=None,\n) -> bool:\n    \"\"\"\n    Writes ``contents`` to ``path``.\n\n    Checks if ``path`` already exists and only write out new contents if the\n    old contents do not match.\n\n    Creates any intermediate missing directories.\n\n    :param contents: the file contents to write\n    :param path: the path to write to\n    :param verbose: whether to print output\n    \"\"\"\n    print_func = logger_func or print\n    path = pathlib.Path(path)\n    if path.exists():\n        with path.open(\"r\") as file_pointer:\n            old_contents = file_pointer.read()\n        if old_contents == contents:\n            if verbose:\n                print_func(\"preserved {}\".format(path))\n            return False\n        else:\n            with path.open(\"w\") as file_pointer:\n                file_pointer.write(contents)\n            if verbose:\n                print_func(\"rewrote {}\".format(path))\n            return True\n    elif not path.exists():\n        if not path.parent.exists():\n            path.parent.mkdir(parents=True)\n        with path.open(\"w\") as file_pointer:\n            file_pointer.write(contents)\n        if verbose:\n            print_func(\"wrote {}\".format(path))\n    return True"}
{"func_code_string": "def pretty_ref(obj: Any) -> str:\n    \"\"\"Pretty object reference using ``module.path:qual.name`` format\"\"\"\n    try:\n        return obj.__module__ + ':' + obj.__qualname__\n    except AttributeError:\n        return pretty_ref(type(obj)) + '(...)'"}
{"func_code_string": "def remote(ctx):\n    \"\"\"Display repo github path\n    \"\"\"\n    with command():\n        m = RepoManager(ctx.obj['agile'])\n        click.echo(m.github_repo().repo_path)"}
{"func_code_string": "def graph_order(self):\n        \"\"\"\n        Get graph-order tuple for node.\n\n        ::\n\n            >>> from uqbar.containers import UniqueTreeContainer, UniqueTreeNode\n            >>> root_container = UniqueTreeContainer(name=\"root\")\n            >>> outer_container = UniqueTreeContainer(name=\"outer\")\n            >>> inner_container = UniqueTreeContainer(name=\"inner\")\n            >>> node_a = UniqueTreeNode(name=\"a\")\n            >>> node_b = UniqueTreeNode(name=\"b\")\n            >>> node_c = UniqueTreeNode(name=\"c\")\n            >>> node_d = UniqueTreeNode(name=\"d\")\n            >>> root_container.extend([node_a, outer_container])\n            >>> outer_container.extend([inner_container, node_d])\n            >>> inner_container.extend([node_b, node_c])\n\n        ::\n\n            >>> for node in root_container.depth_first():\n            ...     print(node.name, node.graph_order)\n            ...\n            a (0,)\n            outer (1,)\n            inner (1, 0)\n            b (1, 0, 0)\n            c (1, 0, 1)\n            d (1, 1)\n\n        \"\"\"\n        parentage = tuple(reversed(self.parentage))\n        graph_order = []\n        for i in range(len(parentage) - 1):\n            parent, child = parentage[i : i + 2]\n            graph_order.append(parent.index(child))\n        return tuple(graph_order)"}
{"func_code_string": "def sendQuery(self, cmd, multilines=False):\n        \"\"\" Send command, wait for response (single or multi lines), test for errors and return the returned code.\n\n        :param cmd: command to send\n        :param multilines: True - multiline response, False - single line response.\n        :return: command return value.\n        \"\"\"\n        self.logger.debug(\"sendQuery(%s)\", cmd)\n        if not self.is_connected():\n            raise socket.error(\"sendQuery on a disconnected socket\")\n\n        if multilines:\n            replies = self.__sendQueryReplies(cmd)\n            for reply in replies:\n                if reply.startswith(XenaSocket.reply_errors):\n                    raise XenaCommandException('sendQuery({}) reply({})'.format(cmd, replies))\n            self.logger.debug(\"sendQuery(%s) -- Begin\", cmd)\n            for l in replies:\n                self.logger.debug(\"%s\", l.strip())\n            self.logger.debug(\"sendQuery(%s) -- End\", cmd)\n            return replies\n        else:\n            reply = self.__sendQueryReply(cmd)\n            if reply.startswith(XenaSocket.reply_errors):\n                raise XenaCommandException('sendQuery({}) reply({})'.format(cmd, reply))\n            self.logger.debug('sendQuery(%s) reply(%s)', cmd, reply)\n            return reply"}
{"func_code_string": "def sendQueryVerify(self, cmd):\n        \"\"\" Send command without return value, wait for completion, verify success.\n\n        :param cmd: command to send\n        \"\"\"\n        cmd = cmd.strip()\n        self.logger.debug(\"sendQueryVerify(%s)\", cmd)\n        if not self.is_connected():\n            raise socket.error(\"sendQueryVerify on a disconnected socket\")\n\n        resp = self.__sendQueryReply(cmd)\n        if resp != self.reply_ok:\n            raise XenaCommandException('Command {} Fail Expected {} Actual {}'.format(cmd, self.reply_ok, resp))\n        self.logger.debug(\"SendQueryVerify(%s) Succeed\", cmd)"}
{"func_code_string": "def find_external_files(self, run_input_dir):\n        \"\"\"\n        Scan all SHIELDHIT12A config files to find external files used and return them.\n        Also change paths in config files to match convention that all resources are\n        symlinked in job_xxxx/symlink\n        \"\"\"\n        beam_file, geo_file, mat_file, _ = self.input_files\n\n        # check for external files in BEAM input file\n        external_beam_files = self._parse_beam_file(beam_file, run_input_dir)\n        if external_beam_files:\n            logger.info(\"External files from BEAM file: {0}\".format(external_beam_files))\n        else:\n            logger.debug(\"No external files from BEAM file\")\n\n        # check for external files in MAT input file\n        icru_numbers = self._parse_mat_file(mat_file)\n        if icru_numbers:\n            logger.info(\"External files from MAT file: {0}\".format(icru_numbers))\n        else:\n            logger.debug(\"No external files from MAT file\")\n        # if ICRU+LOADEX pairs were found - get file names for external material files\n        icru_files = []\n        if icru_numbers:\n            icru_files = self._decrypt_icru_files(icru_numbers)\n\n        # check for external files in GEO input file\n        geo_files = self._parse_geo_file(geo_file, run_input_dir)\n        if geo_files:\n            logger.info(\"External files from GEO file: {0}\".format(geo_files))\n        else:\n            logger.debug(\"No external files from GEO file\")\n\n        external_files = external_beam_files + icru_files + geo_files\n        return [os.path.join(self.input_path, e) for e in external_files]"}
{"func_code_string": "def _parse_beam_file(self, file_path, run_input_dir):\n        \"\"\"Scan SH12A BEAM file for references to external files and return them\"\"\"\n        external_files = []\n        paths_to_replace = []\n        with open(file_path, 'r') as beam_f:\n            for line in beam_f.readlines():\n                split_line = line.split()\n                # line length checking to prevent IndexError\n                if len(split_line) > 2 and split_line[0] == \"USEBMOD\":\n                    logger.debug(\"Found reference to external file in BEAM file: {0} {1}\".format(\n                                 split_line[0], split_line[2]))\n                    external_files.append(split_line[2])\n                    paths_to_replace.append(split_line[2])\n                elif len(split_line) > 1 and split_line[0] == \"USECBEAM\":\n                    logger.debug(\"Found reference to external file in BEAM file: {0} {1}\".format(\n                                 split_line[0], split_line[1]))\n                    external_files.append(split_line[1])\n                    paths_to_replace.append(split_line[1])\n        if paths_to_replace:\n            run_dir_config_file = os.path.join(run_input_dir, os.path.split(file_path)[-1])\n            logger.debug(\"Calling rewrite_paths method on file: {0}\".format(run_dir_config_file))\n            self._rewrite_paths_in_file(run_dir_config_file, paths_to_replace)\n        return external_files"}
{"func_code_string": "def _parse_geo_file(self, file_path, run_input_dir):\n        \"\"\"Scan SH12A GEO file for references to external files (like voxelised geometry) and return them\"\"\"\n        external_files = []\n        paths_to_replace = []\n        with open(file_path, 'r') as geo_f:\n            for line in geo_f.readlines():\n                split_line = line.split()\n                if len(split_line) > 0 and not line.startswith(\"*\"):\n                    base_path = os.path.join(self.input_path, split_line[0])\n                    if os.path.isfile(base_path + '.hed'):\n                        logger.debug(\"Found ctx + hed files: {0}\".format(base_path))\n                        external_files.append(base_path + '.hed')\n                        # try to find ctx file\n                        if os.path.isfile(base_path + '.ctx'):\n                            external_files.append(base_path + '.ctx')\n                        elif os.path.isfile(base_path + '.ctx.gz'):\n                            external_files.append(base_path + '.ctx.gz')\n                        # replace path to match symlink location\n                        paths_to_replace.append(split_line[0])\n        if paths_to_replace:\n            run_dir_config_file = os.path.join(run_input_dir, os.path.split(file_path)[-1])\n            logger.debug(\"Calling rewrite_paths method on file: {0}\".format(run_dir_config_file))\n            self._rewrite_paths_in_file(run_dir_config_file, paths_to_replace)\n        return external_files"}
{"func_code_string": "def _parse_mat_file(self, file_path):\n        \"\"\"Scan SH12A MAT file for ICRU+LOADEX pairs and return found ICRU numbers\"\"\"\n        mat_file_sections = self._extract_mat_sections(file_path)\n        return self._analyse_mat_sections(mat_file_sections)"}
{"func_code_string": "def _analyse_mat_sections(sections):\n        \"\"\"\n        Cases:\n        - ICRU flag present, LOADDEDX flag missing -> data loaded from some data hardcoded in SH12A binary,\n        no need to load external files\n        - ICRU flag present, LOADDEDX flag present -> data loaded from external files. ICRU number read from ICRU flag,\n        any number following LOADDEDX flag is ignored.\n        - ICRU flag missing, LOADDEDX flag present -> data loaded from external files. ICRU number read from LOADDEDX\n        - ICRU flag missing, LOADDEDX flag missing -> nothing happens\n        \"\"\"\n        icru_numbers = []\n        for section in sections:\n            load_present = False\n            load_value = False\n            icru_value = False\n            for e in section:\n                split_line = e.split()\n                if \"LOADDEDX\" in e:\n                    load_present = True\n                    if len(split_line) > 1:\n                        load_value = split_line[1] if \"!\" not in split_line[1] else False  # ignore ! comments\n                elif \"ICRU\" in e and len(split_line) > 1:\n                    icru_value = split_line[1] if \"!\" not in split_line[1] else False  # ignore ! comments\n            if load_present:  # LOADDEDX is present, so external file is required\n                if icru_value:  # if ICRU value was given\n                    icru_numbers.append(icru_value)\n                elif load_value:  # if only LOADDEDX with values was present in section\n                    icru_numbers.append(load_value)\n        return icru_numbers"}
{"func_code_string": "def _decrypt_icru_files(numbers):\n        \"\"\"Find matching file names for given ICRU numbers\"\"\"\n        import json\n        icru_file = resource_string(__name__, os.path.join('data', 'SH12A_ICRU_table.json'))\n        ref_dict = json.loads(icru_file.decode('ascii'))\n        try:\n            return [ref_dict[e] for e in numbers]\n        except KeyError as er:\n            logger.error(\"There is no ICRU file for id: {0}\".format(er))\n            raise"}
{"func_code_string": "def _rewrite_paths_in_file(config_file, paths_to_replace):\n        \"\"\"\n        Rewrite paths in config files to match convention job_xxxx/symlink\n        Requires path to run_xxxx/input/config_file and a list of paths_to_replace\n        \"\"\"\n        lines = []\n        # make a copy of config\n        import shutil\n        shutil.copyfile(config_file, str(config_file + '_original'))\n        with open(config_file) as infile:\n            for line in infile:\n                for old_path in paths_to_replace:\n                    if old_path in line:\n                        new_path = os.path.split(old_path)[-1]\n                        line = line.replace(old_path, new_path)\n                        logger.debug(\"Changed path {0} ---> {1} in file {2}\".format(old_path, new_path, config_file))\n                lines.append(line)\n        with open(config_file, 'w') as outfile:\n            for line in lines:\n                outfile.write(line)"}
{"func_code_string": "def _check_exists(database: Database, table: LdapObjectClass, key: str, value: str):\n    \"\"\" Check if a given LDAP object exists. \"\"\"\n    try:\n        get_one(table, Q(**{key: value}), database=database)\n        return True\n    except ObjectDoesNotExist:\n        return False"}
{"func_code_string": "def save_account(changes: Changeset, table: LdapObjectClass, database: Database) -> Changeset:\n    \"\"\" Modify a changes to add an automatically generated uidNumber. \"\"\"\n    d = {}\n    settings = database.settings\n\n    uid_number = changes.get_value_as_single('uidNumber')\n    if uid_number is None:\n        scheme = settings['NUMBER_SCHEME']\n        first = settings.get('UID_FIRST', 10000)\n        d['uidNumber'] = Counters.get_and_increment(\n            scheme, \"uidNumber\", first,\n            lambda n: not _check_exists(database, table, 'uidNumber', n)\n        )\n\n    changes = changes.merge(d)\n    return changes"}
{"func_code_string": "def transform_source(text):\n    '''Replaces instances of\n\n        switch expression:\n    by\n\n        for __case in _Switch(n):\n\n    and replaces\n\n        case expression:\n\n    by\n\n        if __case(expression):\n\n    and\n\n        default:\n\n    by\n\n        if __case():\n    '''\n    toks = tokenize.generate_tokens(StringIO(text).readline)\n    result = []\n    replacing_keyword = False\n    for toktype, tokvalue, _, _, _ in toks:\n        if toktype == tokenize.NAME and tokvalue == 'switch':\n            result.extend([\n                (tokenize.NAME, 'for'),\n                (tokenize.NAME, '__case'),\n                (tokenize.NAME, 'in'),\n                (tokenize.NAME, '_Switch'),\n                (tokenize.OP, '(')\n            ])\n            replacing_keyword = True\n        elif toktype == tokenize.NAME and (tokvalue == 'case' or tokvalue == 'default'):\n            result.extend([\n                (tokenize.NAME, 'if'),\n                (tokenize.NAME, '__case'),\n                (tokenize.OP, '(')\n            ])\n            replacing_keyword = True\n        elif replacing_keyword and tokvalue == ':':\n            result.extend([\n                (tokenize.OP, ')'),\n                (tokenize.OP, ':')\n            ])\n            replacing_keyword = False\n        else:\n            result.append((toktype, tokvalue))\n    return tokenize.untokenize(result)"}
{"func_code_string": "def search(self, base, scope, filterstr='(objectClass=*)',\n               attrlist=None, limit=None) -> Generator[Tuple[str, dict], None, None]:\n        \"\"\"\n        Search for entries in LDAP database.\n        \"\"\"\n\n        _debug(\"search\", base, scope, filterstr, attrlist, limit)\n\n        # first results\n        if attrlist is None:\n            attrlist = ldap3.ALL_ATTRIBUTES\n        elif isinstance(attrlist, set):\n            attrlist = list(attrlist)\n\n        def first_results(obj):\n            _debug(\"---> searching ldap\", limit)\n            obj.search(\n                base, filterstr, scope, attributes=attrlist, paged_size=limit)\n            return obj.response\n\n        # get the 1st result\n        result_list = self._do_with_retry(first_results)\n\n        # Loop over list of search results\n        for result_item in result_list:\n            # skip searchResRef for now\n            if result_item['type'] != \"searchResEntry\":\n                continue\n            dn = result_item['dn']\n            attributes = result_item['raw_attributes']\n            # did we already retrieve this from cache?\n            _debug(\"---> got ldap result\", dn)\n            _debug(\"---> yielding\", result_item)\n            yield (dn, attributes)\n\n        # we are finished - return results, eat cake\n        _debug(\"---> done\")\n        return"}
{"func_code_string": "def rename(self, dn: str, new_rdn: str, new_base_dn: Optional[str] = None) -> None:\n        \"\"\"\n        rename a dn in the ldap database; see ldap module. doesn't return a\n        result if transactions enabled.\n        \"\"\"\n        raise NotImplementedError()"}
{"func_code_string": "def prepare_env(org):\n    \"\"\" Example shows how to configure environment from scratch \"\"\"\n\n    # Add services\n    key_service = org.service(type='builtin:cobalt_secure_store', name='Keystore')\n    wf_service = org.service(type='builtin:workflow_service', name='Workflow', parameters='{}')\n\n    # Add services to environment\n    env = org.environment(name='default')\n    env.clean()\n    env.add_service(key_service)\n    env.add_service(wf_service)\n    env.add_policy(\n        {\"action\": \"provisionVms\",\n         \"parameter\": \"publicKeyId\",\n         \"value\": key_service.regenerate()['id']})\n\n    # Add cloud provider account\n    access = {\n      \"provider\": \"aws-ec2\",\n      \"usedEnvironments\": [],\n      \"ec2SecurityGroup\": \"default\",\n      \"providerCopy\": \"aws-ec2\",\n      \"name\": \"test-provider\",\n      \"jcloudsIdentity\": KEY,\n      \"jcloudsCredential\": SECRET_KEY,\n      \"jcloudsRegions\": \"us-east-1\"\n    }\n    prov = org.provider(access)\n    env.add_provider(prov)\n    return org.organizationId"}
{"func_code_string": "def start(ctx, debug, version, config):\n    \"\"\"Commands for devops operations\"\"\"\n    ctx.obj = {}\n    ctx.DEBUG = debug\n    if os.path.isfile(config):\n        with open(config) as fp:\n            agile = json.load(fp)\n    else:\n        agile = {}\n    ctx.obj['agile'] = agile\n    if version:\n        click.echo(__version__)\n        ctx.exit(0)\n    if not ctx.invoked_subcommand:\n        click.echo(ctx.get_help())"}
{"func_code_string": "def duplicate(obj, value=None, field=None, duplicate_order=None):\n\n    \"\"\"\n    Duplicate all related objects of obj setting\n    field to value. If one of the duplicate\n    objects has an FK to another duplicate object\n    update that as well. Return the duplicate copy\n    of obj.\n    duplicate_order is a list of models which specify how\n    the duplicate objects are saved. For complex objects\n    this can matter. Check to save if objects are being\n    saved correctly and if not just pass in related objects\n    in the order that they should be saved.\n    \"\"\"\n    using = router.db_for_write(obj._meta.model)\n    collector = CloneCollector(using=using)\n    collector.collect([obj])\n    collector.sort()\n    related_models = list(collector.data.keys())\n    data_snapshot = {}\n    for key in collector.data.keys():\n        data_snapshot.update({\n            key: dict(zip(\n                [item.pk for item in collector.data[key]], [item for item in collector.data[key]]))\n        })\n    root_obj = None\n\n    # Sometimes it's good enough just to save in reverse deletion order.\n    if duplicate_order is None:\n        duplicate_order = reversed(related_models)\n\n    for model in duplicate_order:\n        # Find all FKs on model that point to a related_model.\n        fks = []\n        for f in model._meta.fields:\n            if isinstance(f, ForeignKey) and f.rel.to in related_models:\n                fks.append(f)\n        # Replace each `sub_obj` with a duplicate.\n        if model not in collector.data:\n            continue\n        sub_objects = collector.data[model]\n        for obj in sub_objects:\n            for fk in fks:\n                fk_value = getattr(obj, \"%s_id\" % fk.name)\n                # If this FK has been duplicated then point to the duplicate.\n                fk_rel_to = data_snapshot[fk.rel.to]\n                if fk_value in fk_rel_to:\n                    dupe_obj = fk_rel_to[fk_value]\n                    setattr(obj, fk.name, dupe_obj)\n            # Duplicate the object and save it.\n            obj.id = None\n            if field is not None:\n                setattr(obj, field, value)\n            obj.save()\n            if root_obj is None:\n                root_obj = obj\n    return root_obj"}
{"func_code_string": "def getPayloadStruct(self, attributes, objType):\n        \"\"\" Function getPayloadStruct\n        Get the payload structure to do a creation or a modification\n\n        @param attribute: The data\n        @param objType: SubItem type (e.g: hostgroup for hostgroup_class)\n        @return RETURN: the payload\n        \"\"\"\n        payload = {self.payloadObj: attributes,\n                   objType + \"_class\":\n                       {self.payloadObj: attributes}}\n        return payload"}
{"func_code_string": "def validate_url(value):\n    \"\"\" Validate url. \"\"\"\n    if not re.match(VIMEO_URL_RE, value) and not re.match(YOUTUBE_URL_RE, value):\n        raise ValidationError('Invalid URL - only Youtube, Vimeo can be used.')"}
{"func_code_string": "def enter_transaction_management(using=None):\n    \"\"\"\n    Enters transaction management for a running thread. It must be balanced\n    with the appropriate leave_transaction_management call, since the actual\n    state is managed as a stack.\n\n    The state and dirty flag are carried over from the surrounding block or\n    from the settings, if there is no surrounding block (dirty is always false\n    when no current block is running).\n    \"\"\"\n    if using is None:\n        for using in tldap.backend.connections:\n            connection = tldap.backend.connections[using]\n            connection.enter_transaction_management()\n        return\n    connection = tldap.backend.connections[using]\n    connection.enter_transaction_management()"}
{"func_code_string": "def leave_transaction_management(using=None):\n    \"\"\"\n    Leaves transaction management for a running thread. A dirty flag is carried\n    over to the surrounding block, as a commit will commit all changes, even\n    those from outside. (Commits are on connection level.)\n    \"\"\"\n    if using is None:\n        for using in tldap.backend.connections:\n            connection = tldap.backend.connections[using]\n            connection.leave_transaction_management()\n        return\n    connection = tldap.backend.connections[using]\n    connection.leave_transaction_management()"}
{"func_code_string": "def is_dirty(using=None):\n    \"\"\"\n    Returns True if the current transaction requires a commit for changes to\n    happen.\n    \"\"\"\n    if using is None:\n        dirty = False\n        for using in tldap.backend.connections:\n            connection = tldap.backend.connections[using]\n            if connection.is_dirty():\n                dirty = True\n        return dirty\n    connection = tldap.backend.connections[using]\n    return connection.is_dirty()"}
{"func_code_string": "def is_managed(using=None):\n    \"\"\"\n    Checks whether the transaction manager is in manual or in auto state.\n    \"\"\"\n    if using is None:\n        managed = False\n        for using in tldap.backend.connections:\n            connection = tldap.backend.connections[using]\n            if connection.is_managed():\n                managed = True\n        return managed\n    connection = tldap.backend.connections[using]\n    return connection.is_managed()"}
{"func_code_string": "def commit(using=None):\n    \"\"\"\n    Does the commit itself and resets the dirty flag.\n    \"\"\"\n    if using is None:\n        for using in tldap.backend.connections:\n            connection = tldap.backend.connections[using]\n            connection.commit()\n        return\n    connection = tldap.backend.connections[using]\n    connection.commit()"}
{"func_code_string": "def rollback(using=None):\n    \"\"\"\n    This function does the rollback itself and resets the dirty flag.\n    \"\"\"\n    if using is None:\n        for using in tldap.backend.connections:\n            connection = tldap.backend.connections[using]\n            connection.rollback()\n        return\n    connection = tldap.backend.connections[using]\n    connection.rollback()"}
{"func_code_string": "def _transaction_func(entering, exiting, using):\n    \"\"\"\n    Takes 3 things, an entering function (what to do to start this block of\n    transaction management), an exiting function (what to do to end it, on both\n    success and failure, and using which can be: None, indiciating transaction\n    should occur on all defined servers, or a callable, indicating that using\n    is None and to return the function already wrapped.\n\n    Returns either a Transaction objects, which is both a decorator and a\n    context manager, or a wrapped function, if using is a callable.\n    \"\"\"\n    # Note that although the first argument is *called* `using`, it\n    # may actually be a function; @autocommit and @autocommit('foo')\n    # are both allowed forms.\n    if callable(using):\n        return Transaction(entering, exiting, None)(using)\n    return Transaction(entering, exiting, using)"}
{"func_code_string": "def commit_on_success(using=None):\n    \"\"\"\n    This decorator activates commit on response. This way, if the view function\n    runs successfully, a commit is made; if the viewfunc produces an exception,\n    a rollback is made. This is one of the most common ways to do transaction\n    control in Web apps.\n    \"\"\"\n    def entering(using):\n        enter_transaction_management(using=using)\n\n    def exiting(exc_value, using):\n        try:\n            if exc_value is not None:\n                if is_dirty(using=using):\n                    rollback(using=using)\n            else:\n                commit(using=using)\n        finally:\n            leave_transaction_management(using=using)\n\n    return _transaction_func(entering, exiting, using)"}
{"func_code_string": "def commit_manually(using=None):\n    \"\"\"\n    Decorator that activates manual transaction control. It just disables\n    automatic transaction control and doesn't do any commit/rollback of its\n    own -- it's up to the user to call the commit and rollback functions\n    themselves.\n    \"\"\"\n    def entering(using):\n        enter_transaction_management(using=using)\n\n    def exiting(exc_value, using):\n        leave_transaction_management(using=using)\n\n    return _transaction_func(entering, exiting, using)"}
{"func_code_string": "def run(self) -> Generator[Tuple[int, int, str, type], None, None]:\n        \"\"\"\n        Yields:\n            tuple (line_number: int, offset: int, text: str, check: type)\n        \"\"\"\n        if is_test_file(self.filename):\n            self.load()\n            for func in self.all_funcs():\n                try:\n                    for error in func.check_all():\n                        yield (error.line_number, error.offset, error.text, Checker)\n                except ValidationError as error:\n                    yield error.to_flake8(Checker)"}
{"func_code_string": "def process_request(self, request):\n        \"\"\"\n        Reloads glitter URL patterns if page URLs change.\n\n        Avoids having to restart the server to recreate the glitter URLs being used by Django.\n        \"\"\"\n        global _urlconf_pages\n\n        page_list = list(\n            Page.objects.exclude(glitter_app_name='').values_list('id', 'url').order_by('id')\n        )\n\n        with _urlconf_lock:\n            if page_list != _urlconf_pages:\n                glitter_urls = 'glitter.urls'\n                if glitter_urls in sys.modules:\n                    importlib.reload(sys.modules[glitter_urls])\n                _urlconf_pages = page_list"}
{"func_code_string": "def run(self):\n        \"\"\"\n        Execute all current and future payloads\n\n        Blocks and executes payloads until :py:meth:`stop` is called.\n        It is an error for any orphaned payload to return or raise.\n        \"\"\"\n        self._logger.info('runner started: %s', self)\n        try:\n            with self._lock:\n                assert not self.running.is_set() and self._stopped.is_set(), 'cannot re-run: %s' % self\n                self.running.set()\n                self._stopped.clear()\n            self._run()\n        except Exception:\n            self._logger.exception('runner aborted: %s', self)\n            raise\n        else:\n            self._logger.info('runner stopped: %s', self)\n        finally:\n            with self._lock:\n                self.running.clear()\n                self._stopped.set()"}
{"func_code_string": "def stop(self):\n        \"\"\"Stop execution of all current and future payloads\"\"\"\n        if not self.running.wait(0.2):\n            return\n        self._logger.debug('runner disabled: %s', self)\n        with self._lock:\n            self.running.clear()\n        self._stopped.wait()"}
{"func_code_string": "def delimit_words(string: str) -> Generator[str, None, None]:\n    \"\"\"\n    Delimit a string at word boundaries.\n\n    ::\n\n        >>> import uqbar.strings\n        >>> list(uqbar.strings.delimit_words(\"i want to believe\"))\n        ['i', 'want', 'to', 'believe']\n\n    ::\n\n        >>> list(uqbar.strings.delimit_words(\"S3Bucket\"))\n        ['S3', 'Bucket']\n\n    ::\n\n        >>> list(uqbar.strings.delimit_words(\"Route53\"))\n        ['Route', '53']\n\n    \"\"\"\n    # TODO: Reimplement this\n    wordlike_characters = (\"<\", \">\", \"!\")\n    current_word = \"\"\n    for i, character in enumerate(string):\n        if (\n            not character.isalpha()\n            and not character.isdigit()\n            and character not in wordlike_characters\n        ):\n            if current_word:\n                yield current_word\n                current_word = \"\"\n        elif not current_word:\n            current_word += character\n        elif character.isupper():\n            if current_word[-1].isupper():\n                current_word += character\n            else:\n                yield current_word\n                current_word = character\n        elif character.islower():\n            if current_word[-1].isalpha():\n                current_word += character\n            else:\n                yield current_word\n                current_word = character\n        elif character.isdigit():\n            if current_word[-1].isdigit() or current_word[-1].isupper():\n                current_word += character\n            else:\n                yield current_word\n                current_word = character\n        elif character in wordlike_characters:\n            if current_word[-1] in wordlike_characters:\n                current_word += character\n            else:\n                yield current_word\n                current_word = character\n    if current_word:\n        yield current_word"}
{"func_code_string": "def normalize(string: str) -> str:\n    \"\"\"\n    Normalizes whitespace.\n\n    Strips leading and trailing blank lines, dedents, and removes trailing\n    whitespace from the result.\n    \"\"\"\n    string = string.replace(\"\\t\", \"    \")\n    lines = string.split(\"\\n\")\n    while lines and (not lines[0] or lines[0].isspace()):\n        lines.pop(0)\n    while lines and (not lines[-1] or lines[-1].isspace()):\n        lines.pop()\n    for i, line in enumerate(lines):\n        lines[i] = line.rstrip()\n    string = \"\\n\".join(lines)\n    string = textwrap.dedent(string)\n    return string"}
{"func_code_string": "def to_dash_case(string: str) -> str:\n    \"\"\"\n    Convert a string to dash-delimited words.\n\n    ::\n\n        >>> import uqbar.strings\n        >>> string = 'T\u00f4 \u0110\u1eb7c Bi\u1ec7t Xe L\u1eeda'\n        >>> print(uqbar.strings.to_dash_case(string))\n        to-dac-biet-xe-lua\n\n    ::\n\n        >>> string = 'alpha.beta.gamma'\n        >>> print(uqbar.strings.to_dash_case(string))\n        alpha-beta-gamma\n\n    \"\"\"\n    string = unidecode.unidecode(string)\n    words = (_.lower() for _ in delimit_words(string))\n    string = \"-\".join(words)\n    return string"}
{"func_code_string": "def get_lib2to3_fixers():\n    '''returns a list of all fixers found in the lib2to3 library'''\n    fixers = []\n    fixer_dirname = fixer_dir.__path__[0]\n    for name in sorted(os.listdir(fixer_dirname)):\n        if name.startswith(\"fix_\") and name.endswith(\".py\"):\n            fixers.append(\"lib2to3.fixes.\" + name[:-3])\n    return fixers"}
{"func_code_string": "def get_single_fixer(fixname):\n    '''return a single fixer found in the lib2to3 library'''\n    fixer_dirname = fixer_dir.__path__[0]\n    for name in sorted(os.listdir(fixer_dirname)):\n        if (name.startswith(\"fix_\") and name.endswith(\".py\") \n            and fixname == name[4:-3]):\n            return \"lib2to3.fixes.\" + name[:-3]"}
{"func_code_string": "def to_db(self, value):\n        \"\"\" Returns field's single value prepared for saving into a database. \"\"\"\n\n        # ensure value is valid\n        self.validate(value)\n\n        assert isinstance(value, list)\n        value = list(value)\n        for i, v in enumerate(value):\n            value[i] = self.value_to_db(v)\n\n        # return result\n        assert isinstance(value, list)\n        return value"}
{"func_code_string": "def to_python(self, value):\n        \"\"\"\n        Converts the input value into the expected Python data type, raising\n        django.core.exceptions.ValidationError if the data can't be converted.\n        Returns the converted value. Subclasses should override this.\n        \"\"\"\n        assert isinstance(value, list)\n\n        # convert every value in list\n        value = list(value)\n        for i, v in enumerate(value):\n            value[i] = self.value_to_python(v)\n\n        # return result\n        return value"}
{"func_code_string": "def validate(self, value):\n        \"\"\"\n        Validates value and throws ValidationError. Subclasses should override\n        this to provide validation logic.\n        \"\"\"\n        # check object type\n        if not isinstance(value, list):\n            raise tldap.exceptions.ValidationError(\n                \"is not a list and max_instances is %s\" %\n                self._max_instances)\n        # check maximum instances\n        if (self._max_instances is not None and\n                len(value) > self._max_instances):\n            raise tldap.exceptions.ValidationError(\n                \"exceeds max_instances of %d\" %\n                self._max_instances)\n        # check this required value is given\n        if self._required:\n            if len(value) == 0:\n                raise tldap.exceptions.ValidationError(\n                    \"is required\")\n        # validate the value\n        for i, v in enumerate(value):\n            self.value_validate(v)"}
{"func_code_string": "def clean(self, value):\n        \"\"\"\n        Convert the value's type and run validation. Validation errors from\n        to_python and validate are propagated. The correct value is returned if\n        no error is raised.\n        \"\"\"\n        value = self.to_python(value)\n        self.validate(value)\n        return value"}
{"func_code_string": "def value_to_db(self, value):\n        \"\"\" Returns field's single value prepared for saving into a database. \"\"\"\n        if isinstance(value, six.string_types):\n            value = value.encode(\"utf_8\")\n        return value"}
{"func_code_string": "def value_to_python(self, value):\n        \"\"\"\n        Converts the input single value into the expected Python data type,\n        raising django.core.exceptions.ValidationError if the data can't be\n        converted.  Returns the converted value. Subclasses should override\n        this.\n        \"\"\"\n        if not isinstance(value, bytes):\n            raise tldap.exceptions.ValidationError(\"should be a bytes\")\n        value = value.decode(\"utf_8\")\n        return value"}
{"func_code_string": "def value_validate(self, value):\n        \"\"\"\n        Validates value and throws ValidationError. Subclasses should override\n        this to provide validation logic.\n        \"\"\"\n        if not isinstance(value, six.string_types):\n            raise tldap.exceptions.ValidationError(\"should be a string\")"}
{"func_code_string": "def value_to_python(self, value):\n        \"\"\"\n        Converts the input single value into the expected Python data type,\n        raising django.core.exceptions.ValidationError if the data can't be\n        converted.  Returns the converted value. Subclasses should override\n        this.\n        \"\"\"\n        if not isinstance(value, bytes):\n            raise tldap.exceptions.ValidationError(\"should be bytes\")\n        if value is None:\n            return value\n        try:\n            return int(value)\n        except (TypeError, ValueError):\n            raise tldap.exceptions.ValidationError(\"is invalid integer\")"}
{"func_code_string": "def value_to_db(self, value):\n        \"\"\" Returns field's single value prepared for saving into a database. \"\"\"\n        assert isinstance(value, six.integer_types)\n        return str(value).encode(\"utf_8\")"}
{"func_code_string": "def value_validate(self, value):\n        \"\"\"\n        Converts the input single value into the expected Python data type,\n        raising django.core.exceptions.ValidationError if the data can't be\n        converted.  Returns the converted value. Subclasses should override\n        this.\n        \"\"\"\n        if not isinstance(value, six.integer_types):\n            raise tldap.exceptions.ValidationError(\"should be a integer\")\n\n        try:\n            return str(value)\n        except (TypeError, ValueError):\n            raise tldap.exceptions.ValidationError(\"is invalid integer\")"}
{"func_code_string": "def value_to_python(self, value):\n        \"\"\"\n        Converts the input single value into the expected Python data type,\n        raising django.core.exceptions.ValidationError if the data can't be\n        converted.  Returns the converted value. Subclasses should override\n        this.\n        \"\"\"\n        if not isinstance(value, bytes):\n            raise tldap.exceptions.ValidationError(\"should be a bytes\")\n\n        try:\n            value = int(value)\n        except (TypeError, ValueError):\n            raise tldap.exceptions.ValidationError(\"is invalid integer\")\n\n        try:\n            value = datetime.date.fromtimestamp(value * 24 * 60 * 60)\n        except OverflowError:\n            raise tldap.exceptions.ValidationError(\"is too big a date\")\n\n        return value"}
{"func_code_string": "def value_to_db(self, value):\n        \"\"\" Returns field's single value prepared for saving into a database. \"\"\"\n        assert isinstance(value, datetime.date)\n        assert not isinstance(value, datetime.datetime)\n\n        try:\n            value = value - datetime.date(year=1970, month=1, day=1)\n        except OverflowError:\n            raise tldap.exceptions.ValidationError(\"is too big a date\")\n\n        return str(value.days).encode(\"utf_8\")"}
{"func_code_string": "def value_validate(self, value):\n        \"\"\"\n        Converts the input single value into the expected Python data type,\n        raising django.core.exceptions.ValidationError if the data can't be\n        converted.  Returns the converted value. Subclasses should override\n        this.\n        \"\"\"\n        if not isinstance(value, datetime.date):\n            raise tldap.exceptions.ValidationError(\"is invalid date\")\n        # a datetime is also a date but they are not compatable\n        if isinstance(value, datetime.datetime):\n            raise tldap.exceptions.ValidationError(\"should be a date, not a datetime\")"}
{"func_code_string": "def value_to_db(self, value):\n        \"\"\" Returns field's single value prepared for saving into a database. \"\"\"\n        assert isinstance(value, datetime.datetime)\n\n        try:\n            value = value - datetime.datetime(1970, 1, 1)\n        except OverflowError:\n            raise tldap.exceptions.ValidationError(\"is too big a date\")\n\n        value = value.seconds + value.days * 24 * 3600\n        value = str(value).encode(\"utf_8\")\n\n        return value"}
{"func_code_string": "def value_validate(self, value):\n        \"\"\"\n        Converts the input single value into the expected Python data type,\n        raising django.core.exceptions.ValidationError if the data can't be\n        converted.  Returns the converted value. Subclasses should override\n        this.\n        \"\"\"\n        if not isinstance(value, datetime.datetime):\n            raise tldap.exceptions.ValidationError(\"is invalid date time\")"}
{"func_code_string": "def value_to_python(self, value):\n        \"\"\"\n        Converts the input single value into the expected Python data type,\n        raising django.core.exceptions.ValidationError if the data can't be\n        converted.  Returns the converted value. Subclasses should override\n        this.\n        \"\"\"\n        if not isinstance(value, bytes):\n            raise tldap.exceptions.ValidationError(\"should be a bytes\")\n\n        length = len(value) - 8\n        if length % 4 != 0:\n            raise tldap.exceptions.ValidationError(\"Invalid sid\")\n\n        length = length // 4\n\n        array = struct.unpack('<bbbbbbbb' + 'I' * length, value)\n\n        if array[1] != length:\n            raise tldap.exceptions.ValidationError(\"Invalid sid\")\n\n        if array[2:7] != (0, 0, 0, 0, 0):\n            raise tldap.exceptions.ValidationError(\"Invalid sid\")\n\n        array = (\"S\", ) + array[0:1] + array[7:]\n        return \"-\".join([str(i) for i in array])"}
{"func_code_string": "def value_to_db(self, value):\n        \"\"\" Returns field's single value prepared for saving into a database. \"\"\"\n\n        assert isinstance(value, str)\n\n        array = value.split(\"-\")\n        length = len(array) - 3\n\n        assert length >= 0\n        assert array[0] == 'S'\n\n        array = array[1:2] + [length, 0, 0, 0, 0, 0] + array[2:]\n        array = [int(i) for i in array]\n\n        return struct.pack('<bbbbbbbb' + 'I' * length, *array)"}
{"func_code_string": "def value_validate(self, value):\n        \"\"\"\n        Converts the input single value into the expected Python data type,\n        raising django.core.exceptions.ValidationError if the data can't be\n        converted.  Returns the converted value. Subclasses should override\n        this.\n        \"\"\"\n        if not isinstance(value, str):\n            raise tldap.exceptions.ValidationError(\"Invalid sid\")\n\n        array = value.split(\"-\")\n        length = len(array) - 3\n\n        if length < 1:\n            raise tldap.exceptions.ValidationError(\"Invalid sid\")\n\n        if array.pop(0) != \"S\":\n            raise tldap.exceptions.ValidationError(\"Invalid sid\")\n\n        try:\n            [int(i) for i in array]\n        except TypeError:\n            raise tldap.exceptions.ValidationError(\"Invalid sid\")"}
{"func_code_string": "def get(self, id):\n        \"\"\"Get data for this component\n        \"\"\"\n        id = self.as_id(id)\n        url = '%s/%s' % (self, id)\n        response = self.http.get(url, auth=self.auth)\n        response.raise_for_status()\n        return response.json()"}
{"func_code_string": "def create(self, data):\n        \"\"\"Create a new component\n        \"\"\"\n        response = self.http.post(str(self), json=data, auth=self.auth)\n        response.raise_for_status()\n        return response.json()"}
{"func_code_string": "def update(self, id, data):\n        \"\"\"Update a component\n        \"\"\"\n        id = self.as_id(id)\n        response = self.http.patch(\n            '%s/%s' % (self, id), json=data, auth=self.auth\n        )\n        response.raise_for_status()\n        return response.json()"}
{"func_code_string": "def delete(self, id):\n        \"\"\"Delete a component by id\n        \"\"\"\n        id = self.as_id(id)\n        response = self.http.delete(\n            '%s/%s' % (self.api_url, id),\n            auth=self.auth)\n        response.raise_for_status()"}
{"func_code_string": "def get_list(self, url=None, callback=None, limit=100, **data):\n        \"\"\"Get a list of this github component\n        :param url: full url\n        :param Comp: a :class:`.Component` class\n        :param callback: Optional callback\n        :param limit: Optional number of items to retrieve\n        :param data: additional query data\n        :return: a list of ``Comp`` objects with data\n        \"\"\"\n        url = url or str(self)\n        data = dict(((k, v) for k, v in data.items() if v))\n        all_data = []\n        if limit:\n            data['per_page'] = min(limit, 100)\n        while url:\n            response = self.http.get(url, params=data, auth=self.auth)\n            response.raise_for_status()\n            result = response.json()\n            n = m = len(result)\n            if callback:\n                result = callback(result)\n                m = len(result)\n            all_data.extend(result)\n            if limit and len(all_data) > limit:\n                all_data = all_data[:limit]\n                break\n            elif m == n:\n                data = None\n                next = response.links.get('next', {})\n                url = next.get('url')\n            else:\n                break\n        return all_data"}
{"func_code_string": "def comments(self, issue):\n        \"\"\"Return all comments for this issue/pull request\n        \"\"\"\n        commit = self.as_id(issue)\n        return self.get_list(url='%s/%s/comments' % (self, commit))"}
{"func_code_string": "def has_edit_permission(self, request, obj=None, version=None):\n        \"\"\"\n        Returns a boolean if the user in the request has edit permission for the object.\n\n        Can also be passed a version object to check if the user has permission to edit a version\n        of the object (if they own it).\n        \"\"\"\n        # Has the edit permission for this object type\n        permission_name = '{}.edit_{}'.format(self.opts.app_label, self.opts.model_name)\n        has_permission = request.user.has_perm(permission_name)\n\n        if obj is not None and has_permission is False:\n            has_permission = request.user.has_perm(permission_name, obj=obj)\n\n        if has_permission and version is not None:\n            # Version must not be saved, and must belong to this user\n            if version.version_number or version.owner != request.user:\n                has_permission = False\n\n        return has_permission"}
{"func_code_string": "def has_publish_permission(self, request, obj=None):\n        \"\"\"\n        Returns a boolean if the user in the request has publish permission for the object.\n        \"\"\"\n        permission_name = '{}.publish_{}'.format(self.opts.app_label, self.opts.model_name)\n        has_permission = request.user.has_perm(permission_name)\n\n        if obj is not None and has_permission is False:\n            has_permission = request.user.has_perm(permission_name, obj=obj)\n\n        return has_permission"}
{"func_code_string": "def semantic_version(tag):\n    \"\"\"Get a valid semantic version for tag\n    \"\"\"\n    try:\n        version = list(map(int, tag.split('.')))\n        assert len(version) == 3\n        return tuple(version)\n    except Exception as exc:\n        raise CommandError(\n            'Could not parse \"%s\", please use '\n            'MAJOR.MINOR.PATCH' % tag\n        ) from exc"}
{"func_code_string": "def load(self, data):\n        \"\"\" Function load\n        Store the object data\n        \"\"\"\n        self.clear()\n        self.update(data)\n        self.enhance()"}
{"func_code_string": "def enhance(self):\n        \"\"\" Function enhance\n        Enhance the object with new item or enhanced items\n        \"\"\"\n        if self.objName in ['hosts', 'hostgroups',\n                            'puppet_classes']:\n            from foreman.itemSmartClassParameter\\\n                import ItemSmartClassParameter\n            self.update({'smart_class_parameters':\n                        SubDict(self.api, self.objName,\n                                self.payloadObj, self.key,\n                                ItemSmartClassParameter)})"}
{"func_code_string": "def reload(self):\n        \"\"\" Function reload\n        Sync the full object\n        \"\"\"\n        self.load(self.api.get(self.objName, self.key))"}
{"func_code_string": "def getParam(self, name=None):\n        \"\"\" Function getParam\n        Return a dict of parameters or a parameter value\n\n        @param key: The parameter name\n        @return RETURN: dict of parameters or a parameter value\n        \"\"\"\n        if 'parameters' in self.keys():\n            l = {x['name']: x['value'] for x in self['parameters'].values()}\n            if name:\n                if name in l.keys():\n                    return l[name]\n                else:\n                    return False\n            else:\n                return l"}
{"func_code_string": "def checkAndCreateClasses(self, classes):\n        \"\"\" Function checkAndCreateClasses\n        Check and add puppet class\n\n        @param classes: The classes ids list\n        @return RETURN: boolean\n        \"\"\"\n        actual_classes = self['puppetclasses'].keys()\n        for i in classes:\n            if i not in actual_classes:\n                self['puppetclasses'].append(i)\n        self.reload()\n        return set(classes).issubset(set((self['puppetclasses'].keys())))"}
{"func_code_string": "def checkAndCreateParams(self, params):\n        \"\"\" Function checkAndCreateParams\n        Check and add global parameters\n\n        @param key: The parameter name\n        @param params: The params dict\n        @return RETURN: boolean\n        \"\"\"\n        actual_params = self['parameters'].keys()\n        for k, v in params.items():\n            if k not in actual_params:\n                self['parameters'].append({\"name\": k, \"value\": v})\n        self.reload()\n        return self['parameters'].keys() == params.keys()"}
{"func_code_string": "def version_dict(version):\n    \"\"\"Turn a version string into a dict with major/minor/... info.\"\"\"\n    match = version_re.match(str(version) or '')\n    letters = 'alpha pre'.split()\n    numbers = 'major minor1 minor2 minor3 alpha_ver pre_ver'.split()\n    if match:\n        d = match.groupdict()\n        for letter in letters:\n            d[letter] = d[letter] if d[letter] else None\n        for num in numbers:\n            if d[num] == '*':\n                d[num] = 99\n            else:\n                d[num] = int(d[num]) if d[num] else None\n    else:\n        d = dict((k, None) for k in numbers)\n        d.update((k, None) for k in letters)\n    return d"}
{"func_code_string": "def get_diff(source, dest):\n    \"\"\"Get the diff between two records list in this order:\n        - to_create\n        - to_delete\n    \"\"\"\n    # First build a dict from the lists, with the ID as the key.\n    source_dict = {record['id']: record for record in source}\n    dest_dict = {record['id']: record for record in dest}\n\n    source_keys = set(source_dict.keys())\n    dest_keys = set(dest_dict.keys())\n    to_create = source_keys - dest_keys\n    to_delete = dest_keys - source_keys\n    to_update = set()\n\n    to_check = source_keys - to_create - to_delete\n\n    for record_id in to_check:\n        # Make sure to remove properties that are part of kinto\n        # records and not amo records.\n        # Here we will compare the record properties ignoring:\n        # ID, last_modified and enabled.\n        new = canonical_json(source_dict[record_id])\n        old = canonical_json(dest_dict[record_id])\n        if new != old:\n            to_update.add(record_id)\n\n    return ([source_dict[k] for k in to_create],\n            [source_dict[k] for k in to_update],\n            [dest_dict[k] for k in to_delete])"}
{"func_code_string": "def object_version_choices(obj):\n    \"\"\"\n    Return a list of form choices for versions of this object which can be published.\n    \"\"\"\n    choices = BLANK_CHOICE_DASH + [(PublishAction.UNPUBLISH_CHOICE, 'Unpublish current version')]\n\n    # When creating a new object in the Django admin - obj will be None\n    if obj is not None:\n        saved_versions = Version.objects.filter(\n            content_type=ContentType.objects.get_for_model(obj),\n            object_id=obj.pk,\n        ).exclude(\n            version_number=None,\n        )\n\n        for version in saved_versions:\n            choices.append((version.version_number, version))\n\n    return choices"}
{"func_code_string": "def manifest(self, values, *paths, filename: str = None) -> Dict:\n        \"\"\"Load a manifest file and apply template values\n        \"\"\"\n        filename = filename or self.filename(*paths)\n        with open(filename, 'r') as fp:\n            template = Template(fp.read())\n        return yaml.load(template.render(values))"}
{"func_code_string": "def set_packet_headers(self, headers):\n        \"\"\" Set packet header.\n\n        The method will try to set ps_headerprotocol to inform the Xena GUI and tester how to interpret the packet\n        header byte sequence specified with PS_PACKETHEADER.\n        This is mainly for information purposes, and the stream will transmit the packet header bytes even if no\n        protocol segments are specified.\n        If the method fails to set some segment it will log a warning and skip setup.\n\n        :param headers: current packet headers\n        :type headers: pypacker.layer12.ethernet.Ethernet\n        \"\"\"\n\n        bin_headers = '0x' + binascii.hexlify(headers.bin()).decode('utf-8')\n        self.set_attributes(ps_packetheader=bin_headers)\n\n        body_handler = headers\n        ps_headerprotocol = []\n        while body_handler:\n            segment = pypacker_2_xena.get(str(body_handler).split('(')[0].lower(), None)\n            if not segment:\n                self.logger.warning('pypacker header {} not in conversion list'.format(segment))\n                return\n            ps_headerprotocol.append(segment)\n            if type(body_handler) is Ethernet and body_handler.vlan:\n                ps_headerprotocol.append('vlan')\n            body_handler = body_handler.body_handler\n        self.set_attributes(ps_headerprotocol=' '.join(ps_headerprotocol))"}
{"func_code_string": "def add_modifier(self, m_type=XenaModifierType.standard, **kwargs):\n        \"\"\" Add modifier.\n\n        :param m_type: modifier type - standard or extended.\n        :type: xenamanager.xena_stram.ModifierType\n        :return: newly created modifier.\n        :rtype: xenamanager.xena_stream.XenaModifier\n        \"\"\"\n\n        if m_type == XenaModifierType.standard:\n            modifier = XenaModifier(self, index='{}/{}'.format(self.index, len(self.modifiers)))\n        else:\n            modifier = XenaXModifier(self, index='{}/{}'.format(self.index, len(self.xmodifiers)))\n        modifier._create()\n        modifier.get()\n        modifier.set(**kwargs)\n        return modifier"}
{"func_code_string": "def remove_modifier(self, index, m_type=XenaModifierType.standard):\n        \"\"\" Remove modifier.\n\n        :param m_type: modifier type - standard or extended.\n        :param index: index of modifier to remove.\n        \"\"\"\n\n        if m_type == XenaModifierType.standard:\n            current_modifiers = OrderedDict(self.modifiers)\n            del current_modifiers[index]\n\n            self.set_attributes(ps_modifiercount=0)\n            self.del_objects_by_type('modifier')\n\n        else:\n            current_modifiers = OrderedDict(self.xmodifiers)\n            del current_modifiers[index]\n\n            self.set_attributes(ps_modifierextcount=0)\n            self.del_objects_by_type('xmodifier')\n\n        for modifier in current_modifiers.values():\n            self.add_modifier(m_type,\n                              mask=modifier.mask, action=modifier.action, repeat=modifier.repeat,\n                              min_val=modifier.min_val, step=modifier.step, max_val=modifier.max_val)"}
{"func_code_string": "def modifiers(self):\n        \"\"\"\n        :return: dictionary {index: object} of standard modifiers.\n        \"\"\"\n        if not self.get_objects_by_type('modifier'):\n            for index in range(int(self.get_attribute('ps_modifiercount'))):\n                XenaModifier(self, index='{}/{}'.format(self.index, index)).get()\n        return {s.id: s for s in self.get_objects_by_type('modifier')}"}
{"func_code_string": "def xmodifiers(self):\n        \"\"\"\n        :return: dictionary {index: object} of extended modifiers.\n        \"\"\"\n        if not self.get_objects_by_type('xmodifier'):\n            try:\n                for index in range(int(self.get_attribute('ps_modifierextcount'))):\n                    XenaXModifier(self, index='{}/{}'.format(self.index, index)).get()\n            except Exception as _:\n                pass\n        return {s.id: s for s in self.get_objects_by_type('xmodifier')}"}
{"func_code_string": "def DRAGONS(flat=False, extras=True):\n    \"\"\"DRAGONS cosmology assumes WMAP7 + BAO + H_0 mean from\n    Komatsu et al. (2011) ApJS 192 18K (arxiv:1001.4538v1)\n\n    Parameters\n    ----------\n\n    flat: boolean\n\n      If True, sets omega_lambda_0 = 1 - omega_M_0 to ensure omega_k_0\n      = 0 exactly. Also sets omega_k_0 = 0 explicitly.\n\n    extras: boolean\n\n      If True, sets neutrino number N_nu = 0, neutrino density\n      omega_n_0 = 0.0, Helium mass fraction Y_He = 0.24.\n\n      \"\"\"\n    omega_c_0 = 0.2292\n    omega_b_0 = 0.0458\n    cosmo = {'omega_b_0': omega_b_0,\n             'omega_M_0': omega_b_0 + omega_c_0,\n             'omega_lambda_0': 0.725,\n             'h': 0.702,\n             'n': 0.963,\n             'sigma_8': 0.816,\n             'tau': 0.088,\n             'z_reion': 10.6,\n             't_0': 13.76,\n             }\n    if flat:\n        cosmo['omega_lambda_0'] = 1 - cosmo['omega_M_0']\n        cosmo['omega_k_0'] = 0.0\n    if extras:\n        add_extras(cosmo)\n    return cosmo"}
{"func_code_string": "def Planck_2015(flat=False, extras=True):\n    \"\"\"Planck 2015 XII: Cosmological parameters Table 4\n    column Planck TT, TE, EE + lowP + lensing + ext\n    from Ade et al. (2015) A&A in press (arxiv:1502.01589v1)\n\n    Parameters\n    ----------\n\n    flat: boolean\n\n      If True, sets omega_lambda_0 = 1 - omega_M_0 to ensure omega_k_0\n      = 0 exactly. Also sets omega_k_0 = 0 explicitly.\n\n    extras: boolean\n\n      If True, sets neutrino number N_nu = 0, neutrino density\n      omega_n_0 = 0.0, Helium mass fraction Y_He = 0.24.\n\n      \"\"\"\n    omega_b_0 = 0.02230/(0.6774**2)\n    cosmo = {'omega_b_0': omega_b_0,\n             'omega_M_0': 0.3089,\n             'omega_lambda_0': 0.6911,\n             'h': 0.6774,\n             'n': 0.9667,\n             'sigma_8': 0.8159,\n             'tau': 0.066,\n             'z_reion': 8.8,\n             't_0': 13.799,\n             }\n    if flat:\n        cosmo['omega_lambda_0'] = 1 - cosmo['omega_M_0']\n        cosmo['omega_k_0'] = 0.0\n    if extras:\n        add_extras(cosmo)\n    return cosmo"}
{"func_code_string": "def add(self, dn: str, mod_list: dict) -> None:\n        \"\"\"\n        Add a DN to the LDAP database; See ldap module. Doesn't return a result\n        if transactions enabled.\n        \"\"\"\n\n        return self._do_with_retry(lambda obj: obj.add_s(dn, mod_list))"}
{"func_code_string": "def modify(self, dn: str, mod_list: dict) -> None:\n        \"\"\"\n        Modify a DN in the LDAP database; See ldap module. Doesn't return a\n        result if transactions enabled.\n        \"\"\"\n\n        return self._do_with_retry(lambda obj: obj.modify_s(dn, mod_list))"}
{"func_code_string": "def delete(self, dn: str) -> None:\n        \"\"\"\n        delete a dn in the ldap database; see ldap module. doesn't return a\n        result if transactions enabled.\n        \"\"\"\n\n        return self._do_with_retry(lambda obj: obj.delete_s(dn))"}
{"func_code_string": "def rename(self, dn: str, new_rdn: str, new_base_dn: Optional[str] = None) -> None:\n        \"\"\"\n        rename a dn in the ldap database; see ldap module. doesn't return a\n        result if transactions enabled.\n        \"\"\"\n\n        return self._do_with_retry(\n            lambda obj: obj.rename_s(dn, new_rdn, new_base_dn))"}
{"func_code_string": "def get_column_name(self, column_name):\n        \"\"\" Get a column for given column name from META api. \"\"\"\n        name = pretty_name(column_name)\n        if column_name in self._meta.columns:\n            column_cls = self._meta.columns[column_name]\n            if column_cls.verbose_name:\n                name = column_cls.verbose_name\n        return name"}
{"func_code_string": "def version(self):\n        \"\"\"Software version of the current repository\n        \"\"\"\n        branches = self.branches()\n        if self.info['branch'] == branches.sandbox:\n            try:\n                return self.software_version()\n            except Exception as exc:\n                raise utils.CommandError(\n                    'Could not obtain repo version, do you have a makefile '\n                    'with version entry?\\n%s' % exc\n                )\n        else:\n            branch = self.info['branch'].lower()\n            branch = re.sub('[^a-z0-9_-]+', '-', branch)\n            return f\"{branch}-{self.info['head']['id'][:8]}\""}
{"func_code_string": "def validate_version(self, prefix='v'):\n        \"\"\"Validate version by checking if it is a valid semantic version\n        and its value is higher than latest github tag\n        \"\"\"\n        version = self.software_version()\n        repo = self.github_repo()\n        repo.releases.validate_tag(version, prefix)\n        return version"}
{"func_code_string": "def skip_build(self):\n        \"\"\"Check if build should be skipped\n        \"\"\"\n        skip_msg = self.config.get('skip', '[ci skip]')\n        return (\n            os.environ.get('CODEBUILD_BUILD_SUCCEEDING') == '0' or\n            self.info['current_tag'] or\n            skip_msg in self.info['head']['message']\n        )"}
{"func_code_string": "def message(self, msg):\n        \"\"\"Send a message to third party applications\n        \"\"\"\n        for broker in self.message_brokers:\n            try:\n                broker(msg)\n            except Exception as exc:\n                utils.error(exc)"}
{"func_code_string": "def get_kinto_records(kinto_client, bucket, collection, permissions,\n                      config=None):\n    \"\"\"Return all the kinto records for this bucket/collection.\"\"\"\n    # Create bucket if needed\n    try:\n        kinto_client.create_bucket(id=bucket, if_not_exists=True)\n    except KintoException as e:\n        if hasattr(e, 'response') and e.response.status_code == 403:\n            # The user cannot create buckets on this server, ignore the creation.\n            pass\n\n    try:\n        kinto_client.create_collection(id=collection, bucket=bucket,\n                                       permissions=permissions, if_not_exists=True)\n    except KintoException as e:\n        if hasattr(e, 'response') and e.response.status_code == 403:\n            # The user cannot create collection on this bucket, ignore the creation.\n            pass\n\n    return kinto_client.get_records(bucket=bucket, collection=collection)"}
{"func_code_string": "def add_chassis(self, chassis):\n        \"\"\"\n        :param ip: chassis object\n        \"\"\"\n\n        self.chassis_list[chassis] = XenaSocket(self.logger, chassis.ip, chassis.port)\n        self.chassis_list[chassis].connect()\n        KeepAliveThread(self.chassis_list[chassis]).start()\n        self.send_command(chassis, 'c_logon', '\"{}\"'.format(chassis.password))\n        self.send_command(chassis, 'c_owner', '\"{}\"'.format(chassis.owner))"}
{"func_code_string": "def send_command(self, obj, command, *arguments):\n        \"\"\" Send command and do not parse output (except for communication errors).\n\n        :param obj: requested object.\n        :param command: command to send.\n        :param arguments: list of command arguments.\n        \"\"\"\n        index_command = obj._build_index_command(command, *arguments)\n        self.chassis_list[obj.chassis].sendQueryVerify(index_command)"}
{"func_code_string": "def send_command_return(self, obj, command, *arguments):\n        \"\"\" Send command and wait for single line output. \"\"\"\n        index_command = obj._build_index_command(command, *arguments)\n        return obj._extract_return(command, self.chassis_list[obj.chassis].sendQuery(index_command))"}
{"func_code_string": "def send_command_return_multilines(self, obj, command, *arguments):\n        \"\"\" Send command and wait for multiple lines output. \"\"\"\n        index_command = obj._build_index_command(command, *arguments)\n        return self.chassis_list[obj.chassis].sendQuery(index_command, True)"}
{"func_code_string": "def get_attribute(self, obj, attribute):\n        \"\"\" Returns single object attribute.\n\n        :param obj: requested object.\n        :param attribute: requested attribute to query.\n        :returns: returned value.\n        :rtype: str\n        \"\"\"\n        raw_return = self.send_command_return(obj, attribute, '?')\n        if len(raw_return) > 2 and raw_return[0] == '\"' and raw_return[-1] == '\"':\n            return raw_return[1:-1]\n        return raw_return"}
{"func_code_string": "def get_attributes(self, obj):\n        \"\"\" Get all object's attributes.\n\n        Sends multi-parameter info/config queries and returns the result as dictionary.\n\n        :param obj: requested object.\n        :returns: dictionary of <name, value> of all attributes returned by the query.\n        :rtype: dict of (str, str)\n        \"\"\"\n\n        attributes = {}\n        for info_config_command in obj.info_config_commands:\n            index_commands_values = self.send_command_return_multilines(obj, info_config_command, '?')\n            # poor implementation...\n            li = obj._get_index_len()\n            ci = obj._get_command_len()\n            for index_command_value in index_commands_values:\n                command = index_command_value.split()[ci].lower()\n                if len(index_command_value.split()) > li + 1:\n                    value = ' '.join(index_command_value.split()[li+1:]).replace('\"', '')\n                else:\n                    value = ''\n                attributes[command] = value\n        return attributes"}
{"func_code_string": "def set_attributes(self, obj, **attributes):\n        \"\"\" Set attributes.\n\n        :param obj: requested object.\n        :param attributes: dictionary of {attribute: value} to set\n        \"\"\"\n        for attribute, value in attributes.items():\n            self.send_command(obj, attribute, value)"}
{"func_code_string": "def get_stats(self, obj, stat_name):\n        \"\"\" Send CLI command that returns list of integer counters.\n\n        :param obj: requested object.\n        :param stat_name: statistics command name.\n        :return: list of counters.\n        :rtype: list(int)\n        \"\"\"\n        return [int(v) for v in self.get_attribute(obj, stat_name).split()]"}
{"func_code_string": "def depth_first(self, top_down=True):\n        \"\"\"\n        Iterate depth-first.\n\n        ::\n\n            >>> from uqbar.containers import UniqueTreeContainer, UniqueTreeNode\n            >>> root_container = UniqueTreeContainer(name=\"root\")\n            >>> outer_container = UniqueTreeContainer(name=\"outer\")\n            >>> inner_container = UniqueTreeContainer(name=\"inner\")\n            >>> node_a = UniqueTreeNode(name=\"a\")\n            >>> node_b = UniqueTreeNode(name=\"b\")\n            >>> node_c = UniqueTreeNode(name=\"c\")\n            >>> node_d = UniqueTreeNode(name=\"d\")\n            >>> root_container.extend([node_a, outer_container])\n            >>> outer_container.extend([inner_container, node_d])\n            >>> inner_container.extend([node_b, node_c])\n\n        ::\n\n            >>> for node in root_container.depth_first():\n            ...     print(node.name)\n            ...\n            a\n            outer\n            inner\n            b\n            c\n            d\n\n        ::\n\n            >>> for node in root_container.depth_first(top_down=False):\n            ...     print(node.name)\n            ...\n            a\n            b\n            c\n            inner\n            d\n            outer\n\n        \"\"\"\n        for child in tuple(self):\n            if top_down:\n                yield child\n            if isinstance(child, UniqueTreeContainer):\n                yield from child.depth_first(top_down=top_down)\n            if not top_down:\n                yield child"}
{"func_code_string": "def reserve(self, force=False):\n        \"\"\" Reserve port.\n\n        XenaManager-2G -> Reserve/Relinquish Port.\n\n        :param force: True - take forcefully, False - fail if port is reserved by other user\n        \"\"\"\n\n        p_reservation = self.get_attribute('p_reservation')\n        if p_reservation == 'RESERVED_BY_YOU':\n            return\n        elif p_reservation == 'RESERVED_BY_OTHER' and not force:\n            raise TgnError('Port {} reserved by {}'.format(self, self.get_attribute('p_reservedby')))\n        self.relinquish()\n        self.send_command('p_reservation', 'reserve')"}
{"func_code_string": "def load_config(self, config_file_name):\n        \"\"\" Load configuration file from xpc file.\n\n        :param config_file_name: full path to the configuration file.\n        \"\"\"\n\n        with open(config_file_name) as f:\n            commands = f.read().splitlines()\n\n        for command in commands:\n            if not command.startswith(';'):\n                try:\n                    self.send_command(command)\n                except XenaCommandException as e:\n                    self.logger.warning(str(e))"}
{"func_code_string": "def save_config(self, config_file_name):\n        \"\"\" Save configuration file to xpc file.\n\n        :param config_file_name: full path to the configuration file.\n        \"\"\"\n\n        with open(config_file_name, 'w+') as f:\n            f.write('P_RESET\\n')\n            for line in self.send_command_return_multilines('p_fullconfig', '?'):\n                f.write(line.split(' ', 1)[1].lstrip())"}
{"func_code_string": "def add_stream(self, name=None, tpld_id=None, state=XenaStreamState.enabled):\n        \"\"\" Add stream.\n\n        :param name: stream description.\n        :param tpld_id: TPLD ID. If None the a unique value will be set.\n        :param state: new stream state.\n        :type state: xenamanager.xena_stream.XenaStreamState\n        :return: newly created stream.\n        :rtype: xenamanager.xena_stream.XenaStream\n        \"\"\"\n\n        stream = XenaStream(parent=self, index='{}/{}'.format(self.index, len(self.streams)), name=name)\n        stream._create()\n        tpld_id = tpld_id if tpld_id else XenaStream.next_tpld_id\n        stream.set_attributes(ps_comment='\"{}\"'.format(stream.name), ps_tpldid=tpld_id)\n        XenaStream.next_tpld_id = max(XenaStream.next_tpld_id + 1, tpld_id + 1)\n        stream.set_state(state)\n        return stream"}
{"func_code_string": "def read_port_stats(self):\n        \"\"\"\n        :return: dictionary {group name {stat name: value}}.\n            Sea XenaPort.stats_captions.\n        \"\"\"\n\n        stats_with_captions = OrderedDict()\n        for stat_name in self.stats_captions.keys():\n            stats_with_captions[stat_name] = self.read_stat(self.stats_captions[stat_name], stat_name)\n        return stats_with_captions"}
{"func_code_string": "def read_stream_stats(self):\n        \"\"\"\n        :return: dictionary {stream index {stat name: value}}.\n            Sea XenaStream.stats_captions.\n        \"\"\"\n        stream_stats = OrderedDict()\n        for stream in self.streams.values():\n            stream_stats[stream] = stream.read_stats()\n        return stream_stats"}
{"func_code_string": "def read_tpld_stats(self):\n        \"\"\"\n        :return: dictionary {tpld index {group name {stat name: value}}}.\n            Sea XenaTpld.stats_captions.\n        \"\"\"\n        payloads_stats = OrderedDict()\n        for tpld in self.tplds.values():\n            payloads_stats[tpld] = tpld.read_stats()\n        return payloads_stats"}
{"func_code_string": "def streams(self):\n        \"\"\"\n        :return: dictionary {id: object} of all streams.\n        :rtype: dict of (int, xenamanager.xena_stream.XenaStream)\n        \"\"\"\n\n        if not self.get_objects_by_type('stream'):\n            tpld_ids = []\n            for index in self.get_attribute('ps_indices').split():\n                stream = XenaStream(parent=self, index='{}/{}'.format(self.index, index))\n                tpld_ids.append(stream.get_attribute('ps_tpldid'))\n            if tpld_ids:\n                XenaStream.next_tpld_id = max([XenaStream.next_tpld_id] + [int(t) for t in tpld_ids]) + 1\n        return {s.id: s for s in self.get_objects_by_type('stream')}"}
{"func_code_string": "def tplds(self):\n        \"\"\"\n        :return: dictionary {id: object} of all current tplds.\n        :rtype: dict of (int, xenamanager.xena_port.XenaTpld)\n        \"\"\"\n\n        # As TPLDs are dynamic we must re-read them each time from the port.\n        self.parent.del_objects_by_type('tpld')\n        for tpld in self.get_attribute('pr_tplds').split():\n            XenaTpld(parent=self, index='{}/{}'.format(self.index, tpld))\n        return {t.id: t for t in self.get_objects_by_type('tpld')}"}
{"func_code_string": "def read_stats(self):\n        \"\"\"\n        :return: dictionary {group name {stat name: value}}.\n            Sea XenaTpld.stats_captions.\n        \"\"\"\n\n        stats_with_captions = OrderedDict()\n        for stat_name in self.stats_captions.keys():\n            stats_with_captions[stat_name] = self.read_stat(self.stats_captions[stat_name], stat_name)\n        return stats_with_captions"}
{"func_code_string": "def get_packets(self, from_index=0, to_index=None, cap_type=XenaCaptureBufferType.text,\n                    file_name=None, tshark=None):\n        \"\"\" Get captured packets from chassis.\n\n        :param from_index: index of first packet to read.\n        :param to_index: index of last packet to read. If None - read all packets.\n        :param cap_type: returned capture format. If pcap then file name and tshark must be provided.\n        :param file_name: if specified, capture will be saved in file.\n        :param tshark: tshark object for pcap type only.\n        :type: xenamanager.xena_tshark.Tshark\n        :return: list of requested packets, None for pcap type.\n        \"\"\"\n\n        to_index = to_index if to_index else len(self.packets)\n\n        raw_packets = []\n        for index in range(from_index, to_index):\n            raw_packets.append(self.packets[index].get_attribute('pc_packet').split('0x')[1])\n\n        if cap_type == XenaCaptureBufferType.raw:\n            self._save_captue(file_name, raw_packets)\n            return raw_packets\n\n        text_packets = []\n        for raw_packet in raw_packets:\n            text_packet = ''\n            for c, b in zip(range(len(raw_packet)), raw_packet):\n                if c % 32 == 0:\n                    text_packet += '\\n{:06x} '.format(int(c / 2))\n                elif c % 2 == 0:\n                    text_packet += ' '\n                text_packet += b\n            text_packets.append(text_packet)\n\n        if cap_type == XenaCaptureBufferType.text:\n            self._save_captue(file_name, text_packets)\n            return text_packets\n\n        temp_file_name = file_name + '_'\n        self._save_captue(temp_file_name, text_packets)\n        tshark.text_to_pcap(temp_file_name, file_name)\n        os.remove(temp_file_name)"}
{"func_code_string": "def packets(self):\n        \"\"\"\n        :return: dictionary {id: object} of all packets.\n        :rtype: dict of (int, xenamanager.xena_port.XenaCapturePacket)\n        \"\"\"\n\n        if not self.get_object_by_type('cappacket'):\n            for index in range(0, self.read_stats()['packets']):\n                XenaCapturePacket(parent=self, index='{}/{}'.format(self.index, index))\n        return {p.id: p for p in self.get_objects_by_type('cappacket')}"}
{"func_code_string": "def add(self, rule: ControlRule = None, *, supply: float):\n        \"\"\"\n        Register a new rule above a given ``supply`` threshold\n\n        Registration supports a single-argument form for use as a decorator,\n        as well as a two-argument form for direct application.\n        Use the former for ``def`` or ``class`` definitions,\n        and the later for ``lambda`` functions and existing callables.\n\n        .. code:: python\n\n            @control.add(supply=10)\n            def linear(pool, interval):\n                if pool.utilisation < 0.75:\n                    return pool.supply - interval\n                elif pool.allocation > 0.95:\n                    return pool.supply + interval\n\n            control.add(\n                lambda pool, interval: pool.supply * (1.2 if pool.allocation > 0.75 else 0.9),\n                supply=100\n            )\n        \"\"\"\n        if supply in self._thresholds:\n            raise ValueError('rule for threshold %s re-defined' % supply)\n        if rule is not None:\n            self.rules.append((supply, rule))\n            self._thresholds.add(supply)\n            return rule\n        else:\n            return partial(self.add, supply=supply)"}
{"func_code_string": "def s(self, *args, **kwargs) -> Partial[Stepwise]:\n        \"\"\"\n        Create an unbound prototype of this class, partially applying arguments\n\n        .. code:: python\n\n            @stepwise\n            def control(pool: Pool, interval):\n                return 10\n\n            pipeline = control.s(interval=20) >> pool\n\n        :note: The partial rules are sealed, and :py:meth:`~.UnboundStepwise.add`\n               cannot be called on it.\n        \"\"\"\n        return Partial(Stepwise, self.base, *self.rules, *args, **kwargs)"}
{"func_code_string": "def revisionId(self):\n        \"\"\"\n        revisionId differs from id, it is details of implementation use self.id\n        :return: RevisionId\n        \"\"\"\n        log.warning(\"'RevisionId' requested, ensure that you are don't need 'id'\")\n        revision_id = self.json()['revisionId']\n        assert revision_id == self.id, \"RevisionId differs id-{}!=revisionId-{}\".format(self.id, revision_id)\n        return revision_id"}
{"func_code_string": "def changeset(python_data: LdapObject, d: dict) -> Changeset:\n    \"\"\" Generate changes object for ldap object. \"\"\"\n    table: LdapObjectClass = type(python_data)\n    fields = table.get_fields()\n    changes = Changeset(fields, src=python_data, d=d)\n    return changes"}
{"func_code_string": "def _db_to_python(db_data: dict, table: LdapObjectClass, dn: str) -> LdapObject:\n    \"\"\" Convert a DbDate object to a LdapObject. \"\"\"\n    fields = table.get_fields()\n\n    python_data = table({\n        name: field.to_python(db_data[name])\n        for name, field in fields.items()\n        if field.db_field\n    })\n    python_data = python_data.merge({\n        'dn': dn,\n    })\n    return python_data"}
{"func_code_string": "def _python_to_mod_new(changes: Changeset) -> Dict[str, List[List[bytes]]]:\n    \"\"\" Convert a LdapChanges object to a modlist for add operation. \"\"\"\n    table: LdapObjectClass = type(changes.src)\n    fields = table.get_fields()\n\n    result: Dict[str, List[List[bytes]]] = {}\n\n    for name, field in fields.items():\n        if field.db_field:\n            try:\n                value = field.to_db(changes.get_value_as_list(name))\n                if len(value) > 0:\n                    result[name] = value\n            except ValidationError as e:\n                raise ValidationError(f\"{name}: {e}.\")\n\n    return result"}
{"func_code_string": "def _python_to_mod_modify(changes: Changeset) -> Dict[str, List[Tuple[Operation, List[bytes]]]]:\n    \"\"\" Convert a LdapChanges object to a modlist for a modify operation. \"\"\"\n    table: LdapObjectClass = type(changes.src)\n    changes = changes.changes\n\n    result: Dict[str, List[Tuple[Operation, List[bytes]]]] = {}\n    for key, l in changes.items():\n        field = _get_field_by_name(table, key)\n\n        if field.db_field:\n            try:\n                new_list = [\n                    (operation, field.to_db(value))\n                    for operation, value in l\n                ]\n                result[key] = new_list\n            except ValidationError as e:\n                raise ValidationError(f\"{key}: {e}.\")\n\n    return result"}
{"func_code_string": "def search(table: LdapObjectClass, query: Optional[Q] = None,\n           database: Optional[Database] = None, base_dn: Optional[str] = None) -> Iterator[LdapObject]:\n    \"\"\" Search for a object of given type in the database. \"\"\"\n    fields = table.get_fields()\n    db_fields = {\n        name: field\n        for name, field in fields.items()\n        if field.db_field\n    }\n\n    database = get_database(database)\n    connection = database.connection\n\n    search_options = table.get_search_options(database)\n\n    iterator = tldap.query.search(\n        connection=connection,\n        query=query,\n        fields=db_fields,\n        base_dn=base_dn or search_options.base_dn,\n        object_classes=search_options.object_class,\n        pk=search_options.pk_field,\n    )\n\n    for dn, data in iterator:\n        python_data = _db_to_python(data, table, dn)\n        python_data = table.on_load(python_data, database)\n        yield python_data"}
{"func_code_string": "def get_one(table: LdapObjectClass, query: Optional[Q] = None,\n            database: Optional[Database] = None, base_dn: Optional[str] = None) -> LdapObject:\n    \"\"\" Get exactly one result from the database or fail. \"\"\"\n    results = search(table, query, database, base_dn)\n\n    try:\n        result = next(results)\n    except StopIteration:\n        raise ObjectDoesNotExist(f\"Cannot find result for {query}.\")\n\n    try:\n        next(results)\n        raise MultipleObjectsReturned(f\"Found multiple results for {query}.\")\n    except StopIteration:\n        pass\n\n    return result"}
{"func_code_string": "def preload(python_data: LdapObject, database: Optional[Database] = None) -> LdapObject:\n    \"\"\" Preload all NotLoaded fields in LdapObject. \"\"\"\n\n    changes = {}\n\n    # Load objects within lists.\n    def preload_item(value: Any) -> Any:\n        if isinstance(value, NotLoaded):\n            return value.load(database)\n        else:\n            return value\n\n    for name in python_data.keys():\n        value_list = python_data.get_as_list(name)\n\n        # Check for errors.\n        if isinstance(value_list, NotLoadedObject):\n            raise RuntimeError(f\"{name}: Unexpected NotLoadedObject outside list.\")\n\n        elif isinstance(value_list, NotLoadedList):\n            value_list = value_list.load(database)\n\n        else:\n            if any(isinstance(v, NotLoadedList) for v in value_list):\n                raise RuntimeError(f\"{name}: Unexpected NotLoadedList in list.\")\n            elif any(isinstance(v, NotLoadedObject) for v in value_list):\n                value_list = [preload_item(value) for value in value_list]\n            else:\n                value_list = None\n\n        if value_list is not None:\n            changes[name] = value_list\n\n    return python_data.merge(changes)"}
{"func_code_string": "def insert(python_data: LdapObject, database: Optional[Database] = None) -> LdapObject:\n    \"\"\" Insert a new python_data object in the database. \"\"\"\n    assert isinstance(python_data, LdapObject)\n\n    table: LdapObjectClass = type(python_data)\n\n    # ADD NEW ENTRY\n    empty_data = table()\n    changes = changeset(empty_data, python_data.to_dict())\n\n    return save(changes, database)"}
{"func_code_string": "def save(changes: Changeset, database: Optional[Database] = None) -> LdapObject:\n    \"\"\" Save all changes in a LdapChanges. \"\"\"\n    assert isinstance(changes, Changeset)\n\n    if not changes.is_valid:\n        raise RuntimeError(f\"Changeset has errors {changes.errors}.\")\n\n    database = get_database(database)\n    connection = database.connection\n\n    table = type(changes._src)\n\n    # Run hooks on changes\n    changes = table.on_save(changes, database)\n\n    # src dn   | changes dn | result         | action\n    # ---------------------------------------|--------\n    # None     | None       | error          | error\n    # None     | provided   | use changes dn | create\n    # provided | None       | use src dn     | modify\n    # provided | provided   | error          | error\n\n    src_dn = changes.src.get_as_single('dn')\n    if src_dn is None and 'dn' not in changes:\n        raise RuntimeError(\"No DN was given\")\n    elif src_dn is None and 'dn' in changes:\n        dn = changes.get_value_as_single('dn')\n        assert dn is not None\n        create = True\n    elif src_dn is not None and 'dn' not in changes:\n        dn = src_dn\n        assert dn is not None\n        create = False\n    else:\n        raise RuntimeError(\"Changes to DN are not supported.\")\n\n    assert dn is not None\n\n    if create:\n        # Add new entry\n        mod_list = _python_to_mod_new(changes)\n        try:\n            connection.add(dn, mod_list)\n        except ldap3.core.exceptions.LDAPEntryAlreadyExistsResult:\n            raise ObjectAlreadyExists(\n                \"Object with dn %r already exists doing add\" % dn)\n    else:\n        mod_list = _python_to_mod_modify(changes)\n        if len(mod_list) > 0:\n            try:\n                connection.modify(dn, mod_list)\n            except ldap3.core.exceptions.LDAPNoSuchObjectResult:\n                raise ObjectDoesNotExist(\n                    \"Object with dn %r doesn't already exist doing modify\" % dn)\n\n    # get new values\n    python_data = table(changes.src.to_dict())\n    python_data = python_data.merge(changes.to_dict())\n    python_data = python_data.on_load(python_data, database)\n    return python_data"}
{"func_code_string": "def delete(python_data: LdapObject, database: Optional[Database] = None) -> None:\n    \"\"\" Delete a LdapObject from the database. \"\"\"\n    dn = python_data.get_as_single('dn')\n    assert dn is not None\n\n    database = get_database(database)\n    connection = database.connection\n\n    connection.delete(dn)"}
{"func_code_string": "def _get_field_by_name(table: LdapObjectClass, name: str) -> tldap.fields.Field:\n    \"\"\" Lookup a field by its name. \"\"\"\n    fields = table.get_fields()\n    return fields[name]"}
{"func_code_string": "def rename(python_data: LdapObject, new_base_dn: str = None,\n           database: Optional[Database] = None, **kwargs) -> LdapObject:\n    \"\"\" Move/rename a LdapObject in the database. \"\"\"\n    table = type(python_data)\n    dn = python_data.get_as_single('dn')\n    assert dn is not None\n\n    database = get_database(database)\n    connection = database.connection\n\n    # extract key and value from kwargs\n    if len(kwargs) == 1:\n        name, value = list(kwargs.items())[0]\n\n        # work out the new rdn of the object\n        split_new_rdn = [[(name, value, 1)]]\n\n        field = _get_field_by_name(table, name)\n        assert field.db_field\n\n        python_data = python_data.merge({\n            name: value,\n        })\n\n    elif len(kwargs) == 0:\n        split_new_rdn = [str2dn(dn)[0]]\n    else:\n        assert False\n\n    new_rdn = dn2str(split_new_rdn)\n\n    connection.rename(\n        dn,\n        new_rdn,\n        new_base_dn,\n    )\n\n    if new_base_dn is not None:\n        split_base_dn = str2dn(new_base_dn)\n    else:\n        split_base_dn = str2dn(dn)[1:]\n\n    tmp_list = [split_new_rdn[0]]\n    tmp_list.extend(split_base_dn)\n\n    new_dn = dn2str(tmp_list)\n\n    python_data = python_data.merge({\n        'dn': new_dn,\n    })\n    return python_data"}
{"func_code_string": "def route(route_str):  # decorator param\n    \"\"\"\n    Provides play2 likes routes, with python formatter\n    All string fileds should be named parameters\n    :param route_str: a route \"GET /parent/{parentID}/child/{childId}{ctype}\"\n    :return: the response of requests.request\n    \"\"\"\n\n    def ilog(elapsed):\n        # statistic\n        last_stat = _routes_stat.get(route_str, {\"count\": 0, \"min\": sys.maxint, \"max\": 0, \"avg\": 0})\n        last_count = last_stat[\"count\"]\n        _routes_stat[route_str] = {\n            \"count\": last_count + 1,\n            \"min\": min(elapsed, last_stat[\"min\"]),\n            \"max\": max(elapsed, last_stat[\"max\"]),\n            \"avg\": (last_count * last_stat[\"avg\"] + elapsed) / (last_count + 1)\n        }\n        # log.debug('Route Time: {0} took {1} ms'.format(route_str, elapsed))\n\n    def wrapper(f):  # decorated function\n        @wraps(f)\n        def wrapped_func(*args, **kwargs):  # params of function\n            self = args[0]\n            method, url = route_str.split(\" \")\n\n            def defaults_dict():\n                f_args, varargs, keywords, defaults = inspect.getargspec(f)\n                defaults = defaults or []\n                return dict(zip(f_args[-len(defaults):], defaults))\n\n            defs = defaults_dict()\n\n            route_args = dict(defs.items() + kwargs.items())\n\n            def get_destination_url():\n                try:\n                    return url.format(**route_args)\n                except KeyError as e:\n                    # KeyError in format have a message with key\n                    raise AttributeError(\"Define {0} as named argument for route.\".format(e))\n\n            destination_url = self.base_url + get_destination_url()\n            f(*args, **kwargs)  # generally this is \"pass\"\n\n            bypass_args = dict([\n                                   (param, route_args[param]) for param in\n                                   [\"data\", \"json\", \"cookies\", \"auth\", \"files\", \"content_type\", \"params\"] if\n                                   param in route_args\n                                   ])\n\n            # add json content type for:\n            # - unless files are sent\n            # - private that ends with .json\n            # - all public api with POST/PUT method, meaning have basic auth\n            # - json parameter is present\n            if \"files\" not in bypass_args and (destination_url.endswith('.json') or \"json\" in route_args or (\n                            \"auth\" in bypass_args and method in [\"POST\", \"PUT\"])):\n                bypass_args['headers'] = {'Content-Type': 'application/json'}\n\n            if \"content_type\" in bypass_args and bypass_args['content_type'] == \"yaml\":\n                del bypass_args[\"content_type\"]\n                bypass_args['headers'] = {'Content-Type': 'application/x-yaml'}\n\n            start = time.time()\n            try:\n                response = self._session.request(method, destination_url, verify=self.verify_ssl, **bypass_args)\n            except requests.ConnectionError:\n                log.info('ConnectionError caught. Trying again: \\n %s:%s ' % (method, destination_url))\n                import traceback\n                def log_exception(exc_class, exc, tb):\n                    log.info('Got exception: %s' % exc)\n                    log.info('Class: %s' % exc_class)\n                    log.info('Trace: %s' % traceback.format_tb(tb))\n                    log.error('Got exception while executing: %s' % exc)\n\n                log_exception(*sys.exc_info())\n                time.sleep(2)\n                response = self._session.request(method, destination_url, verify=self.verify_ssl, **bypass_args)\n\n            end = time.time()\n            elapsed = int((end - start) * 1000.0)\n            ilog(elapsed)\n\n            if self.verify_codes:\n                if response.status_code is not 200:\n                    msg = \"Route {0} {1} returned code={2} and error: {3}\".format(method,\n                                                                                  get_destination_url(),\n                                                                                  response.status_code,\n                                                                                  response.text)\n                    if response.status_code in api_http_code_errors.keys():\n                        raise api_http_code_errors[response.status_code](msg)\n                    else:\n                        log.debug(response.text)\n                        log.debug(response.request.body)\n                        raise ApiError(msg)\n            return response\n\n        return wrapped_func\n\n    return wrapper"}
{"func_code_string": "def play_auth(f):\n    \"\"\"\n    Injects cookies, into requests call over route\n    :return: route\n    \"\"\"\n\n    def wrapper(*args, **kwargs):\n        self = args[0]\n        if 'cookies' in kwargs:\n            raise AttributeError(\"don't set cookies explicitly\")\n        if 'auth' in kwargs:\n            raise AttributeError(\"don't set auth token explicitly\")\n\n        assert self.is_connected, \"not connected, call router.connect(email, password) first\"\n\n        if self._jwt_auth:\n            kwargs['auth'] = self._jwt_auth\n            kwargs['cookies'] = None\n        elif self._cookies:\n            kwargs['cookies'] = self._cookies\n            kwargs['auth'] = None\n        else:\n            assert False, \"no cookies, no JWT, but connected o_O\"\n\n        return f(*args, **kwargs)\n\n    return wrapper"}
{"func_code_string": "def basic_auth(f):\n    \"\"\"\n    Injects auth, into requests call over route\n    :return: route\n    \"\"\"\n\n    def wrapper(*args, **kwargs):\n        self = args[0]\n        if 'auth' in kwargs:\n            raise AttributeError(\"don't set auth token explicitly\")\n        assert self.is_connected, \"not connected, call router.connect(email, password) first\"\n\n        if self._jwt_auth:\n            kwargs['auth'] = self._jwt_auth\n        elif self._auth:\n            kwargs['auth'] = self._auth\n        else:\n            assert False, \"no basic token, no JWT, but connected o_O\"\n\n        return f(*args, **kwargs)\n\n    return wrapper"}
{"func_code_string": "def _list_dict(l: Iterator[str], case_insensitive: bool = False):\n    \"\"\"\n    return a dictionary with all items of l being the keys of the dictionary\n\n    If argument case_insensitive is non-zero ldap.cidict.cidict will be\n    used for case-insensitive string keys\n    \"\"\"\n    if case_insensitive:\n        raise NotImplementedError()\n        d = tldap.dict.CaseInsensitiveDict()\n    else:\n        d = {}\n    for i in l:\n        d[i] = None\n    return d"}
{"func_code_string": "def addModlist(entry: dict, ignore_attr_types: Optional[List[str]] = None) -> Dict[str, List[bytes]]:\n    \"\"\"Build modify list for call of method LDAPObject.add()\"\"\"\n    ignore_attr_types = _list_dict(map(str.lower, (ignore_attr_types or [])))\n    modlist: Dict[str, List[bytes]] = {}\n    for attrtype in entry.keys():\n        if attrtype.lower() in ignore_attr_types:\n            # This attribute type is ignored\n            continue\n        for value in entry[attrtype]:\n            assert value is not None\n        if len(entry[attrtype]) > 0:\n            modlist[attrtype] = escape_list(entry[attrtype])\n    return modlist"}
{"func_code_string": "def modifyModlist(\n        old_entry: dict, new_entry: dict, ignore_attr_types: Optional[List[str]] = None,\n        ignore_oldexistent: bool = False) -> Dict[str, Tuple[str, List[bytes]]]:\n    \"\"\"\n    Build differential modify list for calling LDAPObject.modify()/modify_s()\n\n    :param old_entry:\n        Dictionary holding the old entry\n    :param new_entry:\n        Dictionary holding what the new entry should be\n    :param ignore_attr_types:\n        List of attribute type names to be ignored completely\n    :param ignore_oldexistent:\n        If true attribute type names which are in old_entry\n        but are not found in new_entry at all are not deleted.\n        This is handy for situations where your application\n        sets attribute value to '' for deleting an attribute.\n        In most cases leave zero.\n\n    :return: List of tuples suitable for\n        :py:meth:`ldap:ldap.LDAPObject.modify`.\n\n    This function is the same as :py:func:`ldap:ldap.modlist.modifyModlist`\n    except for the following changes:\n\n    * MOD_DELETE/MOD_DELETE used in preference to MOD_REPLACE when updating\n      an existing value.\n    \"\"\"\n    ignore_attr_types = _list_dict(map(str.lower, (ignore_attr_types or [])))\n    modlist: Dict[str, Tuple[str, List[bytes]]] = {}\n    attrtype_lower_map = {}\n    for a in old_entry.keys():\n        attrtype_lower_map[a.lower()] = a\n    for attrtype in new_entry.keys():\n        attrtype_lower = attrtype.lower()\n        if attrtype_lower in ignore_attr_types:\n            # This attribute type is ignored\n            continue\n        # Filter away null-strings\n        new_value = list(filter(lambda x: x is not None, new_entry[attrtype]))\n        if attrtype_lower in attrtype_lower_map:\n            old_value = old_entry.get(attrtype_lower_map[attrtype_lower], [])\n            old_value = list(filter(lambda x: x is not None, old_value))\n            del attrtype_lower_map[attrtype_lower]\n        else:\n            old_value = []\n        if not old_value and new_value:\n            # Add a new attribute to entry\n            modlist[attrtype] = (ldap3.MODIFY_ADD, escape_list(new_value))\n        elif old_value and new_value:\n            # Replace existing attribute\n            old_value_dict = _list_dict(old_value)\n            new_value_dict = _list_dict(new_value)\n\n            delete_values = []\n            for v in old_value:\n                if v not in new_value_dict:\n                    delete_values.append(v)\n\n            add_values = []\n            for v in new_value:\n                if v not in old_value_dict:\n                    add_values.append(v)\n\n            if len(delete_values) > 0 or len(add_values) > 0:\n                modlist[attrtype] = (\n                    ldap3.MODIFY_REPLACE, escape_list(new_value))\n\n        elif old_value and not new_value:\n            # Completely delete an existing attribute\n            modlist[attrtype] = (ldap3.MODIFY_DELETE, [])\n    if not ignore_oldexistent:\n        # Remove all attributes of old_entry which are not present\n        # in new_entry at all\n        for a in attrtype_lower_map.keys():\n            if a in ignore_attr_types:\n                # This attribute type is ignored\n                continue\n            attrtype = attrtype_lower_map[a]\n            modlist[attrtype] = (ldap3.MODIFY_DELETE, [])\n    return modlist"}
{"func_code_string": "def connect(tenant=None, user=None, password=None, token=None, is_public=False):\n        \"\"\"\n        Authenticates user and returns new platform to user.\n        This is an entry point to start working with Qubell Api.\n        :rtype: QubellPlatform\n        :param str tenant: url to tenant, default taken from 'QUBELL_TENANT'\n        :param str user: user email, default taken from 'QUBELL_USER'\n        :param str password: user password, default taken from 'QUBELL_PASSWORD'\n        :param str token: session token, default taken from 'QUBELL_TOKEN'\n        :param bool is_public: either to use public or private api (public is not fully supported use with caution)\n        :return: New Platform instance\n        \"\"\"\n        if not is_public:\n            router = PrivatePath(tenant)\n        else:\n            router = PublicPath(tenant)\n            router.public_api_in_use = is_public\n\n        if token or (user and password):\n            router.connect(user, password, token)\n\n        return QubellPlatform().init_router(router)"}
{"func_code_string": "def connect_to_another_user(self, user, password, token=None, is_public=False):\n        \"\"\"\n        Authenticates user with the same tenant as current platform using and returns new platform to user.\n        :rtype: QubellPlatform\n        :param str user: user email\n        :param str password: user password\n        :param str token: session token\n        :param bool is_public: either to use public or private api (public is not fully supported use with caution)\n        :return: New Platform instance\n        \"\"\"\n        return QubellPlatform.connect(self._router.base_url, user, password, token, is_public)"}
{"func_code_string": "def create_organization(self, name):\n        \"\"\"\n        Creates new organization\n        :rtype: Organization\n        \"\"\"\n        org = Organization.new(name, self._router)\n        assert org.ready(), \"Organization {} hasn't got ready after creation\".format(name)\n        return org"}
{"func_code_string": "def get_organization(self, id=None, name=None):\n        \"\"\"\n        Gets existing and accessible organization\n        :rtype: Organization\n        \"\"\"\n        log.info(\"Picking organization: %s (%s)\" % (name, id))\n        return self.organizations[id or name]"}
{"func_code_string": "def get_or_create_organization(self, id=None, name=None):\n        \"\"\"\n        Gets existing or creates new organization\n        :rtype: Organization\n        \"\"\"\n        if id:\n            return self.get_organization(id)\n        else:\n            assert name\n            try:\n                return self.get_organization(name=name)\n            except exceptions.NotFoundError:\n                return self.create_organization(name)"}
{"func_code_string": "def get_backends_versions(self):\n        \"\"\"\n        Get backends versions\n        :return: dict containing name of backend and version.\n        \"\"\"\n        # We are not always have permission, so find open.\n        for i in range(0, len(self.organizations)):\n            try:\n                backends = self.organizations[i].environments['default'].backends\n            except ApiAuthenticationError:\n                pass\n            else:\n                break\n        versions = dict([(x['name'], x['version']) for x in backends])\n        return versions"}
{"func_code_string": "def make_driver(loop=None):\n    ''' Returns a stop driver. \n    \n    The optional loop argument can be provided to use the driver in another \n    loop than the default one.\n\n    Parameters\n    -----------\n    loop: BaseEventLoop\n        The event loop to use instead of the default one.\n    '''\n    loop = loop or asyncio.get_event_loop()\n\n    def stop(i = None):\n        loop.stop()\n\n    def driver(sink):\n        ''' The stop driver stops the asyncio event loop.\n        The event loop is stopped as soon as an event is received on the \n        control observable or when it completes (both in case of success or \n        error).\n\n        Parameters\n        ----------\n            sink: Sink\n        '''\n        sink.control.subscribe(\n            on_next=stop,\n            on_error=stop,\n            on_completed=stop)\n        return None\n\n    return Component(call=driver, input=Sink)"}
{"func_code_string": "def rdn_to_dn(changes: Changeset, name: str, base_dn: str) -> Changeset:\n    \"\"\" Convert the rdn to a fully qualified DN for the specified LDAP\n    connection.\n\n    :param changes: The changes object to lookup.\n    :param name: rdn to convert.\n    :param base_dn: The base_dn to lookup.\n    :return: fully qualified DN.\n    \"\"\"\n    dn = changes.get_value_as_single('dn')\n    if dn is not None:\n        return changes\n\n    value = changes.get_value_as_single(name)\n    if value is None:\n        raise tldap.exceptions.ValidationError(\n            \"Cannot use %s in dn as it is None\" % name)\n    if isinstance(value, list):\n        raise tldap.exceptions.ValidationError(\n            \"Cannot use %s in dn as it is a list\" % name)\n\n    assert base_dn is not None\n\n    split_base = str2dn(base_dn)\n    split_new_dn = [[(name, value, 1)]] + split_base\n\n    new_dn = dn2str(split_new_dn)\n\n    return changes.set('dn', new_dn)"}
{"func_code_string": "def _stdin_(p):\n    \"\"\"Takes input from user. Works for Python 2 and 3.\"\"\"\n    _v = sys.version[0]\n    return input(p) if _v is '3' else raw_input(p)"}
{"func_code_string": "def survey_loader(sur_dir=SUR_DIR, sur_file=SUR_FILE):\n    \"\"\"Loads up the given survey in the given dir.\"\"\"\n    survey_path = os.path.join(sur_dir, sur_file)\n    survey = None\n    with open(survey_path) as survey_file:\n        survey = Survey(survey_file.read())\n    return survey"}
{"func_code_string": "def format_choices(self):\n        \"\"\"Return the choices in string form.\"\"\"\n        ce = enumerate(self.choices)\n        f = lambda i, c: '%s (%d)' % (c, i+1)\n        # apply formatter and append help token\n        toks = [f(i,c) for i, c in ce] + ['Help (?)']\n        return ' '.join(toks)"}
{"func_code_string": "def is_answer_valid(self, ans):\n        \"\"\"Validate user's answer against available choices.\"\"\"\n        return ans in [str(i+1) for i in range(len(self.choices))]"}
{"func_code_string": "def run_question(self, question, input_func=_stdin_):\n        \"\"\"Run the given question.\"\"\"\n        qi = '[%d/%d] ' % (self.qcount, self.qtotal)\n        print('%s %s:' % (qi, question['label']))\n        while True:\n            # ask for user input until we get a valid one\n            ans = input_func('%s > ' % self.format_choices())\n            if self.is_answer_valid(ans): \n                question['answer'] = int(ans)\n                break\n            else:\n                if ans is '?': print(question['description'])\n                else: print('Invalid answer.')\n        self.qcount += 1"}
{"func_code_string": "def run_section(self, name, input_func=_stdin_):\n        \"\"\"Run the given section.\"\"\"\n        print('\\nStuff %s by the license:\\n' % name)\n        section = self.survey[name]\n        for question in section:\n            self.run_question(question, input_func)"}
{"func_code_string": "def run(self, input_func=_stdin_):\n        \"\"\"Run the sections.\"\"\"\n        # reset question count\n        self.qcount = 1\n        for section_name in self.survey:\n            self.run_section(section_name, input_func)"}
{"func_code_string": "def get_vector(self):\n        \"\"\"Return the vector for this survey.\"\"\"\n        vec = {}\n        for dim in ['forbidden', 'required', 'permitted']:\n            if self.survey[dim] is None:\n                continue\n            dim_vec = map(lambda x: (x['tag'], x['answer']), \n                          self.survey[dim])\n            vec[dim] = dict(dim_vec)\n        return vec"}
{"func_code_string": "def update(self, span: typing.Tuple[int, int], line_type: LineType) -> None:\n        \"\"\"\n        Updates line types for a block's span.\n\n        Args:\n            span: First and last relative line number of a Block.\n            line_type: The type of line to update to.\n\n        Raises:\n            ValidationError: A special error on collision. This prevents Flake8\n                from crashing because it is converted to a Flake8 error tuple,\n                but it indicates to the user that something went wrong with\n                processing the function.\n        \"\"\"\n        first_block_line, last_block_line = span\n        for i in range(first_block_line, last_block_line + 1):\n            try:\n                self.__setitem__(i, line_type)\n            except ValueError as error:\n                raise ValidationError(i + self.fn_offset, 1, 'AAA99 {}'.format(error))"}
{"func_code_string": "def check_arrange_act_spacing(self) -> typing.Generator[AAAError, None, None]:\n        \"\"\"\n        * When no spaces found, point error at line above act block\n        * When too many spaces found, point error at 2nd blank line\n        \"\"\"\n        yield from self.check_block_spacing(\n            LineType.arrange,\n            LineType.act,\n            'AAA03 expected 1 blank line before Act block, found {}',\n        )"}
{"func_code_string": "def check_act_assert_spacing(self) -> typing.Generator[AAAError, None, None]:\n        \"\"\"\n        * When no spaces found, point error at line above assert block\n        * When too many spaces found, point error at 2nd blank line\n        \"\"\"\n        yield from self.check_block_spacing(\n            LineType.act,\n            LineType._assert,\n            'AAA04 expected 1 blank line before Assert block, found {}',\n        )"}
{"func_code_string": "def check_block_spacing(\n        self,\n        first_block_type: LineType,\n        second_block_type: LineType,\n        error_message: str,\n    ) -> typing.Generator[AAAError, None, None]:\n        \"\"\"\n        Checks there is a clear single line between ``first_block_type`` and\n        ``second_block_type``.\n\n        Note:\n            Is tested via ``check_arrange_act_spacing()`` and\n            ``check_act_assert_spacing()``.\n        \"\"\"\n        numbered_lines = list(enumerate(self))\n        first_block_lines = filter(lambda l: l[1] is first_block_type, numbered_lines)\n        try:\n            first_block_lineno = list(first_block_lines)[-1][0]\n        except IndexError:\n            # First block has no lines\n            return\n\n        second_block_lines = filter(lambda l: l[1] is second_block_type, numbered_lines)\n        try:\n            second_block_lineno = next(second_block_lines)[0]\n        except StopIteration:\n            # Second block has no lines\n            return\n\n        blank_lines = [\n            bl for bl in numbered_lines[first_block_lineno + 1:second_block_lineno] if bl[1] is LineType.blank_line\n        ]\n\n        if not blank_lines:\n            # Point at line above second block\n            yield AAAError(\n                line_number=self.fn_offset + second_block_lineno - 1,\n                offset=0,\n                text=error_message.format('none'),\n            )\n            return\n\n        if len(blank_lines) > 1:\n            # Too many blank lines - point at the first extra one, the 2nd\n            yield AAAError(\n                line_number=self.fn_offset + blank_lines[1][0],\n                offset=0,\n                text=error_message.format(len(blank_lines)),\n            )"}
{"func_code_string": "def vector_distance(v1, v2):\n    \"\"\"Given 2 vectors of multiple dimensions, calculate the euclidean \n    distance measure between them.\"\"\"\n    dist = 0\n    for dim in v1:\n        for x in v1[dim]:\n            dd = int(v1[dim][x]) - int(v2[dim][x])\n            dist = dist + dd**2\n    return dist"}
{"func_code_string": "def send_command(self, command, *arguments):\n        \"\"\" Send command with no output.\n\n        :param command: command to send.\n        :param arguments: list of command arguments.\n        \"\"\"\n        self.api.send_command(self, command, *arguments)"}
{"func_code_string": "def send_command_return(self, command, *arguments):\n        \"\"\" Send command and wait for single line output. \"\"\"\n        return self.api.send_command_return(self, command, *arguments)"}
{"func_code_string": "def send_command_return_multilines(self, command, *arguments):\n        \"\"\" Send command and wait for multiple lines output. \"\"\"\n        return self.api.send_command_return_multilines(self, command, *arguments)"}
{"func_code_string": "def load(self, limit=9999):\n        \"\"\" Function list\n        Get the list of all interfaces\n\n        @param key: The targeted object\n        @param limit: The limit of items to return\n        @return RETURN: A ForemanItem list\n        \"\"\"\n        subItemList = self.api.list('{}/{}/{}'.format(self.parentObjName,\n                                                      self.parentKey,\n                                                      self.objName,\n                                                      ),\n                                    limit=limit)\n        if self.objName == 'puppetclass_ids':\n            subItemList = list(map(lambda x: {'id': x}, subItemList))\n        if self.objName == 'puppetclasses':\n            sil_tmp = subItemList.values()\n            subItemList = []\n            for i in sil_tmp:\n                subItemList.extend(i)\n        return {x[self.index]: self.objType(self.api, x['id'],\n                                            self.parentObjName,\n                                            self.parentPayloadObj,\n                                            self.parentKey,\n                                            x)\n                for x in subItemList}"}
{"func_code_string": "def append(self, payload):\n        \"\"\" Function __iadd__\n\n        @param payload: The payload corresponding to the object to add\n        @return RETURN: A ForemanItem\n        \"\"\"\n        if self.objType.setInParentPayload:\n            print('Error, {} is not elibible to addition, but only set'\n                  .format(self.objName))\n            return False\n        ret = self.api.create(\"{}/{}/{}\".format(self.parentObjName,\n                                                self.parentKey,\n                                                self.objNameSet),\n                              self.getPayloadStruct(payload))\n        return ret"}
{"func_code_string": "def getPayloadStruct(self, payload):\n        \"\"\" Function getPayloadStruct\n\n        @param payload: The payload structure to the object to add\n        @return RETURN: A dict\n        \"\"\"\n        newSubItem = self.objType(self.api, 0, self.parentObjName,\n                                  self.parentPayloadObj, self.parentKey, {})\n        return newSubItem.getPayloadStruct(payload, self.parentPayloadObj)"}
{"func_code_string": "def get_repr(expr, multiline=False):\n    \"\"\"\n    Build a repr string for ``expr`` from its vars and signature.\n\n    ::\n\n        >>> class MyObject:\n        ...     def __init__(self, arg1, arg2, *var_args, foo=None, bar=None, **kwargs):\n        ...         self.arg1 = arg1\n        ...         self.arg2 = arg2\n        ...         self.var_args = var_args\n        ...         self.foo = foo\n        ...         self.bar = bar\n        ...         self.kwargs = kwargs\n        ...\n        >>> my_object = MyObject('a', 'b', 'c', 'd', foo='x', quux=['y', 'z'])\n\n    ::\n\n        >>> import uqbar\n        >>> print(uqbar.objects.get_repr(my_object))\n        MyObject(\n            'a',\n            'b',\n            'c',\n            'd',\n            foo='x',\n            quux=['y', 'z'],\n            )\n\n    \"\"\"\n    signature = _get_object_signature(expr)\n    if signature is None:\n        return \"{}()\".format(type(expr).__name__)\n\n    defaults = {}\n    for name, parameter in signature.parameters.items():\n        if parameter.default is not inspect._empty:\n            defaults[name] = parameter.default\n\n    args, var_args, kwargs = get_vars(expr)\n    args_parts = collections.OrderedDict()\n    var_args_parts = []\n    kwargs_parts = {}\n    has_lines = multiline\n    parts = []\n\n    # Format keyword-optional arguments.\n    # print(type(expr), args)\n    for i, (key, value) in enumerate(args.items()):\n        arg_repr = _dispatch_formatting(value)\n        if \"\\n\" in arg_repr:\n            has_lines = True\n        args_parts[key] = arg_repr\n\n    # Format *args\n    for arg in var_args:\n        arg_repr = _dispatch_formatting(arg)\n        if \"\\n\" in arg_repr:\n            has_lines = True\n        var_args_parts.append(arg_repr)\n\n    # Format **kwargs\n    for key, value in sorted(kwargs.items()):\n        if key in defaults and value == defaults[key]:\n            continue\n        value = _dispatch_formatting(value)\n        arg_repr = \"{}={}\".format(key, value)\n        has_lines = True\n        kwargs_parts[key] = arg_repr\n\n    for _, part in args_parts.items():\n        parts.append(part)\n    parts.extend(var_args_parts)\n    for _, part in sorted(kwargs_parts.items()):\n        parts.append(part)\n\n    # If we should format on multiple lines, add the appropriate formatting.\n    if has_lines and parts:\n        for i, part in enumerate(parts):\n            parts[i] = \"\\n\".join(\"    \" + line for line in part.split(\"\\n\"))\n        parts.append(\"    )\")\n        parts = \",\\n\".join(parts)\n        return \"{}(\\n{}\".format(type(expr).__name__, parts)\n\n    parts = \", \".join(parts)\n    return \"{}({})\".format(type(expr).__name__, parts)"}
{"func_code_string": "def get_vars(expr):\n    \"\"\"\n    Get ``args``, ``var args`` and ``kwargs`` for an object ``expr``.\n\n    ::\n\n        >>> class MyObject:\n        ...     def __init__(self, arg1, arg2, *var_args, foo=None, bar=None, **kwargs):\n        ...         self.arg1 = arg1\n        ...         self.arg2 = arg2\n        ...         self.var_args = var_args\n        ...         self.foo = foo\n        ...         self.bar = bar\n        ...         self.kwargs = kwargs\n        ...\n        >>> my_object = MyObject('a', 'b', 'c', 'd', foo='x', quux=['y', 'z'])\n\n    ::\n\n        >>> import uqbar\n        >>> args, var_args, kwargs = uqbar.objects.get_vars(my_object)\n\n    ::\n\n        >>> args\n        OrderedDict([('arg1', 'a'), ('arg2', 'b')])\n\n    ::\n\n        >>> var_args\n        ['c', 'd']\n\n    ::\n\n        >>> kwargs\n        {'foo': 'x', 'quux': ['y', 'z']}\n\n    \"\"\"\n    # print('TYPE?', type(expr))\n    signature = _get_object_signature(expr)\n    if signature is None:\n        return ({}, [], {})\n    # print('SIG?', signature)\n    args = collections.OrderedDict()\n    var_args = []\n    kwargs = {}\n    if expr is None:\n        return args, var_args, kwargs\n    for i, (name, parameter) in enumerate(signature.parameters.items()):\n        # print('   ', parameter)\n\n        if i == 0 and name in (\"self\", \"cls\", \"class_\", \"klass\"):\n            continue\n\n        if parameter.kind is inspect._POSITIONAL_ONLY:\n            try:\n                args[name] = getattr(expr, name)\n            except AttributeError:\n                args[name] = expr[name]\n\n        elif (\n            parameter.kind is inspect._POSITIONAL_OR_KEYWORD\n            or parameter.kind is inspect._KEYWORD_ONLY\n        ):\n            found = False\n            for x in (name, \"_\" + name):\n                try:\n                    value = getattr(expr, x)\n                    found = True\n                    break\n                except AttributeError:\n                    try:\n                        value = expr[x]\n                        found = True\n                        break\n                    except (KeyError, TypeError):\n                        pass\n            if not found:\n                raise ValueError(\"Cannot find value for {!r}\".format(name))\n            if parameter.default is inspect._empty:\n                args[name] = value\n            elif parameter.default != value:\n                kwargs[name] = value\n\n        elif parameter.kind is inspect._VAR_POSITIONAL:\n            value = None\n            try:\n                value = expr[:]\n            except TypeError:\n                value = getattr(expr, name)\n            if value:\n                var_args.extend(value)\n\n        elif parameter.kind is inspect._VAR_KEYWORD:\n            items = {}\n            if hasattr(expr, \"items\"):\n                items = expr.items()\n            elif hasattr(expr, name):\n                mapping = getattr(expr, name)\n                if not isinstance(mapping, dict):\n                    mapping = dict(mapping)\n                items = mapping.items()\n            elif hasattr(expr, \"_\" + name):\n                mapping = getattr(expr, \"_\" + name)\n                if not isinstance(mapping, dict):\n                    mapping = dict(mapping)\n                items = mapping.items()\n            for key, value in items:\n                if key not in args:\n                    kwargs[key] = value\n\n    return args, var_args, kwargs"}
{"func_code_string": "def new(expr, *args, **kwargs):\n    \"\"\"\n    Template an object.\n\n    ::\n\n        >>> class MyObject:\n        ...     def __init__(self, arg1, arg2, *var_args, foo=None, bar=None, **kwargs):\n        ...         self.arg1 = arg1\n        ...         self.arg2 = arg2\n        ...         self.var_args = var_args\n        ...         self.foo = foo\n        ...         self.bar = bar\n        ...         self.kwargs = kwargs\n        ...\n        >>> my_object = MyObject('a', 'b', 'c', 'd', foo='x', quux=['y', 'z'])\n\n    ::\n\n        >>> import uqbar\n        >>> new_object = uqbar.objects.new(my_object, foo=666, bar=1234)\n        >>> print(uqbar.objects.get_repr(new_object))\n        MyObject(\n            'a',\n            'b',\n            'c',\n            'd',\n            bar=1234,\n            foo=666,\n            quux=['y', 'z'],\n            )\n\n    Original object is unchanged:\n\n    ::\n\n        >>> print(uqbar.objects.get_repr(my_object))\n        MyObject(\n            'a',\n            'b',\n            'c',\n            'd',\n            foo='x',\n            quux=['y', 'z'],\n            )\n\n    \"\"\"\n    # TODO: Clarify old vs. new variable naming here.\n    current_args, current_var_args, current_kwargs = get_vars(expr)\n    new_kwargs = current_kwargs.copy()\n\n    recursive_arguments = {}\n    for key in tuple(kwargs):\n        if \"__\" in key:\n            value = kwargs.pop(key)\n            key, _, subkey = key.partition(\"__\")\n            recursive_arguments.setdefault(key, []).append((subkey, value))\n\n    for key, pairs in recursive_arguments.items():\n        recursed_object = current_args.get(key, current_kwargs.get(key))\n        if recursed_object is None:\n            continue\n        kwargs[key] = new(recursed_object, **dict(pairs))\n\n    if args:\n        current_var_args = args\n    for key, value in kwargs.items():\n        if key in current_args:\n            current_args[key] = value\n        else:\n            new_kwargs[key] = value\n\n    new_args = list(current_args.values()) + list(current_var_args)\n    return type(expr)(*new_args, **new_kwargs)"}
{"func_code_string": "def on_builder_inited(app):\n    \"\"\"\n    Hooks into Sphinx's ``builder-inited`` event.\n\n    Builds out the ReST API source.\n    \"\"\"\n    config = app.builder.config\n\n    target_directory = (\n        pathlib.Path(app.builder.env.srcdir) / config.uqbar_api_directory_name\n    )\n\n    initial_source_paths: List[str] = []\n    source_paths = config.uqbar_api_source_paths\n    for source_path in source_paths:\n        if isinstance(source_path, types.ModuleType):\n            if hasattr(source_path, \"__path__\"):\n                initial_source_paths.extend(getattr(source_path, \"__path__\"))\n            else:\n                initial_source_paths.extend(source_path.__file__)\n            continue\n        try:\n            module = importlib.import_module(source_path)\n            if hasattr(module, \"__path__\"):\n                initial_source_paths.extend(getattr(module, \"__path__\"))\n            else:\n                initial_source_paths.append(module.__file__)\n        except ImportError:\n            initial_source_paths.append(source_path)\n\n    root_documenter_class = config.uqbar_api_root_documenter_class\n    if isinstance(root_documenter_class, str):\n        module_name, _, class_name = root_documenter_class.rpartition(\".\")\n        module = importlib.import_module(module_name)\n        root_documenter_class = getattr(module, class_name)\n\n    module_documenter_class = config.uqbar_api_module_documenter_class\n    if isinstance(module_documenter_class, str):\n        module_name, _, class_name = module_documenter_class.rpartition(\".\")\n        module = importlib.import_module(module_name)\n        module_documenter_class = getattr(module, class_name)\n\n    # Don't modify the list in Sphinx's config. Sphinx won't pickle class\n    # references, and strips them from the saved config. That leads to Sphinx\n    # believing that the config has changed on every run.\n    member_documenter_classes = list(config.uqbar_api_member_documenter_classes or [])\n    for i, member_documenter_class in enumerate(member_documenter_classes):\n        if isinstance(member_documenter_class, str):\n            module_name, _, class_name = member_documenter_class.rpartition(\".\")\n            module = importlib.import_module(module_name)\n            member_documenter_classes[i] = getattr(module, class_name)\n\n    api_builder = uqbar.apis.APIBuilder(\n        initial_source_paths=initial_source_paths,\n        target_directory=target_directory,\n        document_empty_modules=config.uqbar_api_document_empty_modules,\n        document_private_members=config.uqbar_api_document_private_members,\n        document_private_modules=config.uqbar_api_document_private_modules,\n        member_documenter_classes=member_documenter_classes or None,\n        module_documenter_class=module_documenter_class,\n        root_documenter_class=root_documenter_class,\n        title=config.uqbar_api_title,\n        logger_func=logger_func,\n    )\n    api_builder()"}
{"func_code_string": "def setup(app) -> Dict[str, Any]:\n    \"\"\"\n    Sets up Sphinx extension.\n    \"\"\"\n    app.add_config_value(\"uqbar_api_directory_name\", \"api\", \"env\")\n    app.add_config_value(\"uqbar_api_document_empty_modules\", False, \"env\")\n    app.add_config_value(\"uqbar_api_document_private_members\", False, \"env\")\n    app.add_config_value(\"uqbar_api_document_private_modules\", False, \"env\")\n    app.add_config_value(\"uqbar_api_member_documenter_classes\", None, \"env\")\n    app.add_config_value(\"uqbar_api_module_documenter_class\", None, \"env\")\n    app.add_config_value(\"uqbar_api_root_documenter_class\", None, \"env\")\n    app.add_config_value(\"uqbar_api_source_paths\", None, \"env\")\n    app.add_config_value(\"uqbar_api_title\", \"API\", \"html\")\n    app.connect(\"builder-inited\", on_builder_inited)\n    return {\n        \"version\": uqbar.__version__,\n        \"parallel_read_safe\": True,\n        \"parallel_write_safe\": True,\n    }"}
{"func_code_string": "def get_glitter_app(self, glitter_app_name):\n        \"\"\"\n        Retrieve the Glitter App config for a specific Glitter App.\n        \"\"\"\n        if not self.discovered:\n            self.discover_glitter_apps()\n\n        try:\n            glitter_app = self.glitter_apps[glitter_app_name]\n            return glitter_app\n        except KeyError:\n            return None"}
{"func_code_string": "def discover_glitter_apps(self):\n        \"\"\"\n        Find all the Glitter App configurations in the current project.\n        \"\"\"\n        for app_name in settings.INSTALLED_APPS:\n            module_name = '{app_name}.glitter_apps'.format(app_name=app_name)\n            try:\n                glitter_apps_module = import_module(module_name)\n                if hasattr(glitter_apps_module, 'apps'):\n                    self.glitter_apps.update(glitter_apps_module.apps)\n            except ImportError:\n                pass\n\n        self.discovered = True"}
{"func_code_string": "def enhance(self):\n        \"\"\" Function enhance\n        Enhance the object with new item or enhanced items\n        \"\"\"\n        self.update({'parameters':\n                     SubDict(self.api, self.objName,\n                             self.payloadObj, self.key,\n                             SubItemParameter)})\n        self.update({'interfaces':\n                     SubDict(self.api, self.objName,\n                             self.payloadObj, self.key,\n                             SubItemInterface)})\n        self.update({'subnets':\n                    SubDict(self.api, self.objName,\n                            self.payloadObj, self.key,\n                            SubItemSubnet)})"}
{"func_code_string": "def kong(ctx, namespace, yes):\n    \"\"\"Update the kong configuration\n    \"\"\"\n    m = KongManager(ctx.obj['agile'], namespace=namespace)\n    click.echo(utils.niceJson(m.create_kong(yes)))"}
{"func_code_string": "def schedule_task(self):\n        \"\"\"\n        Schedules this publish action as a Celery task.\n        \"\"\"\n        from .tasks import publish_task\n\n        publish_task.apply_async(kwargs={'pk': self.pk}, eta=self.scheduled_time)"}
{"func_code_string": "def get_version(self):\n        \"\"\"\n        Get the version object for the related object.\n        \"\"\"\n        return Version.objects.get(\n            content_type=self.content_type,\n            object_id=self.object_id,\n            version_number=self.publish_version,\n        )"}
{"func_code_string": "def _publish(self):\n        \"\"\"\n        Process a publish action on the related object, returns a boolean if a change is made.\n\n        Only objects where a version change is needed will be updated.\n        \"\"\"\n        obj = self.content_object\n        version = self.get_version()\n        actioned = False\n\n        # Only update if needed\n        if obj.current_version != version:\n            version = self.get_version()\n            obj.current_version = version\n            obj.save(update_fields=['current_version'])\n            actioned = True\n\n        return actioned"}
{"func_code_string": "def _unpublish(self):\n        \"\"\"\n        Process an unpublish action on the related object, returns a boolean if a change is made.\n\n        Only objects with a current active version will be updated.\n        \"\"\"\n        obj = self.content_object\n        actioned = False\n\n        # Only update if needed\n        if obj.current_version is not None:\n            obj.current_version = None\n            obj.save(update_fields=['current_version'])\n            actioned = True\n\n        return actioned"}
{"func_code_string": "def _log_action(self):\n        \"\"\"\n        Adds a log entry for this action to the object history in the Django admin.\n        \"\"\"\n        if self.publish_version == self.UNPUBLISH_CHOICE:\n            message = 'Unpublished page (scheduled)'\n        else:\n            message = 'Published version {} (scheduled)'.format(self.publish_version)\n\n        LogEntry.objects.log_action(\n            user_id=self.user.pk,\n            content_type_id=self.content_type.pk,\n            object_id=self.object_id,\n            object_repr=force_text(self.content_object),\n            action_flag=CHANGE,\n            change_message=message\n        )"}
{"func_code_string": "def process_action(self):\n        \"\"\"\n        Process the action and update the related object, returns a boolean if a change is made.\n        \"\"\"\n        if self.publish_version == self.UNPUBLISH_CHOICE:\n            actioned = self._unpublish()\n        else:\n            actioned = self._publish()\n\n        # Only log if an action was actually taken\n        if actioned:\n            self._log_action()\n\n        return actioned"}
{"func_code_string": "def checkAndCreate(self, key, payload,\n                       hostgroupConf,\n                       hostgroupParent,\n                       puppetClassesId):\n        \"\"\" Function checkAndCreate\n        check And Create procedure for an hostgroup\n        - check the hostgroup is not existing\n        - create the hostgroup\n        - Add puppet classes from puppetClassesId\n        - Add params from hostgroupConf\n\n        @param key: The hostgroup name or ID\n        @param payload: The description of the hostgroup\n        @param hostgroupConf: The configuration of the host group from the\n                              foreman.conf\n        @param hostgroupParent: The id of the parent hostgroup\n        @param puppetClassesId: The dict of puppet classes ids in foreman\n        @return RETURN: The ItemHostsGroup object of an host\n        \"\"\"\n        if key not in self:\n            self[key] = payload\n        oid = self[key]['id']\n        if not oid:\n            return False\n\n        # Create Hostgroup classes\n        if 'classes' in hostgroupConf.keys():\n            classList = list()\n            for c in hostgroupConf['classes']:\n                classList.append(puppetClassesId[c])\n            if not self[key].checkAndCreateClasses(classList):\n                print(\"Failed in classes\")\n                return False\n\n        # Set params\n        if 'params' in hostgroupConf.keys():\n            if not self[key].checkAndCreateParams(hostgroupConf['params']):\n                print(\"Failed in params\")\n                return False\n\n        return oid"}
{"func_code_string": "def doublewrap(f):\n    '''\n    a decorator decorator, allowing the decorator to be used as:\n    @decorator(with, arguments, and=kwargs)\n    or\n    @decorator\n    Ref: http://stackoverflow.com/questions/653368/how-to-create-a-python-decorator-that-can-be-used-either-with-or-without-paramet\n    '''\n    @functools.wraps(f)\n    def new_dec(*args, **kwargs):\n        if len(args) == 1 and len(kwargs) == 0 and callable(args[0]):\n            # actual decorated function\n            return f(args[0])\n        else:\n            # decorator arguments\n            return lambda realf: f(realf, *args, **kwargs)\n\n    return new_dec"}
{"func_code_string": "def deprecated(func, msg=None):\n    \"\"\"\n    A decorator which can be used to mark functions\n    as deprecated.It will result in a deprecation warning being shown\n    when the function is used.\n    \"\"\"\n\n    message = msg or \"Use of deprecated function '{}`.\".format(func.__name__)\n\n    @functools.wraps(func)\n    def wrapper_func(*args, **kwargs):\n        warnings.warn(message, DeprecationWarning, stacklevel=2)\n        return func(*args, **kwargs)\n\n    return wrapper_func"}
{"func_code_string": "def create_handler(target: str):\n    \"\"\"Create a handler for logging to ``target``\"\"\"\n    if target == 'stderr':\n        return logging.StreamHandler(sys.stderr)\n    elif target == 'stdout':\n        return logging.StreamHandler(sys.stdout)\n    else:\n        return logging.handlers.WatchedFileHandler(filename=target)"}
{"func_code_string": "def initialise_logging(level: str, target: str, short_format: bool):\n    \"\"\"Initialise basic logging facilities\"\"\"\n    try:\n        log_level = getattr(logging, level)\n    except AttributeError:\n        raise SystemExit(\n            \"invalid log level %r, expected any of 'DEBUG', 'INFO', 'WARNING', 'ERROR' or 'CRITICAL'\" % level\n        )\n    handler = create_handler(target=target)\n    logging.basicConfig(\n        level=log_level,\n        format='%(asctime)-15s (%(process)d) %(message)s' if not short_format else '%(message)s',\n        datefmt='%Y-%m-%d %H:%M:%S',\n        handlers=[handler]\n    )"}
{"func_code_string": "def escape_dn_chars(s):\n    \"\"\"\n    Escape all DN special characters found in s\n    with a back-slash (see RFC 4514, section 2.4)\n    \"\"\"\n    if s:\n        assert isinstance(s, six.string_types)\n        s = s.replace('\\\\', '\\\\\\\\')\n        s = s.replace(',', '\\\\,')\n        s = s.replace('+', '\\\\+')\n        s = s.replace('\"', '\\\\\"')\n        s = s.replace('<', '\\\\<')\n        s = s.replace('>', '\\\\>')\n        s = s.replace(';', '\\\\;')\n        s = s.replace('=', '\\\\=')\n        s = s.replace('\\000', '\\\\\\000')\n        if s[0] == '#' or s[0] == ' ':\n            s = ''.join(('\\\\', s))\n        if s[-1] == ' ':\n            s = ''.join((s[:-1], '\\\\ '))\n    return s"}
{"func_code_string": "def str2dn(dn, flags=0):\n    \"\"\"\n    This function takes a DN as string as parameter and returns\n    a decomposed DN. It's the inverse to dn2str().\n\n    flags describes the format of the dn\n\n    See also the OpenLDAP man-page ldap_str2dn(3)\n    \"\"\"\n\n    # if python2, we need unicode string\n    if not isinstance(dn, six.text_type):\n        dn = dn.decode(\"utf_8\")\n\n    assert flags == 0\n    result, i = _distinguishedName(dn, 0)\n    if result is None:\n        raise tldap.exceptions.InvalidDN(\"Cannot parse dn\")\n    if i != len(dn):\n        raise tldap.exceptions.InvalidDN(\"Cannot parse dn past %s\" % dn[i:])\n    return result"}
{"func_code_string": "def dn2str(dn):\n    \"\"\"\n    This function takes a decomposed DN as parameter and returns\n    a single string. It's the inverse to str2dn() but will always\n    return a DN in LDAPv3 format compliant to RFC 4514.\n    \"\"\"\n    for rdn in dn:\n        for atype, avalue, dummy in rdn:\n            assert isinstance(atype, six.string_types)\n            assert isinstance(avalue, six.string_types)\n            assert dummy == 1\n\n    return ','.join([\n        '+'.join([\n            '='.join((atype, escape_dn_chars(avalue or '')))\n            for atype, avalue, dummy in rdn])\n        for rdn in dn\n    ])"}
{"func_code_string": "def explode_dn(dn, notypes=0, flags=0):\n    \"\"\"\n    explode_dn(dn [, notypes=0]) -> list\n\n    This function takes a DN and breaks it up into its component parts.\n    The notypes parameter is used to specify that only the component's\n    attribute values be returned and not the attribute types.\n    \"\"\"\n    if not dn:\n        return []\n    dn_decomp = str2dn(dn, flags)\n    rdn_list = []\n    for rdn in dn_decomp:\n        if notypes:\n            rdn_list.append('+'.join([\n                escape_dn_chars(avalue or '')\n                for atype, avalue, dummy in rdn\n            ]))\n        else:\n            rdn_list.append('+'.join([\n                '='.join((atype, escape_dn_chars(avalue or '')))\n                for atype, avalue, dummy in rdn\n            ]))\n    return rdn_list"}
{"func_code_string": "def explode_rdn(rdn, notypes=0, flags=0):\n    \"\"\"\n    explode_rdn(rdn [, notypes=0]) -> list\n\n    This function takes a RDN and breaks it up into its component parts\n    if it is a multi-valued RDN.\n    The notypes parameter is used to specify that only the component's\n    attribute values be returned and not the attribute types.\n    \"\"\"\n    if not rdn:\n        return []\n    rdn_decomp = str2dn(rdn, flags)[0]\n    if notypes:\n        return [avalue or '' for atype, avalue, dummy in rdn_decomp]\n    else:\n        return ['='.join((atype, escape_dn_chars(avalue or '')))\n                for atype, avalue, dummy in rdn_decomp]"}
{"func_code_string": "def labels(ctx):\n    \"\"\"Crate or update labels in github\n    \"\"\"\n    config = ctx.obj['agile']\n    repos = config.get('repositories')\n    labels = config.get('labels')\n    if not isinstance(repos, list):\n        raise CommandError(\n            'You need to specify the \"repos\" list in the config'\n        )\n    if not isinstance(labels, dict):\n        raise CommandError(\n            'You need to specify the \"labels\" dictionary in the config'\n        )\n    git = GithubApi()\n    for repo in repos:\n        repo = git.repo(repo)\n        for label, color in labels.items():\n            if repo.label(label, color):\n                click.echo('Created label \"%s\" @ %s' % (label, repo))\n            else:\n                click.echo('Updated label \"%s\" @ %s' % (label, repo))"}
{"func_code_string": "def get_access_token(self, code):\n        \"\"\"Get new access token.\"\"\"\n        try:\n            self._token = super().fetch_token(\n                MINUT_TOKEN_URL,\n                client_id=self._client_id,\n                client_secret=self._client_secret,\n                code=code,\n            )\n        # except Exception as e:\n        except MissingTokenError as error:\n            _LOGGER.debug(\"Token issues: %s\", error)\n        return self._token"}
{"func_code_string": "def _request(self, url, request_type='GET', **params):\n        \"\"\"Send a request to the Minut Point API.\"\"\"\n        try:\n            _LOGGER.debug('Request %s %s', url, params)\n            response = self.request(\n                request_type, url, timeout=TIMEOUT.seconds, **params)\n            response.raise_for_status()\n            _LOGGER.debug('Response %s %s %.200s', response.status_code,\n                          response.headers['content-type'], response.json())\n            response = response.json()\n            if 'error' in response:\n                raise OSError(response['error'])\n            return response\n        except OSError as error:\n            _LOGGER.warning('Failed request: %s', error)"}
{"func_code_string": "def _request_devices(self, url, _type):\n        \"\"\"Request list of devices.\"\"\"\n        res = self._request(url)\n        return res.get(_type) if res else {}"}
{"func_code_string": "def read_sensor(self, device_id, sensor_uri):\n        \"\"\"Return sensor value based on sensor_uri.\"\"\"\n        url = MINUT_DEVICES_URL + \"/{device_id}/{sensor_uri}\".format(\n            device_id=device_id, sensor_uri=sensor_uri)\n        res = self._request(url, request_type='GET', data={'limit': 1})\n        if not res.get('values'):\n            return None\n        return res.get('values')[-1].get('value')"}
{"func_code_string": "def _register_webhook(self, webhook_url, events):\n        \"\"\"Register webhook.\"\"\"\n        response = self._request(\n            MINUT_WEBHOOKS_URL,\n            request_type='POST',\n            json={\n                'url': webhook_url,\n                'events': events,\n            },\n        )\n        return response"}
{"func_code_string": "def remove_webhook(self):\n        \"\"\"Remove webhook.\"\"\"\n        if self._webhook.get('hook_id'):\n            self._request(\n                \"{}/{}\".format(MINUT_WEBHOOKS_URL, self._webhook['hook_id']),\n                request_type='DELETE',\n            )"}
{"func_code_string": "def update_webhook(self, webhook_url, webhook_id, events=None):\n        \"\"\"Register webhook (if it doesn't exit).\"\"\"\n        hooks = self._request(MINUT_WEBHOOKS_URL, request_type='GET')['hooks']\n        try:\n            self._webhook = next(\n                hook for hook in hooks if hook['url'] == webhook_url)\n            _LOGGER.debug(\"Webhook: %s\", self._webhook)\n        except StopIteration:  # Not found\n            if events is None:\n                events = [e for v in EVENTS.values() for e in v if e]\n            self._webhook = self._register_webhook(webhook_url, events)\n            _LOGGER.debug(\"Registered hook: %s\", self._webhook)\n            return self._webhook"}
{"func_code_string": "def update(self):\n        \"\"\"Update all devices from server.\"\"\"\n        with self._lock:\n            devices = self._request_devices(MINUT_DEVICES_URL, 'devices')\n\n            if devices:\n                self._state = {\n                    device['device_id']: device\n                    for device in devices\n                }\n                _LOGGER.debug(\"Found devices: %s\", list(self._state.keys()))\n                # _LOGGER.debug(\"Device status: %s\", devices)\n            homes = self._request_devices(MINUT_HOMES_URL, 'homes')\n            if homes:\n                self._homes = homes\n            return self.devices"}
{"func_code_string": "def _set_alarm(self, status, home_id):\n        \"\"\"Set alarm satus.\"\"\"\n        response = self._request(\n            MINUT_HOMES_URL + \"/{}\".format(home_id),\n            request_type='PUT',\n            json={'alarm_status': status})\n        return response.get('alarm_status', '') == status"}
{"func_code_string": "def sensor(self, sensor_type):\n        \"\"\"Update and return sensor value.\"\"\"\n        _LOGGER.debug(\"Reading %s sensor.\", sensor_type)\n        return self._session.read_sensor(self.device_id, sensor_type)"}
{"func_code_string": "def device_info(self):\n        \"\"\"Info about device.\"\"\"\n        return {\n            'connections': {('mac', self.device['device_mac'])},\n            'identifieres': self.device['device_id'],\n            'manufacturer': 'Minut',\n            'model': 'Point v{}'.format(self.device['hardware_version']),\n            'name': self.device['description'],\n            'sw_version': self.device['firmware']['installed'],\n        }"}
{"func_code_string": "def device_status(self):\n        \"\"\"Status of device.\"\"\"\n        return {\n            'active': self.device['active'],\n            'offline': self.device['offline'],\n            'last_update': self.last_update,\n            'battery_level': self.battery_level,\n        }"}
{"func_code_string": "def glitter_head(context):\n    \"\"\"\n    Template tag which renders the glitter CSS and JavaScript. Any resources\n    which need to be loaded should be added here. This is only shown to users\n    with permission to edit the page.\n    \"\"\"\n    user = context.get('user')\n    rendered = ''\n    template_path = 'glitter/include/head.html'\n\n    if user is not None and user.is_staff:\n        template = context.template.engine.get_template(template_path)\n        rendered = template.render(context)\n\n    return rendered"}
{"func_code_string": "def glitter_startbody(context):\n    \"\"\"\n    Template tag which renders the glitter overlay and sidebar. This is only\n    shown to users with permission to edit the page.\n    \"\"\"\n    user = context.get('user')\n    path_body = 'glitter/include/startbody.html'\n    path_plus = 'glitter/include/startbody_%s_%s.html'\n    rendered = ''\n\n    if user is not None and user.is_staff:\n        templates = [path_body]\n        # We've got a page with a glitter object:\n        # - May need a different startbody template\n        # - Check if user has permission to add\n        glitter = context.get('glitter')\n        if glitter is not None:\n            opts = glitter.obj._meta.app_label, glitter.obj._meta.model_name\n            template_path = path_plus % opts\n            templates.insert(0, template_path)\n\n        template = context.template.engine.select_template(templates)\n        rendered = template.render(context)\n\n    return rendered"}
{"func_code_string": "def escape_filter_chars(assertion_value, escape_mode=0):\n    \"\"\"\n    Replace all special characters found in assertion_value\n    by quoted notation.\n\n    escape_mode\n        If 0 only special chars mentioned in RFC 4515 are escaped.\n        If 1 all NON-ASCII chars are escaped.\n        If 2 all chars are escaped.\n    \"\"\"\n\n    if isinstance(assertion_value, six.text_type):\n        assertion_value = assertion_value.encode(\"utf_8\")\n\n    s = []\n    for c in assertion_value:\n        do_escape = False\n\n        if str != bytes:  # Python 3\n            pass\n        else:  # Python 2\n            c = ord(c)\n\n        if escape_mode == 0:\n            if c == ord('\\\\') or c == ord('*') \\\n                    or c == ord('(') or c == ord(')') \\\n                    or c == ord('\\x00'):\n                do_escape = True\n        elif escape_mode == 1:\n            if c < '0' or c > 'z' or c in \"\\\\*()\":\n                do_escape = True\n        elif escape_mode == 2:\n            do_escape = True\n        else:\n            raise ValueError('escape_mode must be 0, 1 or 2.')\n\n        if do_escape:\n            s.append(b\"\\\\%02x\" % c)\n        else:\n            b = None\n            if str != bytes:  # Python 3\n                b = bytes([c])\n            else:  # Python 2\n                b = chr(c)\n            s.append(b)\n\n    return b''.join(s)"}
{"func_code_string": "def filter_format(filter_template, assertion_values):\n    \"\"\"\n    filter_template\n          String containing %s as placeholder for assertion values.\n    assertion_values\n          List or tuple of assertion values. Length must match\n          count of %s in filter_template.\n    \"\"\"\n    assert isinstance(filter_template, bytes)\n    return filter_template % (\n        tuple(map(escape_filter_chars, assertion_values)))"}
{"func_code_string": "def to_flake8(self, checker_cls: type) -> Flake8Error:\n        \"\"\"\n        Args:\n            checker_cls: Class performing the check to be passed back to\n                flake8.\n        \"\"\"\n        return Flake8Error(\n            line_number=self.line_number,\n            offset=self.offset,\n            text=self.text,\n            checker_cls=checker_cls,\n        )"}
{"func_code_string": "def add_chassis(self, chassis):\n        \"\"\"\n        :param chassis: chassis object\n        \"\"\"\n\n        res = self._request(RestMethod.post, self.user_url, params={'ip': chassis.ip, 'port': chassis.port})\n        assert(res.status_code == 201)"}
{"func_code_string": "def send_command(self, obj, command, *arguments):\n        \"\"\" Send command with no output.\n\n        :param obj: requested object.\n        :param command: command to send.\n        :param arguments: list of command arguments.\n        \"\"\"\n        self._perform_command('{}/{}'.format(self.session_url, obj.ref), command, OperReturnType.no_output, *arguments)"}
{"func_code_string": "def send_command_return(self, obj, command, *arguments):\n        \"\"\" Send command with single line output.\n\n        :param obj: requested object.\n        :param command: command to send.\n        :param arguments: list of command arguments.\n        :return: command output.\n        \"\"\"\n        return self._perform_command('{}/{}'.format(self.session_url, obj.ref), command, OperReturnType.line_output,\n                                     *arguments).json()"}
{"func_code_string": "def send_command_return_multilines(self, obj, command, *arguments):\n        \"\"\" Send command with no output.\n\n        :param obj: requested object.\n        :param command: command to send.\n        :param arguments: list of command arguments.\n        :return: list of command output lines.\n        :rtype: list(str)\n        \"\"\"\n        return self._perform_command('{}/{}'.format(self.session_url, obj.ref), command,\n                                     OperReturnType.multiline_output, *arguments).json()"}
{"func_code_string": "def get_attributes(self, obj):\n        \"\"\" Get all object's attributes.\n\n        Sends multi-parameter info/config queries and returns the result as dictionary.\n\n        :param obj: requested object.\n        :returns: dictionary of <name, value> of all attributes returned by the query.\n        :rtype: dict of (str, str)\n        \"\"\"\n        return self._get_attributes('{}/{}'.format(self.session_url, obj.ref))"}
{"func_code_string": "def set_attributes(self, obj, **attributes):\n        \"\"\" Set attributes.\n\n        :param obj: requested object.\n        :param attributes: dictionary of {attribute: value} to set\n        \"\"\"\n\n        attributes_url = '{}/{}/attributes'.format(self.session_url, obj.ref)\n        attributes_list = [{u'name': str(name), u'value': str(value)} for name, value in attributes.items()]\n        self._request(RestMethod.patch, attributes_url, headers={'Content-Type': 'application/json'},\n                      data=json.dumps(attributes_list))"}
{"func_code_string": "def get_stats(self, obj, stat_name):\n        \"\"\" Send CLI command that returns list of integer counters.\n\n        :param obj: requested object.\n        :param stat_name: statistics command name.\n        :return: list of counters.\n        :rtype: list(int)\n        \"\"\"\n        return [int(v) for v in self.send_command_return(obj, stat_name, '?').split()]"}
{"func_code_string": "def init_common_services(self, with_cloud_account=True, zone_name=None):\n        \"\"\"\n        Initialize common service,\n        When 'zone_name' is defined \" at $zone_name\" is added to service names\n        :param bool with_cloud_account:\n        :param str zone_name:\n        :return: OR tuple(Workflow, Vault), OR tuple(Workflow, Vault, CloudAccount) with services\n        \"\"\"\n        zone_names = ZoneConstants(zone_name)\n        type_to_app = lambda t: self.organization.applications[system_application_types.get(t, t)]\n        wf_service = self.organization.service(name=zone_names.DEFAULT_WORKFLOW_SERVICE,\n                                               application=type_to_app(WORKFLOW_SERVICE_TYPE),\n                                               environment=self)\n        key_service = self.organization.service(name=zone_names.DEFAULT_CREDENTIAL_SERVICE,\n                                                application=type_to_app(COBALT_SECURE_STORE_TYPE),\n                                                environment=self)\n        assert wf_service.running()\n        assert key_service.running()\n        if not with_cloud_account:\n            with self as env:\n                env.add_service(wf_service, force=True)\n                env.add_service(key_service, force=True)\n            return wf_service, key_service\n\n        cloud_account_service = self.organization.instance(name=zone_names.DEFAULT_CLOUD_ACCOUNT_SERVICE,\n                                                           application=type_to_app(CLOUD_ACCOUNT_TYPE),\n                                                           environment=self,\n                                                           parameters=PROVIDER_CONFIG,\n                                                           destroyInterval=0)\n        # Imidiate adding to env cause CA not to drop destroy interval. Known issue 6132. So, add service as instance with\n        # destroyInterval set to 'never'\n        assert cloud_account_service.running()\n\n        with self as env:\n            env.add_service(wf_service, force=True)\n            env.add_service(key_service, force=True)\n            env.add_service(cloud_account_service, force=True)\n        return wf_service, key_service, cloud_account_service"}
{"func_code_string": "def clone(self, name=None):\n        \"\"\"\n        :param name: new env name\n        :rtype: Environment\n        \"\"\"\n        resp = self._router.post_env_clone(env_id=self.environmentId, json=dict(name=name) if name else {}).json()\n        return Environment(self.organization, id=resp['id']).init_router(self._router)"}
{"func_code_string": "def default(self):\n        \"\"\"\n            Returns environment marked as default.\n            When Zone is set marked default makes no sense, special env with proper Zone is returned.\n        \"\"\"\n        if ZONE_NAME:\n            log.info(\"Getting or creating default environment for zone with name '{0}'\".format(DEFAULT_ENV_NAME()))\n            zone_id = self.organization.zones[ZONE_NAME].id\n            return self.organization.get_or_create_environment(name=DEFAULT_ENV_NAME(), zone=zone_id)\n\n        def_envs = [env_j[\"id\"] for env_j in self.json() if env_j[\"isDefault\"]]\n\n        if len(def_envs) > 1:\n            log.warning('Found more than one default environment. Picking last.')\n            return self[def_envs[-1]]\n        elif len(def_envs) == 1:\n            return self[def_envs[0]]\n        raise exceptions.NotFoundError('Unable to get default environment')"}
{"func_code_string": "def build_urls(self: NodeVisitor, node: inheritance_diagram) -> Mapping[str, str]:\n    \"\"\"\n    Builds a mapping of class paths to URLs.\n    \"\"\"\n    current_filename = self.builder.current_docname + self.builder.out_suffix\n    urls = {}\n    for child in node:\n        # Another document\n        if child.get(\"refuri\") is not None:\n            uri = child.get(\"refuri\")\n            package_path = child[\"reftitle\"]\n            if uri.startswith(\"http\"):\n                _, _, package_path = uri.partition(\"#\")\n            else:\n                uri = (\n                    pathlib.Path(\"..\")\n                    / pathlib.Path(current_filename).parent\n                    / pathlib.Path(uri)\n                )\n                uri = str(uri).replace(os.path.sep, \"/\")\n            urls[package_path] = uri\n        # Same document\n        elif child.get(\"refid\") is not None:\n            urls[child[\"reftitle\"]] = (\n                \"../\" + current_filename + \"#\" + child.get(\"refid\")\n            )\n    return urls"}
{"func_code_string": "def html_visit_inheritance_diagram(\n    self: NodeVisitor, node: inheritance_diagram\n) -> None:\n    \"\"\"\n    Builds HTML output from an :py:class:`~uqbar.sphinx.inheritance.inheritance_diagram` node.\n    \"\"\"\n    inheritance_graph = node[\"graph\"]\n    urls = build_urls(self, node)\n    graphviz_graph = inheritance_graph.build_graph(urls)\n    dot_code = format(graphviz_graph, \"graphviz\")\n    # TODO: We can perform unflattening here\n    aspect_ratio = inheritance_graph.aspect_ratio\n    if aspect_ratio:\n        aspect_ratio = math.ceil(math.sqrt(aspect_ratio[1] / aspect_ratio[0]))\n    if aspect_ratio > 1:\n        process = subprocess.Popen(\n            [\"unflatten\", \"-l\", str(aspect_ratio), \"-c\", str(aspect_ratio), \"-f\"],\n            stdout=subprocess.PIPE,\n            stdin=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n        )\n        stdout, stderr = process.communicate(dot_code.encode())\n        dot_code = stdout.decode()\n    render_dot_html(self, node, dot_code, {}, \"inheritance\", \"inheritance\")\n    raise SkipNode"}
{"func_code_string": "def latex_visit_inheritance_diagram(\n    self: NodeVisitor, node: inheritance_diagram\n) -> None:\n    \"\"\"\n    Builds LaTeX output from an :py:class:`~uqbar.sphinx.inheritance.inheritance_diagram` node.\n    \"\"\"\n    inheritance_graph = node[\"graph\"]\n    graphviz_graph = inheritance_graph.build_graph()\n    graphviz_graph.attributes[\"size\"] = 6.0\n    dot_code = format(graphviz_graph, \"graphviz\")\n    render_dot_latex(self, node, dot_code, {}, \"inheritance\")\n    raise SkipNode"}
{"func_code_string": "def setup(app) -> Dict[str, Any]:\n    \"\"\"\n    Sets up Sphinx extension.\n    \"\"\"\n    app.setup_extension(\"sphinx.ext.graphviz\")\n    app.add_node(\n        inheritance_diagram,\n        html=(html_visit_inheritance_diagram, None),\n        latex=(latex_visit_inheritance_diagram, None),\n        man=(skip, None),\n        texinfo=(skip, None),\n        text=(skip, None),\n    )\n    app.add_directive(\"inheritance-diagram\", InheritanceDiagram)\n    return {\n        \"version\": uqbar.__version__,\n        \"parallel_read_safe\": True,\n        \"parallel_write_safe\": True,\n    }"}
{"func_code_string": "def run(configuration: str, level: str, target: str, short_format: bool):\n    \"\"\"Run the daemon and all its services\"\"\"\n    initialise_logging(level=level, target=target, short_format=short_format)\n    logger = logging.getLogger(__package__)\n    logger.info('COBalD %s', cobald.__about__.__version__)\n    logger.info(cobald.__about__.__url__)\n    logger.info('%s %s (%s)', platform.python_implementation(), platform.python_version(), sys.executable)\n    logger.debug(cobald.__file__)\n    logger.info('Using configuration %s', configuration)\n    with load(configuration):\n        logger.info('Starting daemon services...')\n        runtime.accept()"}
{"func_code_string": "def cli_run():\n    \"\"\"Run the daemon from a command line interface\"\"\"\n    options = CLI.parse_args()\n    run(options.CONFIGURATION, options.log_level, options.log_target, options.log_journal)"}
{"func_code_string": "def build_body(cls: Type[AN], body: List[ast.stmt]) -> List:\n        \"\"\"\n        Note:\n            Return type is probably ``-> List[AN]``, but can't get it to pass.\n        \"\"\"\n        act_nodes = []  # type: List[ActNode]\n        for child_node in body:\n            act_nodes += ActNode.build(child_node)\n        return act_nodes"}
{"func_code_string": "def build(cls: Type[AN], node: ast.stmt) -> List[AN]:\n        \"\"\"\n        Starting at this ``node``, check if it's an act node. If it's a context\n        manager, recurse into child nodes.\n\n        Returns:\n            List of all act nodes found.\n        \"\"\"\n        if node_is_result_assignment(node):\n            return [cls(node, ActNodeType.result_assignment)]\n        if node_is_pytest_raises(node):\n            return [cls(node, ActNodeType.pytest_raises)]\n        if node_is_unittest_raises(node):\n            return [cls(node, ActNodeType.unittest_raises)]\n\n        token = node.first_token  # type: ignore\n        # Check if line marked with '# act'\n        if token.line.strip().endswith('# act'):\n            return [cls(node, ActNodeType.marked_act)]\n\n        # Recurse (downwards) if it's a context manager\n        if isinstance(node, ast.With):\n            return cls.build_body(node.body)\n\n        return []"}
{"func_code_string": "def ready(self):\n        \"\"\"\n        Checks if organization properly created.\n        Note: New organization must have 'default' environment and two default services\n        running there. Cannot use DEFAULT_ENV_NAME, because zone could be added there.\n        :rtype: bool\n        \"\"\"\n\n        @retry(tries=3, retry_exception=exceptions.NotFoundError)  # org init, takes some times\n        def check_init():\n            env = self.environments['default']\n            return env.services['Default workflow service'].running(timeout=1) and \\\n                   env.services['Default credentials service'].running(timeout=1)\n        return check_init()"}
{"func_code_string": "def create_application(self, name=None, manifest=None):\n        \"\"\" Creates application and returns Application object.\n        \"\"\"\n        if not manifest:\n            raise exceptions.NotEnoughParams('Manifest not set')\n        if not name:\n            name = 'auto-generated-name'\n        from qubell.api.private.application import Application\n        return Application.new(self, name, manifest, self._router)"}
{"func_code_string": "def get_application(self, id=None, name=None):\n        \"\"\" Get application object by name or id.\n        \"\"\"\n        log.info(\"Picking application: %s (%s)\" % (name, id))\n        return self.applications[id or name]"}
{"func_code_string": "def get_or_create_application(self, id=None, manifest=None, name=None):\n        \"\"\" Get application by id or name.\n        If not found: create with given or generated parameters\n        \"\"\"\n        if id:\n            return self.get_application(id=id)\n        elif name:\n            try:\n                app = self.get_application(name=name)\n            except exceptions.NotFoundError:\n                app = self.create_application(name=name, manifest=manifest)\n            return app\n        raise exceptions.NotEnoughParams('Not enough parameters')"}
{"func_code_string": "def application(self, id=None, manifest=None, name=None):\n        \"\"\" Smart method. Creates, picks or modifies application.\n        If application found by name or id and manifest not changed: return app.\n        If app found by id, but other parameters differs: change them.\n        If no application found, create.\n        \"\"\"\n\n        modify = False\n        found = False\n\n        # Try to find application by name or id\n        if name and id:\n            found = self.get_application(id=id)\n            if not found.name == name:\n                modify = True\n        elif id:\n            found = self.get_application(id=id)\n            name = found.name\n        elif name:\n            try:\n                found = self.get_application(name=name)\n            except exceptions.NotFoundError:\n                pass\n\n        # If found - compare parameters\n        if found:\n            if manifest and not manifest == found.manifest:\n                modify = True\n\n        # We need to update application\n        if found and modify:\n            found.update(name=name, manifest=manifest)\n        if not found:\n            created = self.create_application(name=name, manifest=manifest)\n\n        return found or created"}
{"func_code_string": "def create_instance(self, application, revision=None, environment=None, name=None, parameters=None, submodules=None,\n                        destroyInterval=None, manifestVersion=None):\n        \"\"\" Launches instance in application and returns Instance object.\n        \"\"\"\n        from qubell.api.private.instance import Instance\n        return Instance.new(self._router, application, revision, environment, name,\n                            parameters, submodules, destroyInterval, manifestVersion=manifestVersion)"}
{"func_code_string": "def get_instance(self, id=None, name=None):\n        \"\"\" Get instance object by name or id.\n        If application set, search within the application.\n        \"\"\"\n        log.info(\"Picking instance: %s (%s)\" % (name, id))\n        if id:  # submodule instances are invisible for lists\n            return Instance(id=id, organization=self).init_router(self._router)\n        return Instance.get(self._router, self, name)"}
{"func_code_string": "def list_instances_json(self, application=None, show_only_destroyed=False):\n        \"\"\" Get list of instances in json format converted to list\"\"\"\n        # todo: application should not be parameter here. Application should do its own list, just in sake of code reuse\n        q_filter = {'sortBy': 'byCreation', 'descending': 'true',\n                    'mode': 'short',\n                    'from': '0', 'to': '10000'}\n        if not show_only_destroyed:\n            q_filter['showDestroyed'] = 'false'\n        else:\n            q_filter['showDestroyed'] = 'true'\n            q_filter['showRunning'] = 'false'\n            q_filter['showError'] = 'false'\n            q_filter['showLaunching'] = 'false'\n        if application:\n            q_filter[\"applicationFilterId\"] = application.applicationId\n        resp_json = self._router.get_instances(org_id=self.organizationId, params=q_filter).json()\n        if type(resp_json) == dict:\n            instances = [instance for g in resp_json['groups'] for instance in g['records']]\n        else:  # TODO: This is compatibility fix for platform < 37.1\n            instances = resp_json\n\n        return instances"}
{"func_code_string": "def get_or_create_instance(self, id=None, application=None, revision=None, environment=None, name=None, parameters=None, submodules=None,\n                               destroyInterval=None):\n        \"\"\" Get instance by id or name.\n        If not found: create with given parameters\n        \"\"\"\n        try:\n            instance = self.get_instance(id=id, name=name)\n            if name and name != instance.name:\n                instance.rename(name)\n                instance.ready()\n            return instance\n        except exceptions.NotFoundError:\n            return self.create_instance(application, revision, environment, name, parameters, submodules, destroyInterval)"}
{"func_code_string": "def instance(self, id=None, application=None, name=None, revision=None, environment=None, parameters=None, submodules=None, destroyInterval=None):\n        \"\"\" Smart method. It does everything, to return Instance with given parameters within the application.\n        If instance found running and given parameters are actual: return it.\n        If instance found, but parameters differs - reconfigure instance with new parameters.\n        If instance not found: launch instance with given parameters.\n        Return: Instance object.\n        \"\"\"\n        instance = self.get_or_create_instance(id, application, revision, environment, name, parameters, submodules, destroyInterval)\n\n        reconfigure = False\n        # if found:\n        #     if revision and revision is not found.revision:\n        #         reconfigure = True\n        #     if parameters and parameters is not found.parameters:\n        #         reconfigure = True\n\n        # We need to reconfigure instance\n        if reconfigure:\n            instance.reconfigure(revision=revision, parameters=parameters)\n\n        return instance"}
{"func_code_string": "def create_environment(self, name, default=False, zone=None):\n        \"\"\" Creates environment and returns Environment object.\n        \"\"\"\n        from qubell.api.private.environment import Environment\n        return Environment.new(organization=self, name=name, zone_id=zone, default=default, router=self._router)"}
{"func_code_string": "def get_environment(self, id=None, name=None):\n        \"\"\" Get environment object by name or id.\n        \"\"\"\n        log.info(\"Picking environment: %s (%s)\" % (name, id))\n        return self.environments[id or name]"}
{"func_code_string": "def get_or_create_environment(self, id=None, name=None, zone=None, default=False):\n        \"\"\" Get environment by id or name.\n        If not found: create with given or generated parameters\n        \"\"\"\n        if id:\n            return self.get_environment(id=id)\n        elif name:\n            try:\n                env = self.get_environment(name=name)\n                self._assert_env_and_zone(env, zone)\n            except exceptions.NotFoundError:\n                env = self.create_environment(name=name, zone=zone, default=default)\n            return env\n        else:\n            name = 'auto-generated-env'\n            return self.create_environment(name=name, zone=zone, default=default)"}
{"func_code_string": "def environment(self, id=None, name=None, zone=None, default=False):\n        \"\"\" Smart method. Creates, picks or modifies environment.\n        If environment found by name or id parameters not changed: return env.\n        If env found by id, but other parameters differs: change them.\n        If no environment found, create with given parameters.\n        \"\"\"\n\n        found = False\n\n        # Try to find environment by name or id\n        if name and id:\n            found = self.get_environment(id=id)\n        elif id:\n            found = self.get_environment(id=id)\n            name = found.name\n        elif name:\n            try:\n                found = self.get_environment(name=name)\n            except exceptions.NotFoundError:\n                pass\n\n        # If found - compare parameters\n        if found:\n            self._assert_env_and_zone(found, zone)\n            if default and not found.isDefault:\n                found.set_as_default()\n            # TODO: add abilities to change name.\n        if not found:\n            created = self.create_environment(name=name, zone=zone, default=default)\n        return found or created"}
{"func_code_string": "def get_zone(self, id=None, name=None):\n        \"\"\" Get zone object by name or id.\n        \"\"\"\n        log.info(\"Picking zone: %s (%s)\" % (name, id))\n        return self.zones[id or name]"}
{"func_code_string": "def create_role(self, name=None, permissions=\"\"):\n        \"\"\" Creates role \"\"\"\n        name = name or \"autocreated-role\"\n        from qubell.api.private.role import Role\n        return Role.new(self._router, organization=self, name=name, permissions=permissions)"}
{"func_code_string": "def get_role(self, id=None, name=None):\n        \"\"\" Get role object by name or id.\n        \"\"\"\n        log.info(\"Picking role: %s (%s)\" % (name, id))\n        return self.roles[id or name]"}
{"func_code_string": "def get_user(self, id=None, name=None, email=None):\n        \"\"\" Get user object by email or id.\n        \"\"\"\n        log.info(\"Picking user: %s (%s) (%s)\" % (name, email, id))\n        from qubell.api.private.user import User\n        if email:\n            user = User.get(self._router, organization=self, email=email)\n        else:\n            user = self.users[id or name]\n        return user"}
{"func_code_string": "def invite(self, email, roles=None):\n        \"\"\"\n        Send invitation to email with a list of roles\n        :param email:\n        :param roles: None or \"ALL\" or list of role_names\n        :return:\n        \"\"\"\n        if roles is None:\n            role_ids = [self.roles['Guest'].roleId]\n        elif roles == \"ALL\":\n            role_ids = list([i.id for i in self.roles])\n        else:\n            if \"Guest\" not in roles:\n                roles.append('Guest')\n            role_ids = list([i.id for i in self.roles if i.name in roles])\n\n        self._router.invite_user(data=json.dumps({\n            \"organizationId\": self.organizationId,\n            \"email\": email,\n            \"roles\": role_ids}))"}
{"func_code_string": "def init(self, access_key=None, secret_key=None):\n        \"\"\"\n        Mimics wizard's environment preparation\n        \"\"\"\n        if not access_key and not secret_key:\n            self._router.post_init(org_id=self.organizationId, data='{\"initCloudAccount\": true}')\n        else:\n            self._router.post_init(org_id=self.organizationId, data='{}')\n            ca_data = dict(accessKey=access_key, secretKey=secret_key)\n            self._router.post_init_custom_cloud_account(org_id=self.organizationId, data=json.dumps(ca_data))"}
{"func_code_string": "def set_applications_from_meta(self, metadata, exclude=None):\n        \"\"\"\n        Parses meta and update or create each application\n        :param str metadata: path or url to meta.yml\n        :param list[str] exclude: List of application names, to exclude from meta.\n                                  This might be need when you use meta as list of dependencies\n        \"\"\"\n        if not exclude:\n            exclude = []\n        if metadata.startswith('http'):\n            meta = yaml.safe_load(requests.get(url=metadata).content)\n        else:\n            # noinspection PyArgumentEqualDefault\n            meta = yaml.safe_load(open(metadata, 'r').read())\n\n        applications = []\n        for app in meta['kit']['applications']:\n            if app['name'] not in exclude:\n                applications.append({\n                    'name': app['name'],\n                    'url': app['manifest']})\n        self.restore({'applications': applications})"}
{"func_code_string": "def upload_applications(self, metadata, category=None):\n        \"\"\"\n        Mimics get starter-kit and wizard functionality to create components\n        Note: may create component duplicates, not idempotent\n        :type metadata: str\n        :type category: Category\n        :param metadata: url to meta.yml\n        :param category: category\n        \"\"\"\n        upload_json = self._router.get_upload(params=dict(metadataUrl=metadata)).json()\n        manifests = [dict(name=app['name'], manifest=app['url']) for app in upload_json['applications']]\n        if not category:\n            category = self.categories['Application']\n        data = {'categoryId': category.id, 'applications': manifests}\n        self._router.post_application_kits(org_id=self.organizationId, data=json.dumps(data))"}
{"func_code_string": "def process_response(self, request, response):\n        \"\"\"Commits and leaves transaction management.\"\"\"\n        if tldap.transaction.is_managed():\n            tldap.transaction.commit()\n            tldap.transaction.leave_transaction_management()\n        return response"}
{"func_code_string": "def line_protocol(name, tags: dict = None, fields: dict = None, timestamp: float = None) -> str:\n    \"\"\"\n    Format a report as per InfluxDB line protocol\n\n    :param name: name of the report\n    :param tags: tags identifying the specific report\n    :param fields: measurements of the report\n    :param timestamp: when the measurement was taken, in **seconds** since the epoch\n    \"\"\"\n    output_str = name\n    if tags:\n        output_str += ','\n        output_str += ','.join('%s=%s' % (key, value) for key, value in sorted(tags.items()))\n    output_str += ' '\n    output_str += ','.join(('%s=%r' % (key, value)).replace(\"'\", '\"') for key, value in sorted(fields.items()))\n    if timestamp is not None:\n        # line protocol requires nanosecond precision, python uses seconds\n        output_str += ' %d' % (timestamp * 1E9)\n    return output_str + '\\n'"}
{"func_code_string": "def block_type(self):\n        \"\"\" This gets display on the block header. \"\"\"\n        return capfirst(force_text(\n            self.content_block.content_type.model_class()._meta.verbose_name\n        ))"}
{"func_code_string": "def get_default_blocks(self, top=False):\n        \"\"\"\n        Return a list of column default block tuples (URL, verbose name).\n\n        Used for quick add block buttons.\n        \"\"\"\n        default_blocks = []\n\n        for block_model, block_name in self.glitter_page.default_blocks:\n            block = apps.get_model(block_model)\n            base_url = reverse('block_admin:{}_{}_add'.format(\n                block._meta.app_label, block._meta.model_name,\n            ), kwargs={\n                'version_id': self.glitter_page.version.id,\n            })\n            block_qs = {\n                'column': self.name,\n                'top': top,\n            }\n            block_url = '{}?{}'.format(base_url, urlencode(block_qs))\n            block_text = capfirst(force_text(block._meta.verbose_name))\n\n            default_blocks.append((block_url, block_text))\n\n        return default_blocks"}
{"func_code_string": "def add_block_widget(self, top=False):\n        \"\"\"\n        Return a select widget for blocks which can be added to this column.\n        \"\"\"\n        widget = AddBlockSelect(attrs={\n            'class': 'glitter-add-block-select',\n        }, choices=self.add_block_options(top=top))\n\n        return widget.render(name='', value=None)"}
{"func_code_string": "def add_block_options(self, top):\n        \"\"\"\n        Return a list of URLs and titles for blocks which can be added to this column.\n\n        All available blocks are grouped by block category.\n        \"\"\"\n        from .blockadmin import blocks\n\n        block_choices = []\n\n        # Group all block by category\n        for category in sorted(blocks.site.block_list):\n            category_blocks = blocks.site.block_list[category]\n            category_choices = []\n\n            for block in category_blocks:\n                base_url = reverse('block_admin:{}_{}_add'.format(\n                    block._meta.app_label, block._meta.model_name,\n                ), kwargs={\n                    'version_id': self.glitter_page.version.id,\n                })\n                block_qs = {\n                    'column': self.name,\n                    'top': top,\n                }\n                block_url = '{}?{}'.format(base_url, urlencode(block_qs))\n                block_text = capfirst(force_text(block._meta.verbose_name))\n\n                category_choices.append((block_url, block_text))\n\n            category_choices = sorted(category_choices, key=lambda x: x[1])\n            block_choices.append((category, category_choices))\n\n        return block_choices"}
{"func_code_string": "def default_blocks(self):\n        \"\"\"\n        Return a list of default block tuples (appname.ModelName, verbose name).\n\n        Next to the dropdown list of block types, a small number of common blocks which are\n        frequently used can be added immediately to a column with one click. This method defines\n        the list of default blocks.\n        \"\"\"\n        # Use the block list provided by settings if it's defined\n        block_list = getattr(settings, 'GLITTER_DEFAULT_BLOCKS', None)\n\n        if block_list is not None:\n            return block_list\n\n        # Try and auto fill in default blocks if the apps are installed\n        block_list = []\n\n        for block in GLITTER_FALLBACK_BLOCKS:\n            app_name, model_name = block.split('.')\n\n            try:\n                model_class = apps.get_model(app_name, model_name)\n                verbose_name = capfirst(model_class._meta.verbose_name)\n                block_list.append((block, verbose_name))\n            except LookupError:\n                # Block isn't installed - don't add it as a quick add default\n                pass\n\n        return block_list"}
{"func_code_string": "def has_add_permission(self):\n        \"\"\"\n        Returns a boolean if the current user has permission to add another object of the same\n        type which is being viewed/edited.\n        \"\"\"\n        has_permission = False\n\n        if self.user is not None:\n            # We don't check for the object level permission - as the add permission doesn't make\n            # sense on a per object level here.\n            has_permission = self.user.has_perm(\n                '{}.add_{}'.format(self.opts.app_label, self.opts.model_name)\n            )\n\n        return has_permission"}
{"func_code_string": "def has_change_permission(self):\n        \"\"\"\n        Returns a boolean if the current user has permission to change the current object being\n        viewed/edited.\n        \"\"\"\n        has_permission = False\n\n        if self.user is not None:\n            # We check for the object level permission here, even though by default the Django\n            # admin doesn't. If the Django ModelAdmin is extended to allow object level\n            # permissions - then this will work as expected.\n            permission_name = '{}.change_{}'.format(self.opts.app_label, self.opts.model_name)\n            has_permission = (\n                self.user.has_perm(permission_name) or\n                self.user.has_perm(permission_name, obj=self.obj)\n            )\n\n        return has_permission"}
{"func_code_string": "def get_embed_url(self):\n        \"\"\" Get correct embed url for Youtube or Vimeo. \"\"\"\n        embed_url = None\n        youtube_embed_url = 'https://www.youtube.com/embed/{}'\n        vimeo_embed_url = 'https://player.vimeo.com/video/{}'\n\n        # Get video ID from url.\n        if re.match(YOUTUBE_URL_RE, self.url):\n            embed_url = youtube_embed_url.format(re.match(YOUTUBE_URL_RE, self.url).group(2))\n        if re.match(VIMEO_URL_RE, self.url):\n            embed_url = vimeo_embed_url.format(re.match(VIMEO_URL_RE, self.url).group(3))\n        return embed_url"}
{"func_code_string": "def save(self, force_insert=False, force_update=False, using=None, update_fields=None):\n        \"\"\" Set html field with correct iframe. \"\"\"\n        if self.url:\n            iframe_html = '<iframe src=\"{}\" frameborder=\"0\" title=\"{}\" allowfullscreen></iframe>'\n            self.html = iframe_html.format(\n                self.get_embed_url(),\n                self.title\n            )\n        return super().save(force_insert, force_update, using, update_fields)"}
{"func_code_string": "def _get_ip():\n    \"\"\"Get IP address for the docker host\n    \"\"\"\n    cmd_netstat = ['netstat', '-nr']\n    p1 = subprocess.Popen(cmd_netstat, stdout=subprocess.PIPE)\n    cmd_grep = ['grep', '^0\\.0\\.0\\.0']\n    p2 = subprocess.Popen(cmd_grep, stdin=p1.stdout, stdout=subprocess.PIPE)\n    cmd_awk = ['awk', '{ print $2 }']\n    p3 = subprocess.Popen(cmd_awk, stdin=p2.stdout, stdout=subprocess.PIPE)\n    galaxy_ip = p3.stdout.read()\n    log.debug('Host IP determined to be %s', galaxy_ip)\n    return galaxy_ip"}
{"func_code_string": "def get_galaxy_connection(history_id=None, obj=True):\n    \"\"\"\n        Given access to the configuration dict that galaxy passed us, we try and connect to galaxy's API.\n        First we try connecting to galaxy directly, using an IP address given\n        us by docker (since the galaxy host is the default gateway for docker).\n        Using additional information collected by galaxy like the port it is\n        running on and the application path, we build a galaxy URL and test our\n        connection by attempting to get a history listing. This is done to\n        avoid any nasty network configuration that a SysAdmin has placed\n        between galaxy and us inside docker, like disabling API queries.\n        If that fails, we failover to using the URL the user is accessing\n        through. This will succeed where the previous connection fails under\n        the conditions of REMOTE_USER and galaxy running under uWSGI.\n    \"\"\"\n    history_id = history_id or os.environ['HISTORY_ID']\n    key = os.environ['API_KEY']\n\n    ### Customised/Raw galaxy_url ###\n    galaxy_ip = _get_ip()\n    # Substitute $DOCKER_HOST with real IP\n    url = Template(os.environ['GALAXY_URL']).safe_substitute({'DOCKER_HOST': galaxy_ip})\n    gi = _test_url(url, key, history_id, obj=obj)\n    if gi is not None:\n        return gi\n\n    ### Failover, fully auto-detected URL ###\n    # Remove trailing slashes\n    app_path = os.environ['GALAXY_URL'].rstrip('/')\n    # Remove protocol+host:port if included\n    app_path = ''.join(app_path.split('/')[3:])\n\n    if 'GALAXY_WEB_PORT' not in os.environ:\n        # We've failed to detect a port in the config we were given by\n        # galaxy, so we won't be able to construct a valid URL\n        raise Exception(\"No port\")\n    else:\n        # We should be able to find a port to connect to galaxy on via this\n        # conf var: galaxy_paster_port\n        galaxy_port = os.environ['GALAXY_WEB_PORT']\n\n    built_galaxy_url = 'http://%s:%s/%s' % (galaxy_ip.strip(), galaxy_port, app_path.strip())\n    url = built_galaxy_url.rstrip('/')\n\n    gi = _test_url(url, key, history_id, obj=obj)\n    if gi is not None:\n        return gi\n\n    ### Fail ###\n    msg = \"Could not connect to a galaxy instance. Please contact your SysAdmin for help with this error\"\n    raise Exception(msg)"}
{"func_code_string": "def put(filenames, file_type='auto', history_id=None):\n    \"\"\"\n        Given filename[s] of any file accessible to the docker instance, this\n        function will upload that file[s] to galaxy using the current history.\n        Does not return anything.\n    \"\"\"\n    history_id = history_id or os.environ['HISTORY_ID']\n    gi = get_galaxy_connection(history_id=history_id)\n    for filename in filenames:\n        log.debug('Uploading gx=%s history=%s localpath=%s ft=%s', gi, history_id, filename, file_type)\n        history = gi.histories.get(history_id)\n        history.upload_dataset(filename, file_type=file_type)"}
{"func_code_string": "def get(datasets_identifiers, identifier_type='hid', history_id=None):\n    \"\"\"\n        Given the history_id that is displayed to the user, this function will\n        download the file[s] from the history and stores them under /import/\n        Return value[s] are the path[s] to the dataset[s] stored under /import/\n    \"\"\"\n    history_id = history_id or os.environ['HISTORY_ID']\n    # The object version of bioblend is to slow in retrieving all datasets from a history\n    # fallback to the non-object path\n    gi = get_galaxy_connection(history_id=history_id, obj=False)\n    for dataset_identifier in datasets_identifiers:\n        file_path = '/import/%s' % dataset_identifier\n        log.debug('Downloading gx=%s history=%s dataset=%s', gi, history_id, dataset_identifier)\n        # Cache the file requests. E.g. in the example of someone doing something\n        # silly like a get() for a Galaxy file in a for-loop, wouldn't want to\n        # re-download every time and add that overhead.\n        if not os.path.exists(file_path):\n            hc = HistoryClient(gi)\n            dc = DatasetClient(gi)\n            history = hc.show_history(history_id, contents=True)\n            datasets = {ds[identifier_type]: ds['id'] for ds in history}\n            if identifier_type == 'hid':\n                dataset_identifier = int(dataset_identifier)\n            dc.download_dataset(datasets[dataset_identifier], file_path=file_path, use_default_filename=False)\n        else:\n            log.debug('Cached, not re-downloading')\n\n    return file_path"}
{"func_code_string": "def get_user_history (history_id=None):\n    \"\"\"\n       Get all visible dataset infos of user history.\n       Return a list of dict of each dataset.\n    \"\"\" \n    history_id = history_id or os.environ['HISTORY_ID']\n    gi = get_galaxy_connection(history_id=history_id, obj=False)\n    hc = HistoryClient(gi)\n    history = hc.show_history(history_id, visible=True, contents=True)\n    return history"}
{"func_code_string": "def build_act(cls: Type[_Block], node: ast.stmt, test_func_node: ast.FunctionDef) -> _Block:\n        \"\"\"\n        Act block is a single node - either the act node itself, or the node\n        that wraps the act node.\n        \"\"\"\n        add_node_parents(test_func_node)\n        # Walk up the parent nodes of the parent node to find test's definition.\n        act_block_node = node\n        while act_block_node.parent != test_func_node:  # type: ignore\n            act_block_node = act_block_node.parent  # type: ignore\n        return cls([act_block_node], LineType.act)"}
{"func_code_string": "def build_arrange(cls: Type[_Block], nodes: List[ast.stmt], max_line_number: int) -> _Block:\n        \"\"\"\n        Arrange block is all non-pass and non-docstring nodes before the Act\n        block start.\n        \"\"\"\n        return cls(filter_arrange_nodes(nodes, max_line_number), LineType.arrange)"}
{"func_code_string": "def build_assert(cls: Type[_Block], nodes: List[ast.stmt], min_line_number: int) -> _Block:\n        \"\"\"\n        Assert block is all nodes that are after the Act node.\n\n        Note:\n            The filtering is *still* running off the line number of the Act\n            node, when instead it should be using the last line of the Act\n            block.\n        \"\"\"\n        return cls(filter_assert_nodes(nodes, min_line_number), LineType._assert)"}
{"func_code_string": "def get_span(self, first_line_no: int) -> Tuple[int, int]:\n        \"\"\"\n        Raises:\n            EmptyBlock: when block has no nodes\n        \"\"\"\n        if not self.nodes:\n            raise EmptyBlock('span requested from {} block with no nodes'.format(self.line_type))\n        return (\n            get_first_token(self.nodes[0]).start[0] - first_line_no,\n            get_last_token(self.nodes[-1]).start[0] - first_line_no,\n        )"}
{"func_code_string": "def cli_aliases(self):\n        r\"\"\"Developer script aliases.\n        \"\"\"\n        scripting_groups = []\n        aliases = {}\n        for cli_class in self.cli_classes:\n            instance = cli_class()\n            if getattr(instance, \"alias\", None):\n                scripting_group = getattr(instance, \"scripting_group\", None)\n                if scripting_group:\n                    scripting_groups.append(scripting_group)\n                    entry = (scripting_group, instance.alias)\n                    if (scripting_group,) in aliases:\n                        message = \"alias conflict between scripting group\"\n                        message += \" {!r} and {}\"\n                        message = message.format(\n                            scripting_group, aliases[(scripting_group,)].__name__\n                        )\n                        raise Exception(message)\n                    if entry in aliases:\n                        message = \"alias conflict between {} and {}\"\n                        message = message.format(\n                            aliases[entry].__name__, cli_class.__name__\n                        )\n                        raise Exception(message)\n                    aliases[entry] = cli_class\n                else:\n                    entry = (instance.alias,)\n                    if entry in scripting_groups:\n                        message = \"alias conflict between {}\"\n                        message += \" and scripting group {!r}\"\n                        message = message.format(cli_class.__name__, instance.alias)\n                        raise Exception(message)\n                    if entry in aliases:\n                        message = \"alias conflict be {} and {}\"\n                        message = message.format(cli_class.__name__, aliases[entry])\n                        raise Exception(message)\n                    aliases[(instance.alias,)] = cli_class\n            else:\n                if instance.program_name in scripting_groups:\n                    message = \"Alias conflict between {}\"\n                    message += \" and scripting group {!r}\"\n                    message = message.format(cli_class.__name__, instance.program_name)\n                    raise Exception(message)\n                aliases[(instance.program_name,)] = cli_class\n        alias_map = {}\n        for key, value in aliases.items():\n            if len(key) == 1:\n                alias_map[key[0]] = value\n            else:\n                if key[0] not in alias_map:\n                    alias_map[key[0]] = {}\n                alias_map[key[0]][key[1]] = value\n        return alias_map"}
{"func_code_string": "def cli_program_names(self):\n        r\"\"\"Developer script program names.\n        \"\"\"\n        program_names = {}\n        for cli_class in self.cli_classes:\n            instance = cli_class()\n            program_names[instance.program_name] = cli_class\n        return program_names"}
{"func_code_string": "def get_flat_stats(self):\n        \"\"\"\n        :return: statistics as flat table {port/strea,/tpld name {group_stat name: value}}\n        \"\"\"\n        flat_stats = OrderedDict()\n        for obj, port_stats in self.statistics.items():\n            flat_obj_stats = OrderedDict()\n            for group_name, group_values in port_stats.items():\n                for stat_name, stat_value in group_values.items():\n                    full_stat_name = group_name + '_' + stat_name\n                    flat_obj_stats[full_stat_name] = stat_value\n            flat_stats[obj.name] = flat_obj_stats\n        return flat_stats"}
{"func_code_string": "def read_stats(self):\n        \"\"\" Read current ports statistics from chassis.\n\n        :return: dictionary {port name {group name, {stat name: stat value}}}\n        \"\"\"\n\n        self.statistics = TgnObjectsDict()\n        for port in self.session.ports.values():\n            self.statistics[port] = port.read_port_stats()\n        return self.statistics"}
{"func_code_string": "def read_stats(self):\n        \"\"\" Read current statistics from chassis.\n\n        :return: dictionary {stream: {tx: {stat name: stat value}} rx: {tpld: {stat group {stat name: value}}}}\n        \"\"\"\n\n        self.tx_statistics = TgnObjectsDict()\n        for port in self.session.ports.values():\n            for stream in port.streams.values():\n                self.tx_statistics[stream] = stream.read_stats()\n\n        tpld_statistics = XenaTpldsStats(self.session).read_stats()\n\n        self.statistics = TgnObjectsDict()\n        for stream, stream_stats in self.tx_statistics.items():\n            self.statistics[stream] = OrderedDict()\n            self.statistics[stream]['tx'] = stream_stats\n            self.statistics[stream]['rx'] = OrderedDict()\n            stream_tpld = stream.get_attribute('ps_tpldid')\n            for tpld, tpld_stats in tpld_statistics.items():\n                if tpld.id == stream_tpld:\n                    self.statistics[stream]['rx'][tpld] = tpld_stats\n        return self.statistics"}
{"func_code_string": "def read_stats(self):\n        \"\"\" Read current statistics from chassis.\n\n        :return: dictionary {tpld full index {group name {stat name: stat value}}}\n        \"\"\"\n\n        self.statistics = TgnObjectsDict()\n        for port in self.session.ports.values():\n            for tpld in port.tplds.values():\n                self.statistics[tpld] = tpld.read_stats()\n        return self.statistics"}
{"func_code_string": "def get_user_sets(client_id, user_id):\n    \"\"\"Find all user sets.\"\"\"\n    data = api_call('get', 'users/{}/sets'.format(user_id), client_id=client_id)\n    return [WordSet.from_dict(wordset) for wordset in data]"}
{"func_code_string": "def print_user_sets(wordsets, print_terms):\n    \"\"\"Print all user sets by title. If 'print_terms', also prints all terms of all user sets.\n    :param wordsets: List of WordSet.\n    :param print_terms: If True, also prints all terms of all user sets.\n    \"\"\"\n    if not wordsets:\n        print('No sets found')\n    else:\n        print('Found sets: {}'.format(len(wordsets)))\n        for wordset in wordsets:\n            print('    {}'.format(wordset))\n            if print_terms:\n                for term in wordset.terms:\n                    print('        {}'.format(term))"}
{"func_code_string": "def get_common_terms(*api_envs):\n    \"\"\"Get all term duplicates across all user word sets as a list of\n    (title of first word set, title of second word set, set of terms) tuples.\"\"\"\n    common_terms = []\n    # pylint: disable=no-value-for-parameter\n    wordsets = get_user_sets(*api_envs)\n    # pylint: enable=no-value-for-parameter\n\n    for wordset1, wordset2 in combinations(wordsets, 2):\n        common = wordset1.has_common(wordset2)\n        if common:\n            common_terms.append((wordset1.title, wordset2.title, common))\n    return common_terms"}
{"func_code_string": "def print_common_terms(common_terms):\n    \"\"\"Print common terms for each pair of word sets.\n    :param common_terms: Output of get_common_terms().\n    \"\"\"\n    if not common_terms:\n        print('No duplicates')\n    else:\n        for set_pair in common_terms:\n            set1, set2, terms = set_pair\n            print('{} and {} have in common:'.format(set1, set2))\n            for term in terms:\n                print('    {}'.format(term))"}
{"func_code_string": "def delete_term(set_id, term_id, access_token):\n    \"\"\"Delete the given term.\"\"\"\n    api_call('delete', 'sets/{}/terms/{}'.format(set_id, term_id), access_token=access_token)"}
{"func_code_string": "def add_term(set_id, term, access_token):\n    \"\"\"Add the given term to the given set.\n    :param term: Instance of Term.\n    \"\"\"\n    api_call('post', 'sets/{}/terms'.format(set_id), term.to_dict(), access_token=access_token)"}
{"func_code_string": "def reset_term_stats(set_id, term_id, client_id, user_id, access_token):\n    \"\"\"Reset the stats of a term by deleting and re-creating it.\"\"\"\n    found_sets = [user_set for user_set in get_user_sets(client_id, user_id)\n                  if user_set.set_id == set_id]\n    if len(found_sets) != 1:\n        raise ValueError('{} set(s) found with id {}'.format(len(found_sets), set_id))\n    found_terms = [term for term in found_sets[0].terms if term.term_id == term_id]\n    if len(found_terms) != 1:\n        raise ValueError('{} term(s) found with id {}'.format(len(found_terms), term_id))\n    term = found_terms[0]\n\n    if term.image.url:\n        # Creating a term with an image requires an \"image identifier\", which you get by uploading\n        # an image via https://quizlet.com/api/2.0/docs/images , which can only be used by Quizlet\n        # PLUS members.\n        raise NotImplementedError('\"{}\" has an image and is thus not supported'.format(term))\n\n    print('Deleting \"{}\"...'.format(term))\n    delete_term(set_id, term_id, access_token)\n    print('Re-creating \"{}\"...'.format(term))\n    add_term(set_id, term, access_token)\n    print('Done')"}
{"func_code_string": "def createController(self, key, attributes, ipmi, printer=False):\n        \"\"\" Function createController\n        Create a controller node\n\n        @param key: The host name or ID\n        @param attributes:The payload of the host creation\n        @param printer: - False for no creation progression message\n                        - True to get creation progression printed on STDOUT\n                        - Printer class containig a status method for enhanced\n                          print. def printer.status(status, msg, eol=eol)\n        @return RETURN: The API result\n        \"\"\"\n        if key not in self:\n            self.printer = printer\n            self.async = False\n            # Create the VM in foreman\n            self.__printProgression__('In progress',\n                                      key + ' creation: push in Foreman',\n                                      eol='\\r')\n            self.api.create('hosts', attributes, async=self.async)\n            self[key]['interfaces'].append(ipmi)\n            # Wait for puppet catalog to be applied\n            # self.waitPuppetCatalogToBeApplied(key)\n            self.reload()\n            self[key]['build'] = 'true'\n            self[key]['boot'] = 'pxe'\n            self[key]['power'] = 'cycle'\n        return self[key]"}
{"func_code_string": "def waitPuppetCatalogToBeApplied(self, key, sleepTime=5):\n        \"\"\" Function waitPuppetCatalogToBeApplied\n        Wait for puppet catalog to be applied\n\n        @param key: The host name or ID\n        @return RETURN: None\n        \"\"\"\n        # Wait for puppet catalog to be applied\n        loop_stop = False\n        while not loop_stop:\n            status = self[key].getStatus()\n            if status == 'No Changes' or status == 'Active':\n                self.__printProgression__(True,\n                                          key + ' creation: provisioning OK')\n                loop_stop = True\n            elif status == 'Error':\n                self.__printProgression__(False,\n                                          key + ' creation: Error - '\n                                          'Error during provisioning')\n                loop_stop = True\n                return False\n            else:\n                self.__printProgression__('In progress',\n                                          key + ' creation: provisioning ({})'\n                                          .format(status),\n                                          eol='\\r')\n            time.sleep(sleepTime)"}
{"func_code_string": "def createVM(self, key, attributes, printer=False):\n        \"\"\" Function createVM\n        Create a Virtual Machine\n\n        The creation of a VM with libVirt is a bit complexe.\n        We first create the element in foreman, the ask to start before\n        the result of the creation.\n        To do so, we make async calls to the API and check the results\n\n        @param key: The host name or ID\n        @param attributes:The payload of the host creation\n        @param printer: - False for no creation progression message\n                        - True to get creation progression printed on STDOUT\n                        - Printer class containig a status method for enhanced\n                          print. def printer.status(status, msg, eol=eol)\n        @return RETURN: The API result\n        \"\"\"\n\n        self.printer = printer\n        self.async = False\n        # Create the VM in foreman\n        # NOTA: with 1.8 it will return 422 'Failed to login via SSH'\n        self.__printProgression__('In progress',\n                                  key + ' creation: push in Foreman', eol='\\r')\n        asyncCreation = self.api.create('hosts', attributes, async=self.async)\n\n        #  Wait before asking to power on the VM\n        # sleep = 5\n        # for i in range(0, sleep):\n            # time.sleep(1)\n            # self.__printProgression__('In progress',\n                                      # key + ' creation: start in {0}s'\n                                      # .format(sleep - i),\n                                      # eol='\\r')\n\n        #  Power on the VM\n        self.__printProgression__('In progress',\n                                  key + ' creation: starting', eol='\\r')\n        powerOn = self[key].powerOn()\n\n        #  Show Power on result\n        if powerOn['power']:\n            self.__printProgression__('In progress',\n                                      key + ' creation: wait for end of boot',\n                                      eol='\\r')\n        else:\n            self.__printProgression__(False,\n                                      key + ' creation: Error - ' +\n                                      str(powerOn))\n            return False\n        #  Show creation result\n        # NOTA: with 1.8 it will return 422 'Failed to login via SSH'\n        # if asyncCreation.result().status_code is 200:\n            # self.__printProgression__('In progress',\n                                      # key + ' creation: created',\n                                      # eol='\\r')\n        # else:\n            # self.__printProgression__(False,\n                                      # key + ' creation: Error - ' +\n                                      # str(asyncCreation.result()\n                                          # .status_code) + ' - ' +\n                                      # str(asyncCreation.result().text))\n            # return False\n\n        # Wait for puppet catalog to be applied\n        self.waitPuppetCatalogToBeApplied(key)\n\n        return self[key]['id']"}
{"func_code_string": "def get(router, organization, email):\n        \"\"\"\n        :rtype: User\n        \"\"\"\n        log.info(\"Getting user: %s\" % email)\n        resp = router.get_users(org_id=organization.id).json()\n        ids = [x['id'] for x in resp if x['email'] == email]\n        if len(ids):\n            user = User(organization, ids[0]).init_router(router)\n            return user\n        else:\n            raise exceptions.NotFoundError('User with email: %s not found' % email)"}
{"func_code_string": "def construct(self, mapping: dict, **kwargs):\n        \"\"\"\n        Construct an object from a mapping\n\n        :param mapping: the constructor definition, with ``__type__`` name and keyword arguments\n        :param kwargs: additional keyword arguments to pass to the constructor\n        \"\"\"\n        assert '__type__' not in kwargs and '__args__' not in kwargs\n        mapping = {**mapping, **kwargs}\n        factory_fqdn = mapping.pop('__type__')\n        factory = self.load_name(factory_fqdn)\n        args = mapping.pop('__args__', [])\n        return factory(*args, **mapping)"}
{"func_code_string": "def load_name(absolute_name: str):\n        \"\"\"Load an object based on an absolute, dotted name\"\"\"\n        path = absolute_name.split('.')\n        try:\n            __import__(absolute_name)\n        except ImportError:\n            try:\n                obj = sys.modules[path[0]]\n            except KeyError:\n                raise ModuleNotFoundError('No module named %r' % path[0])\n            else:\n                for component in path[1:]:\n                    try:\n                        obj = getattr(obj, component)\n                    except AttributeError as err:\n                        raise ConfigurationError(what='no such object %r: %s' % (absolute_name, err))\n                return obj\n        else:  # ImportError is not raised if ``absolute_name`` points to a valid module\n            return sys.modules[absolute_name]"}
{"func_code_string": "def patch(self, path, value=None):\n        \"\"\" Set specified value to yaml path.\n        Example:\n        patch('application/components/child/configuration/__locator.application-id','777')\n        Will change child app ID to 777\n        \"\"\"\n        # noinspection PyShadowingNames\n        def pathGet(dictionary, path):\n            for item in path.split(\"/\"):\n                dictionary = dictionary[item]\n            return dictionary\n\n        # noinspection PyShadowingNames\n        def pathSet(dictionary, path, value):\n            path = path.split(\"/\")\n            key = path[-1]\n            dictionary = pathGet(dictionary, \"/\".join(path[:-1]))\n            dictionary[key] = value\n\n        # noinspection PyShadowingNames\n        def pathRm(dictionary, path):\n            path = path.split(\"/\")\n            key = path[-1]\n            dictionary = pathGet(dictionary, \"/\".join(path[:-1]))\n            del dictionary[key]\n\n        src = yaml.load(self.content)\n        if value:\n            pathSet(src, path, value)\n        else:\n            pathRm(src, path)\n        self._raw_content = yaml.safe_dump(src, default_flow_style=False)\n        return True"}
{"func_code_string": "async def _await_all(self):\n        \"\"\"Async component of _run\"\"\"\n        delay = 0.0\n        # we run a top-level nursery that automatically reaps/cancels for us\n        async with trio.open_nursery() as nursery:\n            while self.running.is_set():\n                await self._start_payloads(nursery=nursery)\n                await trio.sleep(delay)\n                delay = min(delay + 0.1, 1.0)\n            # cancel the scope to cancel all payloads\n            nursery.cancel_scope.cancel()"}
{"func_code_string": "async def _start_payloads(self, nursery):\n        \"\"\"Start all queued payloads\"\"\"\n        with self._lock:\n            for coroutine in self._payloads:\n                nursery.start_soon(coroutine)\n            self._payloads.clear()\n        await trio.sleep(0)"}
{"func_code_string": "def contains_list(longer, shorter):\n    \"\"\"Check if longer list starts with shorter list\"\"\"\n    if len(longer) <= len(shorter):\n        return False\n    for a, b in zip(shorter, longer):\n        if a != b:\n            return False\n    return True"}
{"func_code_string": "def load(f, dict_=dict):\n    \"\"\"Load and parse toml from a file object\n    An additional argument `dict_` is used to specify the output type\n    \"\"\"\n    if not f.read:\n        raise ValueError('The first parameter needs to be a file object, ',\n                         '%r is passed' % type(f))\n    return loads(f.read(), dict_)"}
{"func_code_string": "def loads(content, dict_=dict):\n    \"\"\"Parse a toml string\n    An additional argument `dict_` is used to specify the output type\n    \"\"\"\n    if not isinstance(content, basestring):\n        raise ValueError('The first parameter needs to be a string object, ',\n                         '%r is passed' % type(content))\n    decoder = Decoder(content, dict_)\n    decoder.parse()\n    return decoder.data"}
{"func_code_string": "def convert(self, line=None, is_end=True):\n        \"\"\"Read the line content and return the converted value\n\n        :param line: the line to feed to converter\n        :param is_end: if set to True, will raise an error if\n        the line has something remaining.\n        \"\"\"\n        if line is not None:\n            self.line = line\n        if not self.line:\n            raise TomlDecodeError(self.parser.lineno,\n                                  'EOF is hit!')\n        token = None\n        self.line = self.line.lstrip()\n        for key, pattern in self.patterns:\n            m = pattern.match(self.line)\n            if m:\n                self.line = self.line[m.end():]\n                handler = getattr(self, 'convert_%s' % key)\n                token = handler(m)\n                break\n        else:\n            raise TomlDecodeError(self.parser.lineno,\n                                  'Parsing error: %r' % self.line)\n        if is_end and not BLANK_RE.match(self.line):\n            raise TomlDecodeError(self.parser.lineno,\n                                  'Something is remained: %r' % self.line)\n        return token"}
{"func_code_string": "def parse(self, data=None, table_name=None):\n        \"\"\"Parse the lines from index i\n\n        :param data: optional, store the parsed result to it when specified\n        :param table_name: when inside a table array, it is the table name\n        \"\"\"\n        temp = self.dict_()\n        sub_table = None\n        is_array = False\n        line = ''\n        while True:\n            line = self._readline()\n            if not line:\n                self._store_table(sub_table, temp, is_array, data=data)\n                break       # EOF\n            if BLANK_RE.match(line):\n                continue\n            if TABLE_RE.match(line):\n                next_table = self.split_string(\n                    TABLE_RE.match(line).group(1), '.', False)\n                if table_name and not contains_list(next_table, table_name):\n                    self._store_table(sub_table, temp, is_array, data=data)\n                    break\n                table = cut_list(next_table, table_name)\n                if sub_table == table:\n                    raise TomlDecodeError(self.lineno, 'Duplicate table name'\n                                          'in origin: %r' % sub_table)\n                else:       # different table name\n                    self._store_table(sub_table, temp, is_array, data=data)\n                    sub_table = table\n                    is_array = False\n            elif TABLE_ARRAY_RE.match(line):\n                next_table = self.split_string(\n                    TABLE_ARRAY_RE.match(line).group(1), '.', False)\n                if table_name and not contains_list(next_table, table_name):\n                    # Out of current loop\n                    # write current data dict to table dict\n                    self._store_table(sub_table, temp, is_array, data=data)\n                    break\n                table = cut_list(next_table, table_name)\n                if sub_table == table and not is_array:\n                    raise TomlDecodeError(self.lineno, 'Duplicate name of '\n                                          'table and array of table: %r'\n                                          % sub_table)\n                else:   # Begin a nested loop\n                    # Write any temp data to table dict\n                    self._store_table(sub_table, temp, is_array, data=data)\n                    sub_table = table\n                    is_array = True\n                    self.parse(temp, next_table)\n            elif KEY_RE.match(line):\n                m = KEY_RE.match(line)\n                keys = self.split_string(m.group(1), '.')\n                value = self.converter.convert(line[m.end():])\n                if value is None:\n                    raise TomlDecodeError(self.lineno, 'Value is missing')\n                self._store_table(keys[:-1], {keys[-1]: value}, data=temp)\n            else:\n                raise TomlDecodeError(self.lineno,\n                                      'Pattern is not recognized: %r' % line)\n        # Rollback to the last line for next parse\n        # This will do nothing if EOF is hit\n        self.instream.seek(self.instream.tell() - len(line))\n        self.lineno -= 1"}
{"func_code_string": "def split_string(self, string, splitter='.', allow_empty=True):\n        \"\"\"Split the string with respect of quotes\"\"\"\n        i = 0\n        rv = []\n        need_split = False\n        while i < len(string):\n            m = re.compile(_KEY_NAME).match(string, i)\n            if not need_split and m:\n                i = m.end()\n                body = m.group(1)\n                if body[:3] == '\"\"\"':\n                    body = self.converter.unescape(body[3:-3])\n                elif body[:3] == \"'''\":\n                    body = body[3:-3]\n                elif body[0] == '\"':\n                    body = self.converter.unescape(body[1:-1])\n                elif body[0] == \"'\":\n                    body = body[1:-1]\n                if not allow_empty and not body:\n                    raise TomlDecodeError(\n                        self.lineno,\n                        'Empty section name is not allowed: %r' % string)\n                rv.append(body)\n                need_split = True\n            elif need_split and string[i] == splitter:\n                need_split = False\n                i += 1\n                continue\n            else:\n                raise TomlDecodeError(self.lineno,\n                                      'Illegal section name: %r' % string)\n        if not need_split:\n            raise TomlDecodeError(\n                self.lineno,\n                'Empty section name is not allowed: %r' % string)\n        return rv"}
{"func_code_string": "def transform_source(text):\n    '''removes a \"where\" clause which is identified by the use of \"where\"\n    as an identifier and ends at the first DEDENT (i.e. decrease in indentation)'''\n    toks = tokenize.generate_tokens(StringIO(text).readline)\n    result = []\n    where_clause = False\n    for toktype, tokvalue, _, _, _ in toks:\n        if toktype == tokenize.NAME and tokvalue == \"where\":\n            where_clause = True\n        elif where_clause and toktype == tokenize.DEDENT:\n            where_clause = False\n            continue\n\n        if not where_clause:\n            result.append((toktype, tokvalue))\n    return tokenize.untokenize(result)"}
{"func_code_string": "def is_visible(self):\n        \"\"\"\n        Return a boolean if the page is visible in navigation.\n\n        Pages must have show in navigation set. Regular pages must be published (published and\n        have a current version - checked with `is_published`), pages with a glitter app associated\n        don't need any page versions.\n        \"\"\"\n        if self.glitter_app_name:\n            visible = self.show_in_navigation\n        else:\n            visible = self.show_in_navigation and self.is_published\n\n        return visible"}
{"func_code_string": "def main(args=sys.argv[1:]):\n    \"\"\"\n    Main function, called from CLI script\n    :return:\n    \"\"\"\n    import mcpartools\n    parser = argparse.ArgumentParser()\n    parser.add_argument('-V', '--version',\n                        action='version',\n                        version=mcpartools.__version__)\n    parser.add_argument('-v', '--verbose',\n                        action='count',\n                        default=0,\n                        help='Give more output. Option is additive, '\n                             'and can be used up to 3 times')\n    parser.add_argument('-q', '--quiet',\n                        action='count',\n                        default=0,\n                        help='Be silent')\n    parser.add_argument('-w', '--workspace',\n                        type=str,\n                        help='workspace directory')\n    parser.add_argument('-m', '--mc_run_template',\n                        type=str,\n                        default=None,\n                        help='path to optional MC run script')\n    parser.add_argument('-s', '--scheduler_options',\n                        type=str,\n                        default=None,\n                        help='optional scheduler options: path to a file or list of options in square brackets')\n    parser.add_argument('-e', '--mc_engine_options',\n                        type=str,\n                        default=None,\n                        help='optional MC engine options: path to a file or list of options in square brackets')\n    parser.add_argument('-x', '--external_files',\n                        nargs='+',  # list may be empty\n                        type=str,\n                        help='list of external files to be copied into each job working directory')\n    parser.add_argument('-b', '--batch',\n                        type=str,\n                        default=None,\n                        choices=[b.id for b in SchedulerDiscover.supported],\n                        help='Available batch systems: {}'.format([b.id for b in SchedulerDiscover.supported]))\n    parser.add_argument('-c', '--collect',\n                        type=str,\n                        default='mv',\n                        choices=Options.collect_methods,\n                        help='Available collect methods')\n    parser.add_argument('-p', '--particle_no',\n                        dest='particle_no',\n                        metavar='particle_no',\n                        type=int,\n                        required=True,\n                        help='number of primary particles per job')\n    parser.add_argument('-j', '--jobs_no',\n                        type=int,\n                        required=True,\n                        help='number of parallel jobs')\n    parser.add_argument('input',\n                        type=str,\n                        help='path to input configuration')\n    # TODO add grouping of options\n    args = parser.parse_args(args)\n\n    if args.quiet:\n        if args.quiet == 1:\n            level = \"WARNING\"\n        elif args.quiet == 2:\n            level = \"ERROR\"\n        else:\n            level = \"CRITICAL\"\n    elif args.verbose:\n        level = \"DEBUG\"\n    else:\n        level = \"INFO\"\n\n    logging.basicConfig(level=level)\n\n    opt = Options(args)\n    generator = Generator(options=opt)\n    ret_code = generator.run()\n\n    return ret_code"}
{"func_code_string": "def build_node_tree(self, source_paths):\n        \"\"\"\n        Build a node tree.\n        \"\"\"\n        import uqbar.apis\n\n        root = PackageNode()\n        # Build node tree, top-down\n        for source_path in sorted(\n            source_paths, key=lambda x: uqbar.apis.source_path_to_package_path(x)\n        ):\n            package_path = uqbar.apis.source_path_to_package_path(source_path)\n            parts = package_path.split(\".\")\n            if not self.document_private_modules and any(\n                part.startswith(\"_\") for part in parts\n            ):\n                continue\n            # Find parent node.\n            parent_node = root\n            if len(parts) > 1:\n                parent_package_path = \".\".join(parts[:-1])\n                try:\n                    parent_node = root[parent_package_path]\n                except KeyError:\n                    parent_node = root\n                try:\n                    if parent_node is root:\n                        # Backfill missing parent node.\n                        grandparent_node = root\n                        if len(parts) > 2:\n                            grandparent_node = root[\n                                parent_package_path.rpartition(\".\")[0]\n                            ]\n                        parent_node = PackageNode(name=parent_package_path)\n                        grandparent_node.append(parent_node)\n                        grandparent_node[:] = sorted(\n                            grandparent_node, key=lambda x: x.package_path\n                        )\n                except KeyError:\n                    parent_node = root\n            # Create or update child node.\n            node_class = ModuleNode\n            if source_path.name == \"__init__.py\":\n                node_class = PackageNode\n            try:\n                # If the child exists, it was previously backfilled.\n                child_node = root[package_path]\n                child_node.source_path = source_path\n            except KeyError:\n                # Otherwise it needs to be created and appended to the parent.\n                child_node = node_class(name=package_path, source_path=source_path)\n                parent_node.append(child_node)\n                parent_node[:] = sorted(parent_node, key=lambda x: x.package_path)\n        # Build documenters, bottom-up.\n        # This allows parent documenters to easily aggregate their children.\n        for node in root.depth_first(top_down=False):\n            kwargs = dict(\n                document_private_members=self.document_private_members,\n                member_documenter_classes=self.member_documenter_classes,\n            )\n            if isinstance(node, ModuleNode):\n                node.documenter = self.module_documenter_class(\n                    node.package_path, **kwargs\n                )\n            else:\n                # Collect references to child modules and packages.\n                node.documenter = self.module_documenter_class(\n                    node.package_path,\n                    module_documenters=[\n                        child.documenter\n                        for child in node\n                        if child.documenter is not None\n                    ],\n                    **kwargs,\n                )\n            if (\n                not self.document_empty_modules\n                and not node.documenter.module_documenters\n                and not node.documenter.member_documenters\n            ):\n                node.parent.remove(node)\n        return root"}
{"func_code_string": "def validate_unique(self):\n        \"\"\"\n        Add this method because django doesn't validate correctly because required fields are\n        excluded.\n        \"\"\"\n        unique_checks, date_checks = self.instance._get_unique_checks(exclude=[])\n        errors = self.instance._perform_unique_checks(unique_checks)\n        if errors:\n            self.add_error(None, errors)"}
{"func_code_string": "def prepare_monitor(tenant=tenant, user=user, password=password, organization=organization, zone_name=zone_name):\n    \"\"\"\n    :param tenant: tenant url\n    :param user: user's email\n    :param password: user's password\n    :param zone_name: (optional) zone_name\n    :return:\n    \"\"\"\n    router = PrivatePath(tenant, verify_codes=False)\n\n    payload = {\n        \"firstName\": \"AllSeeingEye\",\n        \"lastName\": \"Monitor\",\n        \"email\": user,\n        \"password\": password,\n        \"accept\": \"true\"\n    }\n    try:\n        router.post_quick_sign_up(data=payload)\n    except exceptions.ApiUnauthorizedError:\n        pass\n\n    platform = QubellPlatform.connect(tenant=tenant, user=user, password=password)\n    org = platform.organization(name=organization)\n    if zone_name:\n        zone = org.zones[zone_name]\n    else:\n        zone = org.zone\n    env = org.environment(name=\"Monitor for \"+zone.name, zone=zone.id)\n    env.init_common_services(with_cloud_account=False, zone_name=zone_name)\n\n    # todo: move to env\n    policy_name = lambda policy: \"{}.{}\".format(policy.get('action'), policy.get('parameter'))\n    env_data = env.json()\n    key_id = [p for p in env_data['policies'] if 'provisionVms.publicKeyId' == policy_name(p)][0].get('value')\n\n    with env as envbulk:\n        envbulk.add_marker('monitor')\n        envbulk.add_property('publicKeyId', 'string', key_id)\n\n    monitor = Manifest(file=os.path.join(os.path.dirname(__file__), './monitor_manifests/monitor.yml'))\n    monitor_child = Manifest(file=os.path.join(os.path.dirname(__file__), './monitor_manifests/monitor_child.yml'))\n\n    org.application(manifest=monitor_child, name='monitor-child')\n    app = org.application(manifest=monitor, name='monitor')\n\n    return platform, org.id, app.id, env.id"}
{"func_code_string": "def launch(self, timeout=2):\n        \"\"\"\n        Hierapp instance, with environment dependencies:\n        - can be launched within short timeout\n        - auto-destroys shortly\n        \"\"\"\n        self.start_time = time.time()\n        self.end_time = time.time()\n        instance = self.app.launch(environment=self.env)\n        time.sleep(2) # Instance need time to appear in ui\n\n        assert instance.running(timeout=timeout), \"Monitor didn't get Active state\"\n        launched = instance.status == 'Active'\n        instance.reschedule_workflow(workflow_name='destroy', timestamp=self.destroy_interval)\n        assert instance.destroyed(timeout=timeout), \"Monitor didn't get Destroyed after short time\"\n        stopped = instance.status == 'Destroyed'\n        instance.force_remove()\n        self.end_time = time.time()\n        self.status = launched and stopped"}
{"func_code_string": "def clone(self):\n        \"\"\"\n        Do not initialize again since everything is ready to launch app.\n        :return: Initialized monitor instance\n        \"\"\"\n        return Monitor(org=self.org, app=self.app, env=self.env)"}
{"func_code_string": "def from_dict(raw_data):\n        \"\"\"Create Image from raw dictionary data.\"\"\"\n        url = None\n        width = None\n        height = None\n        try:\n            url = raw_data['url']\n            width = raw_data['width']\n            height = raw_data['height']\n        except KeyError:\n            raise ValueError('Unexpected image json structure')\n        except TypeError:\n            # Happens when raw_data is None, i.e. when a term has no image:\n            pass\n        return Image(url, width, height)"}
{"func_code_string": "def to_dict(self):\n        \"\"\"Convert Image into raw dictionary data.\"\"\"\n        if not self.url:\n            return None\n        return {\n            'url': self.url,\n            'width': self.width,\n            'height': self.height\n        }"}
{"func_code_string": "def from_dict(raw_data):\n        \"\"\"Create Term from raw dictionary data.\"\"\"\n        try:\n            definition = raw_data['definition']\n            term_id = raw_data['id']\n            image = Image.from_dict(raw_data['image'])\n            rank = raw_data['rank']\n            term = raw_data['term']\n            return Term(definition, term_id, image, rank, term)\n        except KeyError:\n            raise ValueError('Unexpected term json structure')"}
{"func_code_string": "def to_dict(self):\n        \"\"\"Convert Term into raw dictionary data.\"\"\"\n        return {\n            'definition': self.definition,\n            'id': self.term_id,\n            'image': self.image.to_dict(),\n            'rank': self.rank,\n            'term': self.term\n        }"}
{"func_code_string": "def has_common(self, other):\n        \"\"\"Return set of common words between two word sets.\"\"\"\n        if not isinstance(other, WordSet):\n            raise ValueError('Can compare only WordSets')\n        return self.term_set & other.term_set"}
{"func_code_string": "def from_dict(raw_data):\n        \"\"\"Create WordSet from raw dictionary data.\"\"\"\n        try:\n            set_id = raw_data['id']\n            title = raw_data['title']\n            terms = [Term.from_dict(term) for term in raw_data['terms']]\n            return WordSet(set_id, title, terms)\n        except KeyError:\n            raise ValueError('Unexpected set json structure')"}
{"func_code_string": "def to_dict(self):\n        \"\"\"Convert WordSet into raw dictionary data.\"\"\"\n        return {\n            'id': self.set_id,\n            'title': self.title,\n            'terms': [term.to_dict() for term in self.terms]\n        }"}
{"func_code_string": "def release(ctx, yes, latest):\n    \"\"\"Create a new release in github\n    \"\"\"\n    m = RepoManager(ctx.obj['agile'])\n    api = m.github_repo()\n    if latest:\n        latest = api.releases.latest()\n        if latest:\n            click.echo(latest['tag_name'])\n    elif m.can_release('sandbox'):\n        branch = m.info['branch']\n        version = m.validate_version()\n        name = 'v%s' % version\n        body = ['Release %s from agiletoolkit' % name]\n        data = dict(\n            tag_name=name,\n            target_commitish=branch,\n            name=name,\n            body='\\n\\n'.join(body),\n            draft=False,\n            prerelease=False\n        )\n        if yes:\n            data = api.releases.create(data=data)\n            m.message('Successfully created a new Github release')\n        click.echo(niceJson(data))\n    else:\n        click.echo('skipped')"}
{"func_code_string": "def on_builder_inited(app):\n    \"\"\"\n    Hooks into Sphinx's ``builder-inited`` event.\n    \"\"\"\n    app.cache_db_path = \":memory:\"\n    if app.config[\"uqbar_book_use_cache\"]:\n        logger.info(bold(\"[uqbar-book]\"), nonl=True)\n        logger.info(\" initializing cache db\")\n        app.connection = uqbar.book.sphinx.create_cache_db(app.cache_db_path)"}
{"func_code_string": "def on_config_inited(app, config):\n    \"\"\"\n    Hooks into Sphinx's ``config-inited`` event.\n    \"\"\"\n    extension_paths = config[\"uqbar_book_extensions\"] or [\n        \"uqbar.book.extensions.GraphExtension\"\n    ]\n    app.uqbar_book_extensions = []\n    for extension_path in extension_paths:\n        module_name, _, class_name = extension_path.rpartition(\".\")\n        module = importlib.import_module(module_name)\n        extension_class = getattr(module, class_name)\n        extension_class.setup_sphinx(app)\n        app.uqbar_book_extensions.append(extension_class)"}
{"func_code_string": "def on_doctree_read(app, document):\n    \"\"\"\n    Hooks into Sphinx's ``doctree-read`` event.\n    \"\"\"\n    literal_blocks = uqbar.book.sphinx.collect_literal_blocks(document)\n    cache_mapping = uqbar.book.sphinx.group_literal_blocks_by_cache_path(literal_blocks)\n    node_mapping = {}\n    use_cache = bool(app.config[\"uqbar_book_use_cache\"])\n    for cache_path, literal_block_groups in cache_mapping.items():\n        kwargs = dict(\n            extensions=app.uqbar_book_extensions,\n            setup_lines=app.config[\"uqbar_book_console_setup\"],\n            teardown_lines=app.config[\"uqbar_book_console_teardown\"],\n            use_black=bool(app.config[\"uqbar_book_use_black\"]),\n        )\n        for literal_blocks in literal_block_groups:\n            try:\n                if use_cache:\n                    local_node_mapping = uqbar.book.sphinx.interpret_code_blocks_with_cache(\n                        literal_blocks, cache_path, app.connection, **kwargs\n                    )\n                else:\n                    local_node_mapping = uqbar.book.sphinx.interpret_code_blocks(\n                        literal_blocks, **kwargs\n                    )\n                node_mapping.update(local_node_mapping)\n            except ConsoleError as exception:\n                message = exception.args[0].splitlines()[-1]\n                logger.warning(message, location=exception.args[1])\n                if app.config[\"uqbar_book_strict\"]:\n                    raise\n    uqbar.book.sphinx.rebuild_document(document, node_mapping)"}
{"func_code_string": "def on_build_finished(app, exception):\n    \"\"\"\n    Hooks into Sphinx's ``build-finished`` event.\n    \"\"\"\n    if not app.config[\"uqbar_book_use_cache\"]:\n        return\n    logger.info(\"\")\n    for row in app.connection.execute(\"SELECT path, hits FROM cache ORDER BY path\"):\n        path, hits = row\n        if not hits:\n            continue\n        logger.info(bold(\"[uqbar-book]\"), nonl=True)\n        logger.info(\" Cache hits for {}: {}\".format(path, hits))"}
{"func_code_string": "def handle_class(signature_node, module, object_name, cache):\n    \"\"\"\n    Styles ``autoclass`` entries.\n\n    Adds ``abstract`` prefix to abstract classes.\n    \"\"\"\n    class_ = getattr(module, object_name, None)\n    if class_ is None:\n        return\n    if class_ not in cache:\n        cache[class_] = {}\n        attributes = inspect.classify_class_attrs(class_)\n        for attribute in attributes:\n            cache[class_][attribute.name] = attribute\n    if inspect.isabstract(class_):\n        emphasis = nodes.emphasis(\"abstract \", \"abstract \", classes=[\"property\"])\n        signature_node.insert(0, emphasis)"}
{"func_code_string": "def handle_method(signature_node, module, object_name, cache):\n    \"\"\"\n    Styles ``automethod`` entries.\n\n    Adds ``abstract`` prefix to abstract methods.\n\n    Adds link to originating class for inherited methods.\n    \"\"\"\n    *class_names, attr_name = object_name.split(\".\")  # Handle nested classes\n    class_ = module\n    for class_name in class_names:\n        class_ = getattr(class_, class_name, None)\n        if class_ is None:\n            return\n    attr = getattr(class_, attr_name)\n    try:\n        inspected_attr = cache[class_][attr_name]\n        defining_class = inspected_attr.defining_class\n    except KeyError:\n        # TODO: This is a hack to handle bad interaction between enum and inspect\n        defining_class = class_\n    if defining_class is not class_:\n        reftarget = \"{}.{}\".format(defining_class.__module__, defining_class.__name__)\n        xref_node = addnodes.pending_xref(\n            \"\", refdomain=\"py\", refexplicit=True, reftype=\"class\", reftarget=reftarget\n        )\n        name_node = nodes.literal(\n            \"\", \"{}\".format(defining_class.__name__), classes=[\"descclassname\"]\n        )\n        xref_node.append(name_node)\n        desc_annotation = list(signature_node.traverse(addnodes.desc_annotation))\n        index = len(desc_annotation)\n        class_annotation = addnodes.desc_addname()\n        class_annotation.extend([nodes.Text(\"(\"), xref_node, nodes.Text(\").\")])\n        class_annotation[\"xml:space\"] = \"preserve\"\n        signature_node.insert(index, class_annotation)\n    else:\n        is_overridden = False\n        for class_ in defining_class.__mro__[1:]:\n            if hasattr(class_, attr_name):\n                is_overridden = True\n        if is_overridden:\n            emphasis = nodes.emphasis(\n                \"overridden \", \"overridden \", classes=[\"property\"]\n            )\n            signature_node.insert(0, emphasis)\n    if getattr(attr, \"__isabstractmethod__\", False):\n        emphasis = nodes.emphasis(\"abstract\", \"abstract\", classes=[\"property\"])\n        signature_node.insert(0, emphasis)"}
{"func_code_string": "def on_doctree_read(app, document):\n    \"\"\"\n    Hooks into Sphinx's ``doctree-read`` event.\n    \"\"\"\n    cache: Dict[type, Dict[str, object]] = {}\n    for desc_node in document.traverse(addnodes.desc):\n        if desc_node.get(\"domain\") != \"py\":\n            continue\n        signature_node = desc_node.traverse(addnodes.desc_signature)[0]\n        module_name = signature_node.get(\"module\")\n        object_name = signature_node.get(\"fullname\")\n        object_type = desc_node.get(\"objtype\")\n        module = importlib.import_module(module_name)\n        if object_type == \"class\":\n            handle_class(signature_node, module, object_name, cache)\n        elif object_type in (\"method\", \"attribute\", \"staticmethod\", \"classmethod\"):\n            handle_method(signature_node, module, object_name, cache)"}
{"func_code_string": "def on_builder_inited(app):\n    \"\"\"\n    Hooks into Sphinx's ``builder-inited`` event.\n\n    Used for copying over CSS files to theme directory.\n    \"\"\"\n    local_css_path = pathlib.Path(__file__).parent / \"uqbar.css\"\n    theme_css_path = (\n        pathlib.Path(app.srcdir) / app.config.html_static_path[0] / \"uqbar.css\"\n    )\n    with local_css_path.open(\"r\") as file_pointer:\n        local_css_contents = file_pointer.read()\n    uqbar.io.write(local_css_contents, theme_css_path)"}
{"func_code_string": "def setup(app) -> Dict[str, Any]:\n    \"\"\"\n    Sets up Sphinx extension.\n    \"\"\"\n    app.connect(\"doctree-read\", on_doctree_read)\n    app.connect(\"builder-inited\", on_builder_inited)\n    app.add_css_file(\"uqbar.css\")\n    app.add_node(\n        nodes.classifier, override=True, html=(visit_classifier, depart_classifier)\n    )\n    app.add_node(\n        nodes.definition, override=True, html=(visit_definition, depart_definition)\n    )\n    app.add_node(nodes.term, override=True, html=(visit_term, depart_term))\n    return {\n        \"version\": uqbar.__version__,\n        \"parallel_read_safe\": True,\n        \"parallel_write_safe\": True,\n    }"}
{"func_code_string": "def init_xena(api, logger, owner, ip=None, port=57911):\n    \"\"\" Create XenaManager object.\n\n    :param api: cli/rest\n    :param logger: python logger\n    :param owner: owner of the scripting session\n    :param ip: rest server IP\n    :param port: rest server TCP port\n    :return: Xena object\n    :rtype: XenaApp\n    \"\"\"\n\n    if api == ApiType.socket:\n        api_wrapper = XenaCliWrapper(logger)\n    elif api == ApiType.rest:\n        api_wrapper = XenaRestWrapper(logger, ip, port)\n    return XenaApp(logger, owner, api_wrapper)"}
{"func_code_string": "def add_chassis(self, chassis, port=22611, password='xena'):\n        \"\"\" Add chassis.\n\n        XenaManager-2G -> Add Chassis.\n\n        :param chassis: chassis IP address\n        :param port: chassis port number\n        :param password: chassis password\n        :return: newly created chassis\n        :rtype: xenamanager.xena_app.XenaChassis\n        \"\"\"\n\n        if chassis not in self.chassis_list:\n            try:\n                XenaChassis(self, chassis, port, password)\n            except Exception as error:\n                self.objects.pop('{}/{}'.format(self.owner, chassis))\n                raise error\n        return self.chassis_list[chassis]"}
{"func_code_string": "def inventory(self):\n        \"\"\" Get inventory for all chassis. \"\"\"\n\n        for chassis in self.chassis_list.values():\n            chassis.inventory(modules_inventory=True)"}
{"func_code_string": "def reserve_ports(self, locations, force=False, reset=True):\n        \"\"\" Reserve ports and reset factory defaults.\n\n        XenaManager-2G -> Reserve/Relinquish Port.\n        XenaManager-2G -> Reserve Port.\n\n        :param locations: list of ports locations in the form <ip/slot/port> to reserve\n        :param force: True - take forcefully. False - fail if port is reserved by other user\n        :param reset: True - reset port, False - leave port configuration\n        :return: ports dictionary (index: object)\n        \"\"\"\n\n        for location in locations:\n            ip, module, port = location.split('/')\n            self.chassis_list[ip].reserve_ports(['{}/{}'.format(module, port)], force, reset)\n\n        return self.ports"}
{"func_code_string": "def start_traffic(self, blocking=False, *ports):\n        \"\"\" Start traffic on list of ports.\n\n        :param blocking: True - start traffic and wait until traffic ends, False - start traffic and return.\n        :param ports: list of ports to start traffic on. Default - all session ports.\n        \"\"\"\n\n        for chassis, chassis_ports in self._per_chassis_ports(*self._get_operation_ports(*ports)).items():\n            chassis.start_traffic(False, *chassis_ports)\n        if blocking:\n            for chassis, chassis_ports in self._per_chassis_ports(*self._get_operation_ports(*ports)).items():\n                chassis.wait_traffic(*chassis_ports)"}
{"func_code_string": "def stop_traffic(self, *ports):\n        \"\"\" Stop traffic on list of ports.\n\n        :param ports: list of ports to stop traffic on. Default - all session ports.\n        \"\"\"\n\n        for chassis, chassis_ports in self._per_chassis_ports(*self._get_operation_ports(*ports)).items():\n            chassis.stop_traffic(*chassis_ports)"}
{"func_code_string": "def ports(self):\n        \"\"\"\n        :return: dictionary {name: object} of all ports.\n        \"\"\"\n\n        ports = {}\n        for chassis in self.chassis_list.values():\n            ports.update({str(p): p for p in chassis.get_objects_by_type('port')})\n        return ports"}
{"func_code_string": "def inventory(self, modules_inventory=False):\n        \"\"\" Get chassis inventory.\n\n        :param modules_inventory: True - read modules inventory, false - don't read.\n        \"\"\"\n\n        self.c_info = self.get_attributes()\n        for m_index, m_portcounts in enumerate(self.c_info['c_portcounts'].split()):\n            if int(m_portcounts):\n                module = XenaModule(parent=self, index=m_index)\n                if modules_inventory:\n                    module.inventory()"}
{"func_code_string": "def reserve_ports(self, locations, force=False, reset=True):\n        \"\"\" Reserve ports and reset factory defaults.\n\n        XenaManager-2G -> Reserve/Relinquish Port.\n        XenaManager-2G -> Reset port.\n\n        :param locations: list of ports locations in the form <module/port> to reserve\n        :param force: True - take forcefully, False - fail if port is reserved by other user\n        :param reset: True - reset port, False - leave port configuration\n        :return: ports dictionary (index: object)\n        \"\"\"\n\n        for location in locations:\n            port = XenaPort(parent=self, index=location)\n            port.reserve(force)\n            if reset:\n                port.reset()\n\n        return self.ports"}
{"func_code_string": "def start_traffic(self, blocking=False, *ports):\n        \"\"\" Start traffic on list of ports.\n\n        :param blocking: True - start traffic and wait until traffic ends, False - start traffic and return.\n        :param ports: list of ports to start traffic on. Default - all session ports.\n        \"\"\"\n\n        self._traffic_command('on', *ports)\n        if blocking:\n            self.wait_traffic(*ports)"}
{"func_code_string": "def modules(self):\n        \"\"\"\n        :return: dictionary {index: object} of all modules.\n        \"\"\"\n\n        if not self.get_objects_by_type('module'):\n            self.inventory()\n        return {int(c.index): c for c in self.get_objects_by_type('module')}"}
{"func_code_string": "def inventory(self):\n        \"\"\" Get module inventory. \"\"\"\n\n        self.m_info = self.get_attributes()\n        if 'NOTCFP' in self.m_info['m_cfptype']:\n            a = self.get_attribute('m_portcount')\n            m_portcount = int(a)\n        else:\n            m_portcount = int(self.get_attribute('m_cfpconfig').split()[0])\n        for p_index in range(m_portcount):\n            XenaPort(parent=self, index='{}/{}'.format(self.index, p_index)).inventory()"}
{"func_code_string": "def ports(self):\n        \"\"\"\n        :return: dictionary {index: object} of all ports.\n        \"\"\"\n\n        if not self.get_objects_by_type('port'):\n            self.inventory()\n        return {int(p.index.split('/')[1]): p for p in self.get_objects_by_type('port')}"}
{"func_code_string": "def run(entry_point, drivers, loop = None):\n    ''' This is a runner wrapping the cyclotron \"run\" implementation. It takes\n    an additional parameter to provide a custom asyncio mainloop.\n    '''\n    program = setup(entry_point, drivers)\n    dispose = program.run()\n    if loop == None:\n        loop = asyncio.get_event_loop()\n\n    loop.run_forever()\n    dispose()"}
{"func_code_string": "def register(model, admin=None, category=None):\n    \"\"\" Decorator to registering you Admin class. \"\"\"\n    def _model_admin_wrapper(admin_class):\n\n        site.register(model, admin_class=admin_class)\n\n        if category:\n            site.register_block(model, category)\n\n        return admin_class\n    return _model_admin_wrapper"}
{"func_code_string": "def has_glitter_edit_permission(self, request, obj):\n        \"\"\"\n        Return a boolean if a user has edit access to the glitter object/page this object is on.\n        \"\"\"\n\n        # We're testing for the edit permission here with the glitter object - not the current\n        # object, not the change permission. Once a user has edit access to an object they can edit\n        # all content on it.\n        permission_name = '{}.edit_{}'.format(\n            obj._meta.app_label, obj._meta.model_name,\n        )\n        has_permission = (\n            request.user.has_perm(permission_name) or\n            request.user.has_perm(permission_name, obj=obj)\n        )\n        return has_permission"}
{"func_code_string": "def change_view(self, request, object_id, form_url='', extra_context=None):\n        \"\"\"The 'change' admin view for this model.\"\"\"\n\n        obj = self.get_object(request, unquote(object_id))\n\n        if obj is None:\n            raise Http404(_('%(name)s object with primary key %(key)r does not exist.') % {\n                'name': force_text(self.opts.verbose_name),\n                'key': escape(object_id),\n            })\n\n        if not self.has_change_permission(request, obj):\n            raise PermissionDenied\n\n        content_block = obj.content_block\n        version = content_block.obj_version\n\n        # Version must not be saved, and must belong to this user\n        if version.version_number or version.owner != request.user:\n            raise PermissionDenied\n\n        return super().change_view(request, object_id, form_url, extra_context)"}
{"func_code_string": "def response_change(self, request, obj):\n        \"\"\"Determine the HttpResponse for the change_view stage.\"\"\"\n        opts = self.opts.app_label, self.opts.model_name\n        pk_value = obj._get_pk_val()\n\n        if '_continue' in request.POST:\n            msg = _(\n                'The %(name)s block was changed successfully. You may edit it again below.'\n            ) % {'name': force_text(self.opts.verbose_name)}\n\n            self.message_user(request, msg, messages.SUCCESS)\n\n            # We redirect to the save and continue page, which updates the\n            # parent window in javascript and redirects back to the edit page\n            # in javascript.\n            return HttpResponseRedirect(reverse(\n                'admin:%s_%s_continue' % opts,\n                args=(pk_value,),\n                current_app=self.admin_site.name\n            ))\n\n        # Update column and close popup - don't bother with a message as they won't see it\n        return self.response_rerender(request, obj, 'admin/glitter/update_column.html')"}
{"func_code_string": "def get_filter_item(name: str, operation: bytes, value: bytes) -> bytes:\n    \"\"\"\n    A field could be found for this term, try to get filter string for it.\n    \"\"\"\n    assert isinstance(name, str)\n    assert isinstance(value, bytes)\n    if operation is None:\n        return filter_format(b\"(%s=%s)\", [name, value])\n    elif operation == \"contains\":\n        assert value != \"\"\n        return filter_format(b\"(%s=*%s*)\", [name, value])\n    else:\n        raise ValueError(\"Unknown search operation %s\" % operation)"}
{"func_code_string": "def get_filter(q: tldap.Q, fields: Dict[str, tldap.fields.Field], pk: str):\n    \"\"\"\n    Translate the Q tree into a filter string to search for, or None\n    if no results possible.\n    \"\"\"\n    # check the details are valid\n    if q.negated and len(q.children) == 1:\n        op = b\"!\"\n    elif q.connector == tldap.Q.AND:\n        op = b\"&\"\n    elif q.connector == tldap.Q.OR:\n        op = b\"|\"\n    else:\n        raise ValueError(\"Invalid value of op found\")\n\n    # scan through every child\n    search = []\n    for child in q.children:\n        # if this child is a node, then descend into it\n        if isinstance(child, tldap.Q):\n            search.append(get_filter(child, fields, pk))\n        else:\n            # otherwise get the values in this node\n            name, value = child\n\n            # split the name if possible\n            name, _, operation = name.rpartition(\"__\")\n            if name == \"\":\n                name, operation = operation, None\n\n            # replace pk with the real attribute\n            if name == \"pk\":\n                name = pk\n\n            # DN is a special case\n            if name == \"dn\":\n                dn_name = \"entryDN:\"\n                if isinstance(value, list):\n                    s = []\n                    for v in value:\n                        assert isinstance(v, str)\n                        v = v.encode('utf_8')\n                        s.append(get_filter_item(dn_name, operation, v))\n                    search.append(\"(&\".join(search) + \")\")\n\n                # or process just the single value\n                else:\n                    assert isinstance(value, str)\n                    v = value.encode('utf_8')\n                    search.append(get_filter_item(dn_name, operation, v))\n                continue\n\n            # try to find field associated with name\n            field = fields[name]\n            if isinstance(value, list) and len(value) == 1:\n                value = value[0]\n                assert isinstance(value, str)\n\n            # process as list\n            if isinstance(value, list):\n                s = []\n                for v in value:\n                    v = field.value_to_filter(v)\n                    s.append(get_filter_item(name, operation, v))\n                search.append(b\"(&\".join(search) + b\")\")\n\n            # or process just the single value\n            else:\n                value = field.value_to_filter(value)\n                search.append(get_filter_item(name, operation, value))\n\n    # output the results\n    if len(search) == 1 and not q.negated:\n        # just one non-negative term, return it\n        return search[0]\n    else:\n        # multiple terms\n        return b\"(\" + op + b\"\".join(search) + b\")\""}
{"func_code_string": "def program_name(self):\n        r\"\"\"The name of the script, callable from the command line.\n        \"\"\"\n        name = \"-\".join(\n            word.lower() for word in uqbar.strings.delimit_words(type(self).__name__)\n        )\n        return name"}
{"func_code_string": "def node_is_result_assignment(node: ast.AST) -> bool:\n    \"\"\"\n    Args:\n        node: An ``ast`` node.\n\n    Returns:\n        bool: ``node`` corresponds to the code ``result =``, assignment to the\n        ``result `` variable.\n\n    Note:\n        Performs a very weak test that the line starts with 'result =' rather\n        than testing the tokens.\n    \"\"\"\n    # `.first_token` is added by asttokens\n    token = node.first_token  # type: ignore\n    return token.line.strip().startswith('result =')"}
{"func_code_string": "def node_is_noop(node: ast.AST) -> bool:\n    \"\"\"\n    Node does nothing.\n    \"\"\"\n    return isinstance(node.value, ast.Str) if isinstance(node, ast.Expr) else isinstance(node, ast.Pass)"}
{"func_code_string": "def function_is_noop(function_node: ast.FunctionDef) -> bool:\n    \"\"\"\n    Function does nothing - is just ``pass`` or docstring.\n    \"\"\"\n    return all(node_is_noop(n) for n in function_node.body)"}
{"func_code_string": "def add_node_parents(root: ast.AST) -> None:\n    \"\"\"\n    Adds \"parent\" attribute to all child nodes of passed node.\n\n    Code taken from https://stackoverflow.com/a/43311383/1286705\n    \"\"\"\n    for node in ast.walk(root):\n        for child in ast.iter_child_nodes(node):\n            child.parent = node"}
{"func_code_string": "def build_footprint(node: ast.AST, first_line_no: int) -> Set[int]:\n    \"\"\"\n    Generates a list of lines that the passed node covers, relative to the\n    marked lines list - i.e. start of function is line 0.\n    \"\"\"\n    return set(\n        range(\n            get_first_token(node).start[0] - first_line_no,\n            get_last_token(node).end[0] - first_line_no + 1,\n        )\n    )"}
{"func_code_string": "def filter_arrange_nodes(nodes: List[ast.stmt], max_line_number: int) -> List[ast.stmt]:\n    \"\"\"\n    Finds all nodes that are before the ``max_line_number`` and are not\n    docstrings or ``pass``.\n    \"\"\"\n    return [\n        node for node in nodes if node.lineno < max_line_number and not isinstance(node, ast.Pass)\n        and not (isinstance(node, ast.Expr) and isinstance(node.value, ast.Str))\n    ]"}
{"func_code_string": "def filter_assert_nodes(nodes: List[ast.stmt], min_line_number: int) -> List[ast.stmt]:\n    \"\"\"\n    Finds all nodes that are after the ``min_line_number``\n    \"\"\"\n    return [node for node in nodes if node.lineno > min_line_number]"}
{"func_code_string": "def find_stringy_lines(tree: ast.AST, first_line_no: int) -> Set[int]:\n    \"\"\"\n    Finds all lines that contain a string in a tree, usually a function. These\n    lines will be ignored when searching for blank lines.\n    \"\"\"\n    str_footprints = set()\n    for node in ast.walk(tree):\n        if isinstance(node, ast.Str):\n            str_footprints.update(build_footprint(node, first_line_no))\n    return str_footprints"}
{"func_code_string": "def check_all(self) -> Generator[AAAError, None, None]:\n        \"\"\"\n        Run everything required for checking this function.\n\n        Returns:\n            A generator of errors.\n\n        Raises:\n            ValidationError: A non-recoverable linting error is found.\n        \"\"\"\n        # Function def\n        if function_is_noop(self.node):\n            return\n        self.mark_bl()\n        self.mark_def()\n        # ACT\n        # Load act block and kick out when none is found\n        self.act_node = self.load_act_node()\n        self.act_block = Block.build_act(self.act_node.node, self.node)\n        act_block_first_line_no, act_block_last_line_no = self.act_block.get_span(0)\n        # ARRANGE\n        self.arrange_block = Block.build_arrange(self.node.body, act_block_first_line_no)\n        # ASSERT\n        assert self.act_node\n        self.assert_block = Block.build_assert(self.node.body, act_block_last_line_no)\n        # SPACING\n        for block in ['arrange', 'act', 'assert']:\n            self_block = getattr(self, '{}_block'.format(block))\n            try:\n                span = self_block.get_span(self.first_line_no)\n            except EmptyBlock:\n                continue\n            self.line_markers.update(span, self_block.line_type)\n        yield from self.line_markers.check_arrange_act_spacing()\n        yield from self.line_markers.check_act_assert_spacing()\n        yield from self.line_markers.check_blank_lines()"}
{"func_code_string": "def load_act_node(self) -> ActNode:\n        \"\"\"\n        Raises:\n            ValidationError: AAA01 when no act block is found and AAA02 when\n                multiple act blocks are found.\n        \"\"\"\n        act_nodes = ActNode.build_body(self.node.body)\n\n        if not act_nodes:\n            raise ValidationError(self.first_line_no, self.node.col_offset, 'AAA01 no Act block found in test')\n\n        # Allow `pytest.raises` and `self.assertRaises()` in assert nodes - if\n        # any of the additional nodes are `pytest.raises`, then raise\n        for a_n in act_nodes[1:]:\n            if a_n.block_type in [ActNodeType.marked_act, ActNodeType.result_assignment]:\n                raise ValidationError(\n                    self.first_line_no,\n                    self.node.col_offset,\n                    'AAA02 multiple Act blocks found in test',\n                )\n\n        return act_nodes[0]"}
{"func_code_string": "def get_line_relative_to_node(self, target_node: ast.AST, offset: int) -> str:\n        \"\"\"\n        Raises:\n            IndexError: when ``offset`` takes the request out of bounds of this\n                Function's lines.\n        \"\"\"\n        return self.lines[target_node.lineno - self.node.lineno + offset]"}
{"func_code_string": "def mark_def(self) -> int:\n        \"\"\"\n        Marks up this Function's definition lines (including decorators) into\n        the ``line_markers`` attribute.\n\n        Returns:\n            Number of lines found for the definition.\n\n        Note:\n            Does not spot the closing ``):`` of a function when it occurs on\n            its own line.\n\n        Note:\n            Can not use ``helpers.build_footprint()`` because function nodes\n            cover the whole function. In this case, just the def lines are\n            wanted with any decorators.\n        \"\"\"\n        first_line = get_first_token(self.node).start[0] - self.first_line_no  # Should usually be 0\n        try:\n            end_token = get_last_token(self.node.args.args[-1])\n        except IndexError:\n            # Fn has no args, so end of function is the fn def itself...\n            end_token = get_first_token(self.node)\n        last_line = end_token.end[0] - self.first_line_no\n        self.line_markers.update((first_line, last_line), LineType.func_def)\n        return last_line - first_line + 1"}
{"func_code_string": "def mark_bl(self) -> int:\n        \"\"\"\n        Mark unprocessed lines that have no content and no string nodes\n        covering them as blank line BL.\n\n        Returns:\n            Number of blank lines found with no stringy parent node.\n        \"\"\"\n        counter = 0\n        stringy_lines = find_stringy_lines(self.node, self.first_line_no)\n        for relative_line_number, line in enumerate(self.lines):\n            if relative_line_number not in stringy_lines and line.strip() == '':\n                counter += 1\n                self.line_markers[relative_line_number] = LineType.blank_line\n\n        return counter"}
{"func_code_string": "def enhance(self):\n        \"\"\" Function enhance\n        Enhance the object with new item or enhanced items\n        \"\"\"\n        self.update({'puppetclasses':\n                     SubDict(self.api, self.objName,\n                             self.payloadObj, self.key,\n                             SubItemPuppetClasses)})\n        self.update({'parameters':\n                     SubDict(self.api, self.objName,\n                             self.payloadObj, self.key,\n                             SubItemParameter)})\n        self.update({'interfaces':\n                     SubDict(self.api, self.objName,\n                             self.payloadObj, self.key,\n                             SubItemInterface)})\n        self.update({'smart_class_parameters':\n                    SubDict(self.api, self.objName,\n                            self.payloadObj, self.key,\n                            SubItemSmartClassParameter)})"}
{"func_code_string": "def getParamFromEnv(self, var, default=''):\n        \"\"\" Function getParamFromEnv\n        Search a parameter in the host environment\n\n        @param var: the var name\n        @param hostgroup: the hostgroup item linked to this host\n        @param default: default value\n        @return RETURN: the value\n        \"\"\"\n        if self.getParam(var):\n            return self.getParam(var)\n        if self.hostgroup:\n            if self.hostgroup.getParam(var):\n                return self.hostgroup.getParam(var)\n        if self.domain.getParam('password'):\n            return self.domain.getParam('password')\n        else:\n            return default"}
{"func_code_string": "def getUserData(self,\n                    hostgroup,\n                    domain,\n                    defaultPwd='',\n                    defaultSshKey='',\n                    proxyHostname='',\n                    tplFolder='metadata/templates/'):\n        \"\"\" Function getUserData\n        Generate a userdata script for metadata server from Foreman API\n\n        @param domain: the domain item linked to this host\n        @param hostgroup: the hostgroup item linked to this host\n        @param defaultPwd: the default password if no password is specified\n                           in the host>hostgroup>domain params\n        @param defaultSshKey: the default ssh key if no password is specified\n                              in the host>hostgroup>domain params\n        @param proxyHostname: hostname of the smartproxy\n        @param tplFolder: the templates folder\n        @return RETURN: the user data\n        \"\"\"\n        if 'user-data' in self.keys():\n            return self['user-data']\n        else:\n            self.hostgroup = hostgroup\n            self.domain = domain\n            if proxyHostname == '':\n                proxyHostname = 'foreman.' + domain['name']\n            password = self.getParamFromEnv('password', defaultPwd)\n            sshauthkeys = self.getParamFromEnv('global_sshkey', defaultSshKey)\n            with open(tplFolder+'puppet.conf', 'r') as puppet_file:\n                p = MyTemplate(puppet_file.read())\n                content = p.substitute(foremanHostname=proxyHostname)\n                enc_puppet_file = base64.b64encode(bytes(content, 'utf-8'))\n            with open(tplFolder+'cloud-init.tpl', 'r') as content_file:\n                s = MyTemplate(content_file.read())\n                if sshauthkeys:\n                    sshauthkeys = ' - '+sshauthkeys\n                self.userdata = s.substitute(\n                    password=password,\n                    fqdn=self['name'],\n                    sshauthkeys=sshauthkeys,\n                    foremanurlbuilt=\"http://{}/unattended/built\"\n                                    .format(proxyHostname),\n                    puppet_conf_content=enc_puppet_file.decode('utf-8'))\n                return self.userdata"}
{"func_code_string": "def register_payload(self, *payloads, flavour: ModuleType):\n        \"\"\"Queue one or more payload for execution after its runner is started\"\"\"\n        for payload in payloads:\n            self._logger.debug('registering payload %s (%s)', NameRepr(payload), NameRepr(flavour))\n            self.runners[flavour].register_payload(payload)"}
{"func_code_string": "def run_payload(self, payload, *, flavour: ModuleType):\n        \"\"\"Execute one payload after its runner is started and return its output\"\"\"\n        return self.runners[flavour].run_payload(payload)"}
{"func_code_string": "def run(self):\n        \"\"\"Run all runners, blocking until completion or error\"\"\"\n        self._logger.info('starting all runners')\n        try:\n            with self._lock:\n                assert not self.running.set(), 'cannot re-run: %s' % self\n                self.running.set()\n            thread_runner = self.runners[threading]\n            for runner in self.runners.values():\n                if runner is not thread_runner:\n                    thread_runner.register_payload(runner.run)\n            if threading.current_thread() == threading.main_thread():\n                asyncio_main_run(root_runner=thread_runner)\n            else:\n                thread_runner.run()\n        except Exception as err:\n            self._logger.exception('runner terminated: %s', err)\n            raise RuntimeError from err\n        finally:\n            self._stop_runners()\n            self._logger.info('stopped all runners')\n            self.running.clear()"}
{"func_code_string": "def formfield_for_dbfield(self, db_field, **kwargs):\n        \"\"\"\n        Hook for specifying the form Field instance for a given database Field\n        instance.\n\n        If kwargs are given, they're passed to the form Field's constructor.\n        \"\"\"\n        formfield = super().formfield_for_dbfield(db_field, **kwargs)\n        if db_field.name == 'image':\n            formfield.widget = ImageRelatedFieldWidgetWrapper(\n                ImageSelect(), db_field.rel, self.admin_site, can_add_related=True,\n                can_change_related=True,\n            )\n        return formfield"}
{"func_code_string": "def compare_schemas(one, two):\n    \"\"\"Compare two structures that represents JSON schemas.\n\n    For comparison you can't use normal comparison, because in JSON schema\n    lists DO NOT keep order (and Python lists do), so this must be taken into\n    account during comparison.\n\n    Note this wont check all configurations, only first one that seems to\n    match, which can lead to wrong results.\n\n    :param one: First schema to compare.\n    :param two: Second schema to compare.\n    :rtype: `bool`\n\n    \"\"\"\n    one = _normalize_string_type(one)\n    two = _normalize_string_type(two)\n\n    _assert_same_types(one, two)\n\n    if isinstance(one, list):\n        return _compare_lists(one, two)\n    elif isinstance(one, dict):\n        return _compare_dicts(one, two)\n    elif isinstance(one, SCALAR_TYPES):\n        return one == two\n    elif one is None:\n        return one is two\n    else:\n        raise RuntimeError('Not allowed type \"{type}\"'.format(\n            type=type(one).__name__))"}
{"func_code_string": "def is_ecma_regex(regex):\n    \"\"\"Check if given regex is of type ECMA 262 or not.\n\n    :rtype: bool\n\n    \"\"\"\n    parts = regex.split('/')\n\n    if len(parts) == 1:\n        return False\n\n    if len(parts) < 3:\n        raise ValueError('Given regex isn\\'t ECMA regex nor Python regex.')\n    parts.pop()\n    parts.append('')\n\n    raw_regex = '/'.join(parts)\n    if raw_regex.startswith('/') and raw_regex.endswith('/'):\n        return True\n    return False"}
{"func_code_string": "def convert_ecma_regex_to_python(value):\n    \"\"\"Convert ECMA 262 regex to Python tuple with regex and flags.\n\n    If given value is already Python regex it will be returned unchanged.\n\n    :param string value: ECMA regex.\n    :return: 2-tuple with `regex` and `flags`\n    :rtype: namedtuple\n\n    \"\"\"\n    if not is_ecma_regex(value):\n        return PythonRegex(value, [])\n\n    parts = value.split('/')\n    flags = parts.pop()\n\n    try:\n        result_flags = [ECMA_TO_PYTHON_FLAGS[f] for f in flags]\n    except KeyError:\n        raise ValueError('Wrong flags \"{}\".'.format(flags))\n\n    return PythonRegex('/'.join(parts[1:]), result_flags)"}
{"func_code_string": "def convert_python_regex_to_ecma(value, flags=[]):\n    \"\"\"Convert Python regex to ECMA 262 regex.\n\n    If given value is already ECMA regex it will be returned unchanged.\n\n    :param string value: Python regex.\n    :param list flags: List of flags (allowed flags: `re.I`, `re.M`)\n    :return: ECMA 262 regex\n    :rtype: str\n\n    \"\"\"\n    if is_ecma_regex(value):\n        return value\n\n    result_flags = [PYTHON_TO_ECMA_FLAGS[f] for f in flags]\n    result_flags = ''.join(result_flags)\n\n    return '/{value}/{flags}'.format(value=value, flags=result_flags)"}
{"func_code_string": "def populate(self, **values):\n        \"\"\"Populate values to fields. Skip non-existing.\"\"\"\n        values = values.copy()\n        fields = list(self.iterate_with_name())\n        for _, structure_name, field in fields:\n            if structure_name in values:\n                field.__set__(self, values.pop(structure_name))\n        for name, _, field in fields:\n            if name in values:\n                field.__set__(self, values.pop(name))"}
{"func_code_string": "def get_field(self, field_name):\n        \"\"\"Get field associated with given attribute.\"\"\"\n        for attr_name, field in self:\n            if field_name == attr_name:\n                return field\n\n        raise errors.FieldNotFound('Field not found', field_name)"}
{"func_code_string": "def validate(self):\n        \"\"\"Explicitly validate all the fields.\"\"\"\n        for name, field in self:\n            try:\n                field.validate_for_object(self)\n            except ValidationError as error:\n                raise ValidationError(\n                    \"Error for field '{name}'.\".format(name=name),\n                    error,\n                )"}
{"func_code_string": "def iterate_over_fields(cls):\n        \"\"\"Iterate through fields as `(attribute_name, field_instance)`.\"\"\"\n        for attr in dir(cls):\n            clsattr = getattr(cls, attr)\n            if isinstance(clsattr, BaseField):\n                yield attr, clsattr"}
{"func_code_string": "def iterate_with_name(cls):\n        \"\"\"Iterate over fields, but also give `structure_name`.\n\n        Format is `(attribute_name, structue_name, field_instance)`.\n        Structure name is name under which value is seen in structure and\n        schema (in primitives) and only there.\n        \"\"\"\n        for attr_name, field in cls.iterate_over_fields():\n            structure_name = field.structue_name(attr_name)\n            yield attr_name, structure_name, field"}
{"func_code_string": "def parse_value(self, value):\n        \"\"\"Cast value to `int`, e.g. from string or long\"\"\"\n        parsed = super(IntField, self).parse_value(value)\n        if parsed is None:\n            return parsed\n        return int(parsed)"}
{"func_code_string": "def parse_value(self, value):\n        \"\"\"Cast value to `bool`.\"\"\"\n        parsed = super(BoolField, self).parse_value(value)\n        return bool(parsed) if parsed is not None else None"}
{"func_code_string": "def parse_value(self, values):\n        \"\"\"Cast value to proper collection.\"\"\"\n        result = self.get_default_value()\n\n        if not values:\n            return result\n\n        if not isinstance(values, list):\n            return values\n\n        return [self._cast_value(value) for value in values]"}
{"func_code_string": "def parse_value(self, value):\n        \"\"\"Parse value to proper model type.\"\"\"\n        if not isinstance(value, dict):\n            return value\n\n        embed_type = self._get_embed_type()\n        return embed_type(**value)"}
{"func_code_string": "def to_struct(self, value):\n        \"\"\"Cast `time` object to string.\"\"\"\n        if self.str_format:\n            return value.strftime(self.str_format)\n        return value.isoformat()"}
{"func_code_string": "def parse_value(self, value):\n        \"\"\"Parse string into instance of `time`.\"\"\"\n        if value is None:\n            return value\n        if isinstance(value, datetime.time):\n            return value\n        return parse(value).timetz()"}
{"func_code_string": "def to_struct(self, value):\n        \"\"\"Cast `date` object to string.\"\"\"\n        if self.str_format:\n            return value.strftime(self.str_format)\n        return value.strftime(self.default_format)"}
{"func_code_string": "def parse_value(self, value):\n        \"\"\"Parse string into instance of `datetime`.\"\"\"\n        if isinstance(value, datetime.datetime):\n            return value\n        if value:\n            return parse(value)\n        else:\n            return None"}
{"func_code_string": "def validate(self, value):\n        \"\"\"Validate value.\"\"\"\n        if self.exclusive:\n            if value <= self.minimum_value:\n                tpl = \"'{value}' is lower or equal than minimum ('{min}').\"\n                raise ValidationError(\n                    tpl.format(value=value, min=self.minimum_value))\n        else:\n            if value < self.minimum_value:\n                raise ValidationError(\n                    \"'{value}' is lower than minimum ('{min}').\".format(\n                        value=value, min=self.minimum_value))"}
{"func_code_string": "def modify_schema(self, field_schema):\n        \"\"\"Modify field schema.\"\"\"\n        field_schema['minimum'] = self.minimum_value\n        if self.exclusive:\n            field_schema['exclusiveMinimum'] = True"}
{"func_code_string": "def validate(self, value):\n        \"\"\"Validate value.\"\"\"\n        if self.exclusive:\n            if value >= self.maximum_value:\n                tpl = \"'{val}' is bigger or equal than maximum ('{max}').\"\n                raise ValidationError(\n                    tpl.format(val=value, max=self.maximum_value))\n        else:\n            if value > self.maximum_value:\n                raise ValidationError(\n                    \"'{value}' is bigger than maximum ('{max}').\".format(\n                        value=value, max=self.maximum_value))"}
{"func_code_string": "def modify_schema(self, field_schema):\n        \"\"\"Modify field schema.\"\"\"\n        field_schema['maximum'] = self.maximum_value\n        if self.exclusive:\n            field_schema['exclusiveMaximum'] = True"}
{"func_code_string": "def validate(self, value):\n        \"\"\"Validate value.\"\"\"\n        flags = self._calculate_flags()\n\n        try:\n            result = re.search(self.pattern, value, flags)\n        except TypeError as te:\n            raise ValidationError(*te.args)\n\n        if not result:\n            raise ValidationError(\n                'Value \"{value}\" did not match pattern \"{pattern}\".'.format(\n                    value=value, pattern=self.pattern\n                ))"}
{"func_code_string": "def modify_schema(self, field_schema):\n        \"\"\"Modify field schema.\"\"\"\n        field_schema['pattern'] = utilities.convert_python_regex_to_ecma(\n            self.pattern, self.flags)"}
{"func_code_string": "def validate(self, value):\n        \"\"\"Validate value.\"\"\"\n        len_ = len(value)\n\n        if self.minimum_value is not None and len_ < self.minimum_value:\n            tpl = \"Value '{val}' length is lower than allowed minimum '{min}'.\"\n            raise ValidationError(tpl.format(\n                val=value, min=self.minimum_value\n            ))\n\n        if self.maximum_value is not None and len_ > self.maximum_value:\n            raise ValidationError(\n                \"Value '{val}' length is bigger than \"\n                \"allowed maximum '{max}'.\".format(\n                    val=value,\n                    max=self.maximum_value,\n                ))"}
{"func_code_string": "def modify_schema(self, field_schema):\n        \"\"\"Modify field schema.\"\"\"\n        if self.minimum_value:\n            field_schema['minLength'] = self.minimum_value\n\n        if self.maximum_value:\n            field_schema['maxLength'] = self.maximum_value"}
{"func_code_string": "def to_struct(model):\n    \"\"\"Cast instance of model to python structure.\n\n    :param model: Model to be casted.\n    :rtype: ``dict``\n\n    \"\"\"\n    model.validate()\n\n    resp = {}\n    for _, name, field in model.iterate_with_name():\n        value = field.__get__(model)\n        if value is None:\n            continue\n\n        value = field.to_struct(value)\n        resp[name] = value\n    return resp"}
{"func_code_string": "def run_from_argv(self, argv):\n        \"\"\"\n        Set up any environment changes requested (e.g., Python path\n        and Django settings), then run this command. If the\n        command raises a ``CommandError``, intercept it and print it sensibly\n        to stderr.\n        \"\"\"\n        parser = self.create_parser(argv[0], argv[1])\n        args = parser.parse_args(argv[2:])\n        handle_default_options(args)\n        try:\n            self.execute(args)\n        except Exception as e:\n            # self.stderr is not guaranteed to be set here\n            try:\n                fallback_stderr = OutputWrapper(sys.stderr, self.style.ERROR)\n            except:\n                fallback_stderr = self.stdout\n            stderr = getattr(self, 'stderr', fallback_stderr)\n            if args.traceback:\n                stderr.write(traceback.format_exc())\n            else:\n                stderr.write('%s: %s' % (e.__class__.__name__, e))\n            sys.exit(1)"}
{"func_code_string": "def create_parser(self, prog_name, subcommand):\n        \"\"\"\n        Create and return the ``OptionParser`` which will be used to\n        parse the arguments to this command.\n\n        \"\"\"\n        parser = argparse.ArgumentParser(prog='%s %s' % (prog_name, subcommand), description=self.help)\n\n        parser.add_argument('-v', '--verbosity', action='store', default=1, type=int, choices=range(4),\n            help='Verbosity level; 0=minimal output, 1=normal output, 2=verbose output, 3=very verbose output'),\n        parser.add_argument('--settings',\n            help='The Python path to a settings module, e.g. \"myproject.settings.main\". '\n            'If this isn\\'t provided, the DJANGO_SETTINGS_MODULE environment variable will be used.'),\n        parser.add_argument('--pythonpath',\n            help='A directory to add to the Python path, e.g. \"/home/djangoprojects/myproject\".'),\n        parser.add_argument('--traceback', action='store_true', help='Print traceback on exception'),\n\n        subparsers = parser.add_subparsers(description='JavaScript command to execute')\n\n        for subparser in self.subparsers:\n            subparser(self, subparsers)\n\n        return parser"}
{"func_code_string": "def rst(filename):\n    '''\n    Load rst file and sanitize it for PyPI.\n    Remove unsupported github tags:\n     - code-block directive\n     - travis ci build badge\n    '''\n    content = codecs.open(filename, encoding='utf-8').read()\n    for regex, replacement in PYPI_RST_FILTERS:\n        content = re.sub(regex, replacement, content)\n    return content"}
{"func_code_string": "def javascript(filename, type='text/javascript'):\n    '''A simple shortcut to render a ``script`` tag to a static javascript file'''\n    if '?' in filename and len(filename.split('?')) is 2:\n        filename, params = filename.split('?')\n        return '<script type=\"%s\" src=\"%s?%s\"></script>' % (type, staticfiles_storage.url(filename), params)\n    else:\n        return '<script type=\"%s\" src=\"%s\"></script>' % (type, staticfiles_storage.url(filename))"}
{"func_code_string": "def jquery_js(version=None, migrate=False):\n    '''A shortcut to render a ``script`` tag for the packaged jQuery'''\n    version = version or settings.JQUERY_VERSION\n    suffix = '.min' if not settings.DEBUG else ''\n    libs = [js_lib('jquery-%s%s.js' % (version, suffix))]\n    if _boolean(migrate):\n        libs.append(js_lib('jquery-migrate-%s%s.js' % (JQUERY_MIGRATE_VERSION, suffix)))\n    return '\\n'.join(libs)"}
{"func_code_string": "def django_js(context, jquery=True, i18n=True, csrf=True, init=True):\n    '''Include Django.js javascript library in the page'''\n    return {\n        'js': {\n            'minified': not settings.DEBUG,\n            'jquery': _boolean(jquery),\n            'i18n': _boolean(i18n),\n            'csrf': _boolean(csrf),\n            'init': _boolean(init),\n        }\n    }"}
{"func_code_string": "def django_js_init(context, jquery=False, i18n=True, csrf=True, init=True):\n    '''Include Django.js javascript library initialization in the page'''\n    return {\n        'js': {\n            'jquery': _boolean(jquery),\n            'i18n': _boolean(i18n),\n            'csrf': _boolean(csrf),\n            'init': _boolean(init),\n        }\n    }"}
{"func_code_string": "def as_dict(self):\n        '''\n        Serialize the context as a dictionnary from a given request.\n        '''\n        data = {}\n        if settings.JS_CONTEXT_ENABLED:\n            for context in RequestContext(self.request):\n                for key, value in six.iteritems(context):\n                    if settings.JS_CONTEXT and key not in settings.JS_CONTEXT:\n                        continue\n                    if settings.JS_CONTEXT_EXCLUDE and key in settings.JS_CONTEXT_EXCLUDE:\n                        continue\n                    handler_name = 'process_%s' % key\n                    if hasattr(self, handler_name):\n                        handler = getattr(self, handler_name)\n                        data[key] = handler(value, data)\n                    elif isinstance(value, SERIALIZABLE_TYPES):\n                        data[key] = value\n        if settings.JS_USER_ENABLED:\n            self.handle_user(data)\n        return data"}
{"func_code_string": "def process_LANGUAGE_CODE(self, language_code, data):\n        '''\n        Fix language code when set to non included default `en`\n        and add the extra variables ``LANGUAGE_NAME`` and ``LANGUAGE_NAME_LOCAL``.\n        '''\n        # Dirty hack to fix non included default\n        language_code = 'en-us' if language_code == 'en' else language_code\n        language = translation.get_language_info('en' if language_code == 'en-us' else language_code)\n        if not settings.JS_CONTEXT or 'LANGUAGE_NAME' in settings.JS_CONTEXT \\\n            or (settings.JS_CONTEXT_EXCLUDE and 'LANGUAGE_NAME' in settings.JS_CONTEXT_EXCLUDE):\n            data['LANGUAGE_NAME'] = language['name']\n        if not settings.JS_CONTEXT or 'LANGUAGE_NAME_LOCAL' in settings.JS_CONTEXT \\\n            or (settings.JS_CONTEXT_EXCLUDE and 'LANGUAGE_NAME_LOCAL' in settings.JS_CONTEXT_EXCLUDE):\n            data['LANGUAGE_NAME_LOCAL'] = language['name_local']\n        return language_code"}
{"func_code_string": "def handle_user(self, data):\n        '''\n        Insert user informations in data\n\n        Override it to add extra user attributes.\n        '''\n        # Default to unauthenticated anonymous user\n        data['user'] = {\n            'username': '',\n            'is_authenticated': False,\n            'is_staff': False,\n            'is_superuser': False,\n            'permissions': tuple(),\n        }\n        if 'django.contrib.sessions.middleware.SessionMiddleware' in settings.MIDDLEWARE_CLASSES:\n            user = self.request.user\n            data['user']['is_authenticated'] = user.is_authenticated()\n            if hasattr(user, 'username'):\n                data['user']['username'] = user.username\n            elif hasattr(user, 'get_username'):\n                data['user']['username'] = user.get_username()\n            if hasattr(user, 'is_staff'):\n                data['user']['is_staff'] = user.is_staff\n            if hasattr(user, 'is_superuser'):\n                data['user']['is_superuser'] = user.is_superuser\n            if hasattr(user, 'get_all_permissions'):\n                data['user']['permissions'] = tuple(user.get_all_permissions())"}
{"func_code_string": "def class_from_string(name):\n    '''\n    Get a python class object from its name\n    '''\n    module_name, class_name = name.rsplit('.', 1)\n    __import__(module_name)\n    module = sys.modules[module_name]\n    return getattr(module, class_name)"}
{"func_code_string": "def glob(cls, files=None):\n        '''\n        Glob a pattern or a list of pattern static storage relative(s).\n        '''\n        files = files or []\n        if isinstance(files, str):\n            files = os.path.normpath(files)\n            matches = lambda path: matches_patterns(path, [files])\n            return [path for path in cls.get_static_files() if matches(path)]\n        elif isinstance(files, (list, tuple)):\n            all_files = cls.get_static_files()\n            files = [os.path.normpath(f) for f in files]\n            sorted_result = []\n            for pattern in files:\n                sorted_result.extend([f for f in all_files if matches_patterns(f, [pattern])])\n            return sorted_result"}
{"func_code_string": "def execute(self, command):\n        '''\n        Execute a subprocess yielding output lines\n        '''\n        process = Popen(command, stdout=PIPE, stderr=STDOUT, universal_newlines=True)\n        while True:\n            if process.poll() is not None:\n                self.returncode = process.returncode  # pylint: disable=W0201\n                break\n            yield process.stdout.readline()"}
{"func_code_string": "def phantomjs(self, *args, **kwargs):\n        '''\n        Execute PhantomJS by giving ``args`` as command line arguments.\n\n        If test are run in verbose mode (``-v/--verbosity`` = 2), it output:\n          - the title as header (with separators before and after)\n          - modules and test names\n          - assertions results (with ``django.utils.termcolors`` support)\n\n        In case of error, a JsTestException is raised to give details about javascript errors.\n        '''\n        separator = '=' * LINE_SIZE\n        title = kwargs['title'] if 'title' in kwargs else 'phantomjs output'\n        nb_spaces = (LINE_SIZE - len(title)) // 2\n\n        if VERBOSE:\n            print('')\n            print(separator)\n            print(' ' * nb_spaces + title)\n            print(separator)\n            sys.stdout.flush()\n\n        with NamedTemporaryFile(delete=True) as cookies_file:\n            cmd = ('phantomjs', '--cookies-file=%s' % cookies_file.name) + args\n            if self.timeout:\n                cmd += (str(self.timeout),)\n            parser = TapParser(debug=VERBOSITY > 2)\n            output = self.execute(cmd)\n\n            for item in parser.parse(output):\n                if VERBOSE:\n                    print(item.display())\n                    sys.stdout.flush()\n\n        if VERBOSE:\n            print(separator)\n            sys.stdout.flush()\n\n        failures = parser.suites.get_all_failures()\n        if failures:\n            raise JsTestException('Failed javascript assertions', failures)\n        if self.returncode > 0:\n            raise JsTestException('PhantomJS return with non-zero exit code (%s)' % self.returncode)"}
{"func_code_string": "def run_suite(self):\n        '''\n        Run a phantomjs test suite.\n\n         - ``phantomjs_runner`` is mandatory.\n         - Either ``url`` or ``url_name`` needs to be defined.\n        '''\n        if not self.phantomjs_runner:\n            raise JsTestException('phantomjs_runner need to be defined')\n\n        url = self.get_url()\n\n        self.phantomjs(self.phantomjs_runner, url, title=self.title)\n        self.cleanup()"}
{"func_code_string": "def write(self, rows, keyed=False):\n        \"\"\"Write rows/keyed_rows to table\n        \"\"\"\n        for row in rows:\n            keyed_row = row\n            if not keyed:\n                keyed_row = dict(zip(self.__schema.field_names, row))\n            keyed_row = self.__convert_row(keyed_row)\n            if self.__check_existing(keyed_row):\n                for wr in self.__insert():\n                    yield wr\n                ret = self.__update(keyed_row)\n                if ret is not None:\n                    yield WrittenRow(keyed_row, True, ret if self.__autoincrement else None)\n                    continue\n            self.__buffer.append(keyed_row)\n            if len(self.__buffer) > BUFFER_SIZE:\n                for wr in self.__insert():\n                    yield wr\n        for wr in self.__insert():\n            yield wr"}
{"func_code_string": "def __prepare_bloom(self):\n        \"\"\"Prepare bloom for existing checks\n        \"\"\"\n        self.__bloom = pybloom_live.ScalableBloomFilter()\n        columns = [getattr(self.__table.c, key) for key in self.__update_keys]\n        keys = select(columns).execution_options(stream_results=True).execute()\n        for key in keys:\n            self.__bloom.add(tuple(key))"}
{"func_code_string": "def __insert(self):\n        \"\"\"Insert rows to table\n        \"\"\"\n        if len(self.__buffer) > 0:\n            # Insert data\n            statement = self.__table.insert()\n            if self.__autoincrement:\n                statement = statement.returning(\n                    getattr(self.__table.c, self.__autoincrement))\n                statement = statement.values(self.__buffer)\n                res = statement.execute()\n                for id, in res:\n                    row = self.__buffer.pop(0)\n                    yield WrittenRow(row, False, id)\n            else:\n                statement.execute(self.__buffer)\n                for row in self.__buffer:\n                    yield WrittenRow(row, False, None)\n            # Clean memory\n            self.__buffer = []"}
{"func_code_string": "def __update(self, row):\n        \"\"\"Update rows in table\n        \"\"\"\n        expr = self.__table.update().values(row)\n        for key in self.__update_keys:\n            expr = expr.where(getattr(self.__table.c, key) == row[key])\n        if self.__autoincrement:\n            expr = expr.returning(getattr(self.__table.c, self.__autoincrement))\n        res = expr.execute()\n        if res.rowcount > 0:\n            if self.__autoincrement:\n                first = next(iter(res))\n                last_row_id = first[0]\n                return last_row_id\n            return 0\n        return None"}
{"func_code_string": "def __check_existing(self, row):\n        \"\"\"Check if row exists in table\n        \"\"\"\n        if self.__update_keys is not None:\n            key = tuple(row[key] for key in self.__update_keys)\n            if key in self.__bloom:\n                return True\n            self.__bloom.add(key)\n            return False\n        return False"}
{"func_code_string": "def buckets(self):\n        \"\"\"https://github.com/frictionlessdata/tableschema-sql-py#storage\n        \"\"\"\n        buckets = []\n        for table in self.__metadata.sorted_tables:\n            bucket = self.__mapper.restore_bucket(table.name)\n            if bucket is not None:\n                buckets.append(bucket)\n        return buckets"}
{"func_code_string": "def create(self, bucket, descriptor, force=False, indexes_fields=None):\n        \"\"\"https://github.com/frictionlessdata/tableschema-sql-py#storage\n        \"\"\"\n\n        # Make lists\n        buckets = bucket\n        if isinstance(bucket, six.string_types):\n            buckets = [bucket]\n        descriptors = descriptor\n        if isinstance(descriptor, dict):\n            descriptors = [descriptor]\n        if indexes_fields is None or len(indexes_fields) == 0:\n            indexes_fields = [()] * len(descriptors)\n        elif type(indexes_fields[0][0]) not in {list, tuple}:\n            indexes_fields = [indexes_fields]\n\n        # Check dimensions\n        if not (len(buckets) == len(descriptors) == len(indexes_fields)):\n            raise tableschema.exceptions.StorageError('Wrong argument dimensions')\n\n        # Check buckets for existence\n        for bucket in reversed(self.buckets):\n            if bucket in buckets:\n                if not force:\n                    message = 'Bucket \"%s\" already exists.' % bucket\n                    raise tableschema.exceptions.StorageError(message)\n                self.delete(bucket)\n\n        # Define buckets\n        for bucket, descriptor, index_fields in zip(buckets, descriptors, indexes_fields):\n            tableschema.validate(descriptor)\n            table_name = self.__mapper.convert_bucket(bucket)\n            columns, constraints, indexes, fallbacks, table_comment = self.__mapper \\\n                .convert_descriptor(bucket, descriptor, index_fields, self.__autoincrement)\n            Table(table_name, self.__metadata, *(columns + constraints + indexes),\n                  comment=table_comment)\n            self.__descriptors[bucket] = descriptor\n            self.__fallbacks[bucket] = fallbacks\n\n        # Create tables, update metadata\n        self.__metadata.create_all()"}
{"func_code_string": "def delete(self, bucket=None, ignore=False):\n        \"\"\"https://github.com/frictionlessdata/tableschema-sql-py#storage\n        \"\"\"\n\n        # Make lists\n        buckets = bucket\n        if isinstance(bucket, six.string_types):\n            buckets = [bucket]\n        elif bucket is None:\n            buckets = reversed(self.buckets)\n\n        # Iterate\n        tables = []\n        for bucket in buckets:\n\n            # Check existent\n            if bucket not in self.buckets:\n                if not ignore:\n                    message = 'Bucket \"%s\" doesn\\'t exist.' % bucket\n                    raise tableschema.exceptions.StorageError(message)\n                return\n\n            # Remove from buckets\n            if bucket in self.__descriptors:\n                del self.__descriptors[bucket]\n\n            # Add table to tables\n            table = self.__get_table(bucket)\n            tables.append(table)\n\n        # Drop tables, update metadata\n        self.__metadata.drop_all(tables=tables)\n        self.__metadata.clear()\n        self.__reflect()"}
{"func_code_string": "def describe(self, bucket, descriptor=None):\n        \"\"\"https://github.com/frictionlessdata/tableschema-sql-py#storage\n        \"\"\"\n\n        # Set descriptor\n        if descriptor is not None:\n            self.__descriptors[bucket] = descriptor\n\n        # Get descriptor\n        else:\n            descriptor = self.__descriptors.get(bucket)\n            if descriptor is None:\n                table = self.__get_table(bucket)\n                descriptor = self.__mapper.restore_descriptor(\n                    table.name, table.columns, table.constraints, self.__autoincrement)\n\n        return descriptor"}
{"func_code_string": "def iter(self, bucket):\n        \"\"\"https://github.com/frictionlessdata/tableschema-sql-py#storage\n        \"\"\"\n\n        # Get table and fallbacks\n        table = self.__get_table(bucket)\n        schema = tableschema.Schema(self.describe(bucket))\n\n        # Open and close transaction\n        with self.__connection.begin():\n            # Streaming could be not working for some backends:\n            # http://docs.sqlalchemy.org/en/latest/core/connections.html\n            select = table.select().execution_options(stream_results=True)\n            result = select.execute()\n            for row in result:\n                row = self.__mapper.restore_row(row, schema=schema)\n                yield row"}
{"func_code_string": "def write(self, bucket, rows, keyed=False, as_generator=False, update_keys=None):\n        \"\"\"https://github.com/frictionlessdata/tableschema-sql-py#storage\n        \"\"\"\n\n        # Check update keys\n        if update_keys is not None and len(update_keys) == 0:\n            message = 'Argument \"update_keys\" cannot be an empty list'\n            raise tableschema.exceptions.StorageError(message)\n\n        # Get table and description\n        table = self.__get_table(bucket)\n        schema = tableschema.Schema(self.describe(bucket))\n        fallbacks = self.__fallbacks.get(bucket, [])\n\n        # Write rows to table\n        convert_row = partial(self.__mapper.convert_row, schema=schema, fallbacks=fallbacks)\n        writer = Writer(table, schema, update_keys, self.__autoincrement, convert_row)\n        with self.__connection.begin():\n            gen = writer.write(rows, keyed=keyed)\n            if as_generator:\n                return gen\n            collections.deque(gen, maxlen=0)"}
{"func_code_string": "def __get_table(self, bucket):\n        \"\"\"Get table by bucket\n        \"\"\"\n        table_name = self.__mapper.convert_bucket(bucket)\n        if self.__dbschema:\n            table_name = '.'.join((self.__dbschema, table_name))\n        return self.__metadata.tables[table_name]"}
{"func_code_string": "def __reflect(self):\n        \"\"\"Reflect metadata\n        \"\"\"\n\n        def only(name, _):\n            return self.__only(name) and self.__mapper.restore_bucket(name) is not None\n\n        self.__metadata.reflect(only=only)"}
{"func_code_string": "def _get_field_comment(field, separator=' - '):\n    \"\"\"\n    Create SQL comment from field's title and description\n\n    :param field: tableschema-py Field, with optional 'title' and 'description' values\n    :param separator:\n    :return:\n\n    >>> _get_field_comment(tableschema.Field({'title': 'my_title', 'description': 'my_desc'}))\n    'my_title - my_desc'\n    >>> _get_field_comment(tableschema.Field({'title': 'my_title', 'description': None}))\n    'my_title'\n    >>> _get_field_comment(tableschema.Field({'title': '', 'description': 'my_description'}))\n    'my_description'\n    >>> _get_field_comment(tableschema.Field({}))\n    ''\n    \"\"\"\n    title = field.descriptor.get('title') or ''\n    description = field.descriptor.get('description') or ''\n    return _get_comment(description, title, separator)"}
{"func_code_string": "def convert_descriptor(self, bucket, descriptor, index_fields=[], autoincrement=None):\n        \"\"\"Convert descriptor to SQL\n        \"\"\"\n\n        # Prepare\n        columns = []\n        indexes = []\n        fallbacks = []\n        constraints = []\n        column_mapping = {}\n        table_name = self.convert_bucket(bucket)\n        table_comment = _get_comment(descriptor.get('title', ''), descriptor.get('description', ''))\n        schema = tableschema.Schema(descriptor)\n\n        # Autoincrement\n        if autoincrement is not None:\n            columns.append(sa.Column(\n                autoincrement, sa.Integer, autoincrement=True, nullable=False))\n\n        # Fields\n        for field in schema.fields:\n            column_type = self.convert_type(field.type)\n            if not column_type:\n                column_type = sa.Text\n                fallbacks.append(field.name)\n            nullable = not field.required\n            table_comment = _get_field_comment(field)\n            unique = field.constraints.get('unique', False)\n            column = sa.Column(field.name, column_type, nullable=nullable, comment=table_comment,\n                               unique=unique)\n            columns.append(column)\n            column_mapping[field.name] = column\n\n        # Primary key\n        pk = descriptor.get('primaryKey', None)\n        if pk is not None:\n            if isinstance(pk, six.string_types):\n                pk = [pk]\n        if autoincrement is not None:\n            if pk is not None:\n                pk = [autoincrement] + pk\n            else:\n                pk = [autoincrement]\n        if pk is not None:\n            constraint = sa.PrimaryKeyConstraint(*pk)\n            constraints.append(constraint)\n\n        # Foreign keys\n        if self.__dialect == 'postgresql':\n            fks = descriptor.get('foreignKeys', [])\n            for fk in fks:\n                fields = fk['fields']\n                resource = fk['reference']['resource']\n                foreign_fields = fk['reference']['fields']\n                if isinstance(fields, six.string_types):\n                    fields = [fields]\n                if resource != '':\n                    table_name = self.convert_bucket(resource)\n                if isinstance(foreign_fields, six.string_types):\n                    foreign_fields = [foreign_fields]\n                composer = lambda field: '.'.join([table_name, field])\n                foreign_fields = list(map(composer, foreign_fields))\n                constraint = sa.ForeignKeyConstraint(fields, foreign_fields)\n                constraints.append(constraint)\n\n        # Indexes\n        if self.__dialect == 'postgresql':\n            for index, index_definition in enumerate(index_fields):\n                name = table_name + '_ix%03d' % index\n                index_columns = [column_mapping[field] for field in index_definition]\n                indexes.append(sa.Index(name, *index_columns))\n\n        return columns, constraints, indexes, fallbacks, table_comment"}
{"func_code_string": "def convert_row(self, keyed_row, schema, fallbacks):\n        \"\"\"Convert row to SQL\n        \"\"\"\n        for key, value in list(keyed_row.items()):\n            field = schema.get_field(key)\n            if not field:\n                del keyed_row[key]\n            if key in fallbacks:\n                value = _uncast_value(value, field=field)\n            else:\n                value = field.cast_value(value)\n            keyed_row[key] = value\n        return keyed_row"}
{"func_code_string": "def convert_type(self, type):\n        \"\"\"Convert type to SQL\n        \"\"\"\n\n        # Default dialect\n        mapping = {\n            'any': sa.Text,\n            'array': None,\n            'boolean': sa.Boolean,\n            'date': sa.Date,\n            'datetime': sa.DateTime,\n            'duration': None,\n            'geojson': None,\n            'geopoint': None,\n            'integer': sa.Integer,\n            'number': sa.Float,\n            'object': None,\n            'string': sa.Text,\n            'time': sa.Time,\n            'year': sa.Integer,\n            'yearmonth': None,\n        }\n\n        # Postgresql dialect\n        if self.__dialect == 'postgresql':\n            mapping.update({\n                'array': JSONB,\n                'geojson': JSONB,\n                'number': sa.Numeric,\n                'object': JSONB,\n            })\n\n        # Not supported type\n        if type not in mapping:\n            message = 'Field type \"%s\" is not supported'\n            raise tableschema.exceptions.StorageError(message % type)\n\n        return mapping[type]"}
{"func_code_string": "def restore_bucket(self, table_name):\n        \"\"\"Restore bucket from SQL\n        \"\"\"\n        if table_name.startswith(self.__prefix):\n            return table_name.replace(self.__prefix, '', 1)\n        return None"}
{"func_code_string": "def restore_descriptor(self, table_name, columns, constraints, autoincrement_column=None):\n        \"\"\"Restore descriptor from SQL\n        \"\"\"\n\n        # Fields\n        fields = []\n        for column in columns:\n            if column.name == autoincrement_column:\n                continue\n            field_type = self.restore_type(column.type)\n            field = {'name': column.name, 'type': field_type}\n            if not column.nullable:\n                field['constraints'] = {'required': True}\n            fields.append(field)\n\n        # Primary key\n        pk = []\n        for constraint in constraints:\n            if isinstance(constraint, sa.PrimaryKeyConstraint):\n                for column in constraint.columns:\n                    if column.name == autoincrement_column:\n                        continue\n                    pk.append(column.name)\n\n        # Foreign keys\n        fks = []\n        if self.__dialect == 'postgresql':\n            for constraint in constraints:\n                if isinstance(constraint, sa.ForeignKeyConstraint):\n                    resource = ''\n                    own_fields = []\n                    foreign_fields = []\n                    for element in constraint.elements:\n                        own_fields.append(element.parent.name)\n                        if element.column.table.name != table_name:\n                            resource = self.restore_bucket(element.column.table.name)\n                        foreign_fields.append(element.column.name)\n                    if len(own_fields) == len(foreign_fields) == 1:\n                        own_fields = own_fields.pop()\n                        foreign_fields = foreign_fields.pop()\n                    fks.append({\n                        'fields': own_fields,\n                        'reference': {'resource': resource, 'fields': foreign_fields},\n                    })\n\n        # Desscriptor\n        descriptor = {}\n        descriptor['fields'] = fields\n        if len(pk) > 0:\n            if len(pk) == 1:\n                pk = pk.pop()\n            descriptor['primaryKey'] = pk\n        if len(fks) > 0:\n            descriptor['foreignKeys'] = fks\n\n        return descriptor"}
{"func_code_string": "def restore_row(self, row, schema):\n        \"\"\"Restore row from SQL\n        \"\"\"\n        row = list(row)\n        for index, field in enumerate(schema.fields):\n            if self.__dialect == 'postgresql':\n                if field.type in ['array', 'object']:\n                    continue\n            row[index] = field.cast_value(row[index])\n        return row"}
{"func_code_string": "def restore_type(self, type):\n        \"\"\"Restore type from SQL\n        \"\"\"\n\n        # All dialects\n        mapping = {\n            ARRAY: 'array',\n            sa.Boolean: 'boolean',\n            sa.Date: 'date',\n            sa.DateTime: 'datetime',\n            sa.Float: 'number',\n            sa.Integer: 'integer',\n            JSONB: 'object',\n            JSON: 'object',\n            sa.Numeric: 'number',\n            sa.Text: 'string',\n            sa.Time: 'time',\n            sa.VARCHAR: 'string',\n            UUID: 'string',\n        }\n\n        # Get field type\n        field_type = None\n        for key, value in mapping.items():\n            if isinstance(type, key):\n                field_type = value\n\n        # Not supported\n        if field_type is None:\n            message = 'Type \"%s\" is not supported'\n            raise tableschema.exceptions.StorageError(message % type)\n\n        return field_type"}
{"func_code_string": "def open_hierarchy(self, path, relative_to_object_id, object_id, create_file_type=0):\n        \"\"\"\n          CreateFileType\n          0 - Creates no new object.\n          1 - Creates a notebook with the specified name at the specified location.\n          2 - Creates a section group with the specified name at the specified location.\n          3 - Creates a section with the specified name at the specified location.\n        \"\"\"\n        try:\n            return(self.process.OpenHierarchy(path, relative_to_object_id, \"\", create_file_type))\n        except Exception as e: \n            print(e)\n            print(\"Could not Open Hierarchy\")"}
{"func_code_string": "def create_new_page (self, section_id, new_page_style=0):\n        \"\"\"\n          NewPageStyle\n          0 - Create a Page that has Default Page Style\n          1 - Create a blank page with no title\n          2 - Createa blank page that has no title\n        \"\"\"\n        try:\n            self.process.CreateNewPage(section_id, \"\", new_page_style)\n        except Exception as e: \n            print(e)\n            print(\"Unable to create the page\")"}
{"func_code_string": "def get_page_content(self, page_id, page_info=0):\n        \"\"\"\n          PageInfo\n          0 - Returns only basic page content, without selection markup and binary data objects. This is the standard value to pass.\n          1 - Returns page content with no selection markup, but with all binary data.\n          2 - Returns page content with selection markup, but no binary data.\n          3 - Returns page content with selection markup and all binary data.\n        \"\"\"\n        try:\n            return(self.process.GetPageContent(page_id, \"\", page_info))\n        except Exception as e: \n            print(e)\n            print(\"Could not get Page Content\")"}
{"func_code_string": "def publish(self, hierarchy_id, target_file_path, publish_format, clsid_of_exporter=\"\"):\n        \"\"\"\n         PublishFormat\n          0 - Published page is in .one format.\n          1 - Published page is in .onea format.\n          2 - Published page is in .mht format.\n          3 - Published page is in .pdf format.\n          4 - Published page is in .xps format.\n          5 - Published page is in .doc or .docx format.\n          6 - Published page is in enhanced metafile (.emf) format.\n        \"\"\"\n        try:\n            self.process.Publish(hierarchy_id, target_file_path, publish_format, clsid_of_exporter)\n        except Exception as e: \n            print(e)\n            print(\"Could not Publish\")"}
{"func_code_string": "def get_special_location(self, special_location=0):\n        \"\"\"\n          SpecialLocation\n          0 - Gets the path to the Backup Folders folder location.\n          1 - Gets the path to the Unfiled Notes folder location.\n          2 - Gets the path to the Default Notebook folder location.\n        \"\"\"\n        try:\n            return(self.process.GetSpecialLocation(special_location))\n        except Exception as e: \n            print(e)\n            print(\"Could not retreive special location\")"}
{"func_code_string": "def memory():\n    \"\"\"Determine memory specifications of the machine.\n\n    Returns\n    -------\n    mem_info : dictonary\n        Holds the current values for the total, free and used memory of the system.\n    \"\"\"\n\n    mem_info = dict()\n\n    for k, v in psutil.virtual_memory()._asdict().items():\n           mem_info[k] = int(v)\n           \n    return mem_info"}
{"func_code_string": "def get_chunk_size(N, n):\n    \"\"\"Given a two-dimensional array with a dimension of size 'N', \n        determine the number of rows or columns that can fit into memory.\n\n    Parameters\n    ----------\n    N : int\n        The size of one of the dimensions of a two-dimensional array.  \n\n    n : int\n        The number of arrays of size 'N' times 'chunk_size' that can fit in memory.\n\n    Returns\n    -------\n    chunk_size : int\n        The size of the dimension orthogonal to the one of size 'N'. \n    \"\"\"\n\n    mem_free = memory()['free']\n    if mem_free > 60000000:\n        chunk_size = int(((mem_free - 10000000) * 1000) / (4 * n * N))\n        return chunk_size\n    elif mem_free > 40000000:\n        chunk_size = int(((mem_free - 7000000) * 1000) / (4 * n * N))\n        return chunk_size\n    elif mem_free > 14000000:\n        chunk_size = int(((mem_free - 2000000) * 1000) / (4 * n * N))\n        return chunk_size\n    elif mem_free > 8000000:\n        chunk_size = int(((mem_free - 1400000) * 1000) / (4 * n * N))\n        return chunk_size\n    elif mem_free > 2000000:\n        chunk_size = int(((mem_free - 900000) * 1000) / (4 * n * N))\n        return chunk_size\n    elif mem_free > 1000000:\n        chunk_size = int(((mem_free - 400000) * 1000) / (4 * n * N))\n        return chunk_size\n    else:\n        print(\"\\nERROR: Cluster_Ensembles: get_chunk_size: \"\n              \"this machine does not have enough free memory resources \"\n              \"to perform ensemble clustering.\\n\")\n        sys.exit(1)"}
{"func_code_string": "def get_compression_filter(byte_counts):\n    \"\"\"Determine whether or not to use a compression on the array stored in\n        a hierarchical data format, and which compression library to use to that purpose.\n        Compression reduces the HDF5 file size and also helps improving I/O efficiency\n        for large datasets.\n    \n    Parameters\n    ----------\n    byte_counts : int\n    \n    Returns\n    -------\n    FILTERS : instance of the tables.Filters class\n    \"\"\"\n\n    assert isinstance(byte_counts, numbers.Integral) and byte_counts > 0\n    \n    if 2 * byte_counts > 1000 * memory()['free']:\n        try:\n            FILTERS = tables.filters(complevel = 5, complib = 'blosc', \n                                     shuffle = True, least_significant_digit = 6)\n        except tables.FiltersWarning:\n            FILTERS = tables.filters(complevel = 5, complib = 'lzo', \n                                     shuffle = True, least_significant_digit = 6)   \n    else:\n        FILTERS = None\n\n    return FILTERS"}
{"func_code_string": "def build_hypergraph_adjacency(cluster_runs):\n    \"\"\"Return the adjacency matrix to a hypergraph, in sparse matrix representation.\n    \n    Parameters\n    ----------\n    cluster_runs : array of shape (n_partitions, n_samples)\n    \n    Returns\n    -------\n    hypergraph_adjacency : compressed sparse row matrix\n        Represents the hypergraph associated with an ensemble of partitions,\n        each partition corresponding to a row of the array 'cluster_runs'\n        provided at input.\n    \"\"\"\n\n    N_runs = cluster_runs.shape[0]\n\n    hypergraph_adjacency = create_membership_matrix(cluster_runs[0])\n    for i in range(1, N_runs):\n        hypergraph_adjacency = scipy.sparse.vstack([hypergraph_adjacency,\n                                                   create_membership_matrix(cluster_runs[i])], \n                                                   format = 'csr')\n\n    return hypergraph_adjacency"}
{"func_code_string": "def store_hypergraph_adjacency(hypergraph_adjacency, hdf5_file_name):\n    \"\"\"Write an hypergraph adjacency to disk to disk in an HDF5 data structure.\n    \n    Parameters\n    ----------\n    hypergraph_adjacency : compressed sparse row matrix\n    \n    hdf5_file_name : file handle or string\n    \"\"\"\n   \n    assert(hypergraph_adjacency.__class__ == scipy.sparse.csr.csr_matrix)\n    \n    byte_counts = hypergraph_adjacency.data.nbytes + hypergraph_adjacency.indices.nbytes + hypergraph_adjacency.indptr.nbytes\n    FILTERS = get_compression_filter(byte_counts)\n\n    with tables.open_file(hdf5_file_name, 'r+') as fileh:\n        for par in ('data', 'indices', 'indptr', 'shape'):\n            try:\n                n = getattr(fileh.root.consensus_group, par)\n                n._f_remove()\n            except AttributeError:\n                pass\n\n            array = np.array(getattr(hypergraph_adjacency, par))\n\n            atom = tables.Atom.from_dtype(array.dtype)\n            ds = fileh.create_carray(fileh.root.consensus_group, par, atom, \n                                     array.shape, filters = FILTERS)\n\n            ds[:] = array"}
{"func_code_string": "def load_hypergraph_adjacency(hdf5_file_name):\n    \"\"\"\n    \n    Parameters\n    ----------\n    hdf5_file_name : file handle or string\n    \n    Returns\n    -------\n    hypergraph_adjacency : compressed sparse row matrix\n    \"\"\"\n\n    with tables.open_file(hdf5_file_name, 'r+') as fileh:\n        pars = []\n        for par in ('data', 'indices', 'indptr', 'shape'):\n            pars.append(getattr(fileh.root.consensus_group, par).read())\n\n    hypergraph_adjacency = scipy.sparse.csr_matrix(tuple(pars[:3]), shape = pars[3])\n    \n    return hypergraph_adjacency"}
{"func_code_string": "def cluster_ensembles(cluster_runs, hdf5_file_name = None, verbose = False, N_clusters_max = None):\n    \"\"\"Call up to three different functions for heuristic ensemble clustering\n       (namely CSPA, HGPA and MCLA) then select as the definitive\n       consensus clustering the one with the highest average mutual information score \n       between its vector of consensus labels and the vectors of labels associated to each\n       partition from the ensemble.\n\n    Parameters\n    ----------\n    cluster_runs : array of shape (n_partitions, n_samples)\n        Each row of this matrix is such that the i-th entry corresponds to the\n        cluster ID to which the i-th sample of the data-set has been classified\n        by this particular clustering. Samples not selected for clustering\n        in a given round are are tagged by an NaN.\n        \n    hdf5_file_name : file object or string, optional (default = None)\n        The handle or name of an HDF5 file where any array needed\n        for consensus_clustering and too large to fit into memory \n        is to be stored. Created if not specified at input.\n        \n    verbose : Boolean, optional (default = False)\n        Specifies if messages concerning the status of the many functions \n        subsequently called 'cluster_ensembles' will be displayed\n        on the standard output.\n\n    N_clusters_max : int, optional\n        The number of clusters in which to partition the samples into \n        a consensus clustering. This defaults to the highest number of clusters\n        encountered in the sets of independent clusterings on subsamples \n        of the data-set (i.e. the maximum of the entries in \"cluster_runs\").\n\n    Returns\n    -------\n    cluster_ensemble : array of shape (n_samples,)\n        For the final ensemble clustering, this vector contains the \n        cluster IDs of each sample in the whole data-set.\n\n    Reference\n    ---------\n    A. Strehl and J. Ghosh, \"Cluster Ensembles - A Knowledge Reuse Framework\n    for Combining Multiple Partitions\".\n    In: Journal of Machine Learning Research, 3, pp. 583-617. 2002  \n    \"\"\"\n\n    if hdf5_file_name is None:\n        hdf5_file_name = './Cluster_Ensembles.h5'\n    fileh = tables.open_file(hdf5_file_name, 'w')\n    fileh.create_group(fileh.root, 'consensus_group')\n    fileh.close()\n\n    cluster_ensemble = []\n    score = np.empty(0)\n\n    if cluster_runs.shape[1] > 10000:\n        consensus_functions = [HGPA, MCLA]\n        function_names = ['HGPA', 'MCLA']\n        print(\"\\nINFO: Cluster_Ensembles: cluster_ensembles: \"\n              \"due to a rather large number of cells in your data-set, \"\n              \"using only 'HyperGraph Partitioning Algorithm' (HGPA) \"\n              \"and 'Meta-CLustering Algorithm' (MCLA) \"\n              \"as ensemble consensus functions.\\n\")\n    else:\n        consensus_functions = [CSPA, HGPA, MCLA]\n        function_names = ['CSPA', 'HGPA', 'MCLA']\n\n    hypergraph_adjacency = build_hypergraph_adjacency(cluster_runs)\n    store_hypergraph_adjacency(hypergraph_adjacency, hdf5_file_name)\n\n    for i in range(len(consensus_functions)):\n        cluster_ensemble.append(consensus_functions[i](hdf5_file_name, cluster_runs, verbose, N_clusters_max))\n        score = np.append(score, ceEvalMutual(cluster_runs, cluster_ensemble[i], verbose))\n        print(\"\\nINFO: Cluster_Ensembles: cluster_ensembles: \"\n              \"{0} at {1}.\".format(function_names[i], score[i]))\n        print('*****')\n\n    return cluster_ensemble[np.argmax(score)]"}
{"func_code_string": "def ceEvalMutual(cluster_runs, cluster_ensemble = None, verbose = False):\n    \"\"\"Compute a weighted average of the mutual information with the known labels, \n        the weights being proportional to the fraction of known labels.\n\n    Parameters\n    ----------\n    cluster_runs : array of shape (n_partitions, n_samples)\n        Each row of this matrix is such that the i-th entry corresponds to the\n        cluster ID to which the i-th sample of the data-set has been classified\n        by this particular clustering. Samples not selected for clustering\n        in a given round are are tagged by an NaN.\n\n    cluster_ensemble : array of shape (n_samples,), optional (default = None)\n        The identity of the cluster to which each sample of the whole data-set \n        belong to according to consensus clustering.\n \n    verbose : Boolean, optional (default = False)\n        Specifies if status messages will be displayed\n        on the standard output.\n\n    Returns\n    -------\n    unnamed variable : float\n        The weighted average of the mutual information between\n        the consensus clustering and the many runs from the ensemble\n        of independent clusterings on subsamples of the data-set.\n    \"\"\"\n\n    if cluster_ensemble is None:\n        return 0.0\n\n    if reduce(operator.mul, cluster_runs.shape, 1) == max(cluster_runs.shape):\n        cluster_runs = cluster_runs.reshape(1, -1)\n\n    weighted_average_mutual_information = 0\n\n    N_labelled_indices = 0\n\n    for i in range(cluster_runs.shape[0]):\n        labelled_indices = np.where(np.isfinite(cluster_runs[i]))[0]\n        N = labelled_indices.size\n\n        x = np.reshape(checkcl(cluster_ensemble[labelled_indices], verbose), newshape = N)\n        y = np.reshape(checkcl(np.rint(cluster_runs[i, labelled_indices]), verbose), newshape = N)\n\n        q = normalized_mutual_info_score(x, y)\n\n        weighted_average_mutual_information += q * N\n        N_labelled_indices += N\n\n    return float(weighted_average_mutual_information) / N_labelled_indices"}
{"func_code_string": "def checkcl(cluster_run, verbose = False):\n    \"\"\"Ensure that a cluster labelling is in a valid format. \n\n    Parameters\n    ----------\n    cluster_run : array of shape (n_samples,)\n        A vector of cluster IDs for each of the samples selected for a given\n        round of clustering. The samples not selected are labelled with NaN.\n\n    verbose : Boolean, optional (default = False)\n        Specifies if status messages will be displayed\n        on the standard output.\n\n    Returns\n    -------\n    cluster_run : array of shape (n_samples,)\n        The input vector is modified in place, such that invalid values are\n        either rejected or altered. In particular, the labelling of cluster IDs\n        starts at zero and increases by 1 without any gap left.\n    \"\"\"\n    \n    cluster_run = np.asanyarray(cluster_run)\n\n    if cluster_run.size == 0:\n        raise ValueError(\"\\nERROR: Cluster_Ensembles: checkcl: \"\n                         \"empty vector provided as input.\\n\")\n    elif reduce(operator.mul, cluster_run.shape, 1) != max(cluster_run.shape):\n        raise ValueError(\"\\nERROR: Cluster_Ensembles: checkl: \"\n                         \"problem in dimensions of the cluster label vector \"\n                         \"under consideration.\\n\")\n    elif np.where(np.isnan(cluster_run))[0].size != 0:\n        raise ValueError(\"\\nERROR: Cluster_Ensembles: checkl: vector of cluster \"\n                         \"labellings provided as input contains at least one 'NaN'.\\n\")\n    else:\n        min_label = np.amin(cluster_run)\n        if min_label < 0:\n            if verbose:\n                print(\"\\nINFO: Cluster_Ensembles: checkcl: detected negative values \"\n                      \"as cluster labellings.\")\n\n            cluster_run -= min_label\n\n            if verbose:\n                print(\"\\nINFO: Cluster_Ensembles: checkcl: \"\n                      \"offset to a minimum value of '0'.\")\n\n        x = one_to_max(cluster_run) \n        if np.amax(cluster_run) != np.amax(x):\n            if verbose:\n                print(\"\\nINFO: Cluster_Ensembles: checkcl: the vector cluster \"\n                      \"labellings provided is not a dense integer mapping.\")\n\n            cluster_run = x\n\n            if verbose:\n                print(\"INFO: Cluster_Ensembles: checkcl: brought modification \"\n                      \"to this vector so that its labels range \"\n                      \"from 0 to {0}, included.\\n\".format(np.amax(cluster_run)))\n\n    return cluster_run"}
{"func_code_string": "def one_to_max(array_in):\n    \"\"\"Alter a vector of cluster labels to a dense mapping. \n        Given that this function is herein always called after passing \n        a vector to the function checkcl, one_to_max relies on the assumption \n        that cluster_run does not contain any NaN entries.\n\n    Parameters\n    ----------\n    array_in : a list or one-dimensional array\n        The list of cluster IDs to be processed.\n    \n    Returns\n    -------\n    result : one-dimensional array\n        A massaged version of the input vector of cluster identities.\n    \"\"\"\n    \n    x = np.asanyarray(array_in)\n    N_in = x.size\n    array_in = x.reshape(N_in)    \n\n    sorted_array = np.sort(array_in)\n    sorting_indices = np.argsort(array_in)\n\n    last = np.nan\n    current_index = -1\n    for i in range(N_in):\n        if last != sorted_array[i] or np.isnan(last):\n            last = sorted_array[i]\n            current_index += 1\n\n        sorted_array[i] = current_index\n\n    result = np.empty(N_in, dtype = int)\n    result[sorting_indices] = sorted_array\n\n    return result"}
{"func_code_string": "def checks(similarities, verbose = False):\n    \"\"\"Check that a matrix is a proper similarity matrix and bring \n        appropriate changes if applicable.\n\n    Parameters\n    ----------\n    similarities : array of shape (n_samples, n_samples)\n        A matrix of pairwise similarities between (sub)-samples of the data-set. \n\n    verbose : Boolean, optional (default = False)\n        Alerts of any issue with the similarities matrix provided\n        and of any step possibly taken to remediate such problem.\n    \"\"\"\n    \n    if similarities.size == 0:\n        raise ValueError(\"\\nERROR: Cluster_Ensembles: checks: the similarities \"\n                         \"matrix provided as input happens to be empty.\\n\")\n    elif np.where(np.isnan(similarities))[0].size != 0:\n        raise ValueError(\"\\nERROR: Cluster_Ensembles: checks: input similarities \"\n                         \"matrix contains at least one 'NaN'.\\n\")\n    elif np.where(np.isinf(similarities))[0].size != 0:\n        raise ValueError(\"\\nERROR: Cluster_Ensembles: checks: at least one infinite entry \"\n                         \"detected in input similarities matrix.\\n\")\n    else:\n        if np.where(np.logical_not(np.isreal(similarities)))[0].size != 0:\n            if verbose:\n                print(\"\\nINFO: Cluster_Ensembles: checks: complex entries found \"\n                      \"in the similarities matrix.\")\n\n            similarities = similarities.real\n\n            if verbose:\n                print(\"\\nINFO: Cluster_Ensembles: checks: \"\n                      \"truncated to their real components.\")\n\n        if similarities.shape[0] != similarities.shape[1]:\n            if verbose:\n                print(\"\\nINFO: Cluster_Ensembles: checks: non-square matrix provided.\")\n\n            N_square = min(similarities.shape)\n            similarities = similarities[:N_square, :N_square]\n\n            if verbose:\n                print(\"\\nINFO: Cluster_Ensembles: checks: using largest square sub-matrix.\")\n\n        max_sim = np.amax(similarities)\n        min_sim = np.amin(similarities)\n        if max_sim > 1 or min_sim < 0:\n            if verbose:\n                print(\"\\nINFO: Cluster_Ensembles: checks: strictly negative \"\n                      \"or bigger than unity entries spotted in input similarities matrix.\")\n\n            indices_too_big = np.where(similarities > 1) \n            indices_negative = np.where(similarities < 0)\n            similarities[indices_too_big] = 1.0\n            similarities[indices_negative] = 0.0\n\n            if verbose:\n                print(\"\\nINFO: Cluster_Ensembles: checks: done setting them to \"\n                      \"the lower or upper accepted values.\")     \n\n        if not np.allclose(similarities, np.transpose(similarities)):\n            if verbose:\n                print(\"\\nINFO: Cluster_Ensembles: checks: non-symmetric input \"\n                      \"similarities matrix.\")\n\n            similarities = np.divide(similarities + np.transpose(similarities), 2.0)\n\n            if verbose:\n                print(\"\\nINFO: Cluster_Ensembles: checks: now symmetrized.\")\n\n        if not np.allclose(np.diag(similarities), np.ones(similarities.shape[0])):\n            if verbose:\n                print(\"\\nINFO: Cluster_Ensembles: checks: the self-similarities \"\n                      \"provided as input are not all of unit value.\")\n\n            similarities[np.diag_indices(similarities.shape[0])] = 1\n\n            if verbose:\n                print(\"\\nINFO: Cluster_Ensembles: checks: issue corrected.\")"}
{"func_code_string": "def CSPA(hdf5_file_name, cluster_runs, verbose = False, N_clusters_max = None):\n    \"\"\"Cluster-based Similarity Partitioning Algorithm for a consensus function.\n    \n    Parameters\n    ----------\n    hdf5_file_name : file handle or string\n    \n    cluster_runs : array of shape (n_partitions, n_samples)\n    \n    verbose : bool, optional (default = False)\n    \n    N_clusters_max : int, optional (default = None)\n    \n    Returns\n    -------\n    A vector specifying the cluster label to which each sample has been assigned\n    by the CSPA heuristics for consensus clustering.\n\n    Reference\n    ---------\n    A. Strehl and J. Ghosh, \"Cluster Ensembles - A Knowledge Reuse Framework\n    for Combining Multiple Partitions\".\n    In: Journal of Machine Learning Research, 3, pp. 583-617. 2002\n    \"\"\"\n\n    print('*****')\n    print(\"INFO: Cluster_Ensembles: CSPA: consensus clustering using CSPA.\")\n\n    if N_clusters_max == None:\n        N_clusters_max = int(np.nanmax(cluster_runs)) + 1\n\n    N_runs = cluster_runs.shape[0]\n    N_samples = cluster_runs.shape[1]\n    if N_samples > 20000:\n        raise ValueError(\"\\nERROR: Cluster_Ensembles: CSPA: cannot efficiently \"\n                         \"deal with too large a number of cells.\")\n\n    hypergraph_adjacency = load_hypergraph_adjacency(hdf5_file_name)\n\n    s = scipy.sparse.csr_matrix.dot(hypergraph_adjacency.transpose().tocsr(), hypergraph_adjacency)\n    s = np.squeeze(np.asarray(s.todense()))\n    \n    del hypergraph_adjacency\n    gc.collect()\n\n    checks(np.divide(s, float(N_runs)), verbose)\n\n    e_sum_before = s.sum()\n    sum_after = 100000000.0  \n    scale_factor = sum_after / float(e_sum_before)\n\n    with tables.open_file(hdf5_file_name, 'r+') as fileh:\n        atom = tables.Float32Atom()\n        FILTERS = get_compression_filter(4 * (N_samples ** 2))\n\n        S = fileh.create_carray(fileh.root.consensus_group, 'similarities_CSPA', atom,\n                               (N_samples, N_samples), \"Matrix of similarities arising \"\n                               \"in Cluster-based Similarity Partitioning\", \n                               filters = FILTERS)\n\n        expr = tables.Expr(\"s * scale_factor\")\n        expr.set_output(S)\n        expr.eval()\n\n        chunks_size = get_chunk_size(N_samples, 3)\n        for i in range(0, N_samples, chunks_size):\n            tmp = S[i:min(i+chunks_size, N_samples)]\n            S[i:min(i+chunks_size, N_samples)] = np.rint(tmp)\n\n    return metis(hdf5_file_name, N_clusters_max)"}
{"func_code_string": "def HGPA(hdf5_file_name, cluster_runs, verbose = False, N_clusters_max = None):\n    \"\"\"HyperGraph-Partitioning Algorithm for a consensus function.\n    \n    Parameters\n    ----------\n    hdf5_file_name : string or file handle\n    \n    cluster_runs: array of shape (n_partitions, n_samples)\n    \n    verbose : bool, optional (default = False)\n    \n    N_clusters_max : int, optional (default = None)\n    \n    Returns\n    -------\n    A vector specifying the cluster label to which each sample has been assigned\n    by the HGPA approximation algorithm for consensus clustering.\n\n    Reference\n    ---------\n    A. Strehl and J. Ghosh, \"Cluster Ensembles - A Knowledge Reuse Framework\n    for Combining Multiple Partitions\".\n    In: Journal of Machine Learning Research, 3, pp. 583-617. 2002\n    \"\"\"\n    \n    print('\\n*****')\n    print(\"INFO: Cluster_Ensembles: HGPA: consensus clustering using HGPA.\")\n\n    if N_clusters_max == None:\n        N_clusters_max = int(np.nanmax(cluster_runs)) + 1\n\n    return hmetis(hdf5_file_name, N_clusters_max)"}
{"func_code_string": "def MCLA(hdf5_file_name, cluster_runs, verbose = False, N_clusters_max = None):\n    \"\"\"Meta-CLustering Algorithm for a consensus function.\n    \n    Parameters\n    ----------\n    hdf5_file_name : file handle or string\n    \n    cluster_runs : array of shape (n_partitions, n_samples)\n    \n    verbose : bool, optional (default = False)\n    \n    N_clusters_max : int, optional (default = None)\n    \n    Returns\n    -------\n    A vector specifying the cluster label to which each sample has been assigned\n    by the MCLA approximation algorithm for consensus clustering.\n\n    Reference\n    ---------\n    A. Strehl and J. Ghosh, \"Cluster Ensembles - A Knowledge Reuse Framework\n    for Combining Multiple Partitions\".\n    In: Journal of Machine Learning Research, 3, pp. 583-617. 2002\n    \"\"\"\n\n    print('\\n*****')\n    print('INFO: Cluster_Ensembles: MCLA: consensus clustering using MCLA.')\n\n    if N_clusters_max == None:\n        N_clusters_max = int(np.nanmax(cluster_runs)) + 1\n\n    N_runs = cluster_runs.shape[0]\n    N_samples = cluster_runs.shape[1]\n\n    print(\"INFO: Cluster_Ensembles: MCLA: preparing graph for meta-clustering.\")\n\n    hypergraph_adjacency = load_hypergraph_adjacency(hdf5_file_name)\n    w = hypergraph_adjacency.sum(axis = 1)\n\n    N_rows = hypergraph_adjacency.shape[0]\n\n    print(\"INFO: Cluster_Ensembles: MCLA: done filling hypergraph adjacency matrix. \"\n          \"Starting computation of Jaccard similarity matrix.\")\n\n    # Next, obtain a matrix of pairwise Jaccard similarity scores between the rows of the hypergraph adjacency matrix.\n    with tables.open_file(hdf5_file_name, 'r+') as fileh:\n        FILTERS = get_compression_filter(4 * (N_rows ** 2))\n    \n        similarities_MCLA = fileh.create_carray(fileh.root.consensus_group, \n                                   'similarities_MCLA', tables.Float32Atom(), \n                                   (N_rows, N_rows), \"Matrix of pairwise Jaccard \"\n                                   \"similarity scores\", filters = FILTERS)\n\n        scale_factor = 100.0\n\n        print(\"INFO: Cluster_Ensembles: MCLA: \"\n              \"starting computation of Jaccard similarity matrix.\")\n\n        squared_MCLA = hypergraph_adjacency.dot(hypergraph_adjacency.transpose())\n\n        squared_sums = hypergraph_adjacency.sum(axis = 1)\n        squared_sums = np.squeeze(np.asarray(squared_sums))\n\n        chunks_size = get_chunk_size(N_rows, 7)\n        for i in range(0, N_rows, chunks_size):\n            n_dim = min(chunks_size, N_rows - i)\n\n            temp = squared_MCLA[i:min(i+chunks_size, N_rows), :].todense()\n            temp = np.squeeze(np.asarray(temp))\n\n            x = squared_sums[i:min(i+chunks_size, N_rows)]\n            x = x.reshape(-1, 1)\n            x = np.dot(x, np.ones((1, squared_sums.size)))\n\n            y = np.dot(np.ones((n_dim, 1)), squared_sums.reshape(1, -1))\n        \n            temp = np.divide(temp, x + y - temp)\n            temp *= scale_factor\n\n            Jaccard_matrix = np.rint(temp)\n            similarities_MCLA[i:min(i+chunks_size, N_rows)] = Jaccard_matrix\n\n            del Jaccard_matrix, temp, x, y\n            gc.collect()\n \n    # Done computing the matrix of pairwise Jaccard similarity scores.\n    print(\"INFO: Cluster_Ensembles: MCLA: done computing the matrix of \"\n          \"pairwise Jaccard similarity scores.\")\n\n    cluster_labels = cmetis(hdf5_file_name, N_clusters_max, w)\n    cluster_labels = one_to_max(cluster_labels)\n    # After 'cmetis' returns, we are done with clustering hyper-edges\n\n    # We are now ready to start the procedure meant to collapse meta-clusters.\n    N_consensus = np.amax(cluster_labels) + 1\n\n    fileh = tables.open_file(hdf5_file_name, 'r+')\n\n    FILTERS = get_compression_filter(4 * N_consensus * N_samples)\n    \n    clb_cum = fileh.create_carray(fileh.root.consensus_group, 'clb_cum', \n                                  tables.Float32Atom(), (N_consensus, N_samples), \n                                  'Matrix of mean memberships, forming meta-clusters', \n                                  filters = FILTERS)  \n \n    chunks_size = get_chunk_size(N_samples, 7)\n    for i in range(0, N_consensus, chunks_size):\n        x = min(chunks_size, N_consensus - i)\n        matched_clusters = np.where(cluster_labels == np.reshape(np.arange(i, min(i + chunks_size, N_consensus)), newshape = (x, 1)))\n        M = np.zeros((x, N_samples))\n        for j in range(x):\n            coord = np.where(matched_clusters[0] == j)[0]\n            M[j] = np.asarray(hypergraph_adjacency[matched_clusters[1][coord], :].mean(axis = 0))\n        clb_cum[i:min(i+chunks_size, N_consensus)] = M\n    \n    # Done with collapsing the hyper-edges into a single meta-hyper-edge, \n    # for each of the (N_consensus - 1) meta-clusters.\n\n    del hypergraph_adjacency\n    gc.collect()\n\n    # Each object will now be assigned to its most associated meta-cluster.\n    chunks_size = get_chunk_size(N_consensus, 4)\n    N_chunks, remainder = divmod(N_samples, chunks_size)\n    if N_chunks == 0:\n        null_columns = np.where(clb_cum[:].sum(axis = 0) == 0)[0]\n    else:\n        szumsz = np.zeros(0)\n        for i in range(N_chunks):\n            M = clb_cum[:, i*chunks_size:(i+1)*chunks_size]\n            szumsz = np.append(szumsz, M.sum(axis = 0))\n        if remainder != 0:\n            M = clb_cum[:, N_chunks*chunks_size:N_samples]\n            szumsz = np.append(szumsz, M.sum(axis = 0))\n        null_columns = np.where(szumsz == 0)[0]\n\n    if null_columns.size != 0:\n        print(\"INFO: Cluster_Ensembles: MCLA: {} objects with all zero associations \"\n              \"in 'clb_cum' matrix of meta-clusters.\".format(null_columns.size))\n        clb_cum[:, null_columns] = np.random.rand(N_consensus, null_columns.size)\n\n    random_state = np.random.RandomState()\n\n    tmp = fileh.create_carray(fileh.root.consensus_group, 'tmp', tables.Float32Atom(),\n                              (N_consensus, N_samples), \"Temporary matrix to help with \"\n                              \"collapsing to meta-hyper-edges\", filters = FILTERS)\n\n    chunks_size = get_chunk_size(N_samples, 2)\n    N_chunks, remainder = divmod(N_consensus, chunks_size)\n    if N_chunks == 0:\n        tmp[:] = random_state.rand(N_consensus, N_samples)\n    else:\n        for i in range(N_chunks):\n            tmp[i*chunks_size:(i+1)*chunks_size] = random_state.rand(chunks_size, N_samples)\n        if remainder !=0:\n            tmp[N_chunks*chunks_size:N_consensus] = random_state.rand(remainder, N_samples)\n\n    expr = tables.Expr(\"clb_cum + (tmp / 10000)\")\n    expr.set_output(clb_cum)\n    expr.eval()\n\n    expr = tables.Expr(\"abs(tmp)\")\n    expr.set_output(tmp)\n    expr.eval()\n\n    chunks_size = get_chunk_size(N_consensus, 2)\n    N_chunks, remainder = divmod(N_samples, chunks_size)\n    if N_chunks == 0:\n        sum_diag = tmp[:].sum(axis = 0)\n    else:\n        sum_diag = np.empty(0)\n        for i in range(N_chunks):\n            M = tmp[:, i*chunks_size:(i+1)*chunks_size]\n            sum_diag = np.append(sum_diag, M.sum(axis = 0))\n        if remainder != 0:\n            M = tmp[:, N_chunks*chunks_size:N_samples]\n            sum_diag = np.append(sum_diag, M.sum(axis = 0))\n\n    fileh.remove_node(fileh.root.consensus_group, \"tmp\") \n    # The corresponding disk space will be freed after a call to 'fileh.close()'.\n\n    inv_sum_diag = np.reciprocal(sum_diag.astype(float))\n\n    if N_chunks == 0:\n        clb_cum *= inv_sum_diag\n        max_entries = np.amax(clb_cum, axis = 0)\n    else:\n        max_entries = np.zeros(N_samples)\n        for i in range(N_chunks):\n            clb_cum[:, i*chunks_size:(i+1)*chunks_size] *= inv_sum_diag[i*chunks_size:(i+1)*chunks_size]\n            max_entries[i*chunks_size:(i+1)*chunks_size] = np.amax(clb_cum[:, i*chunks_size:(i+1)*chunks_size], axis = 0)\n        if remainder != 0:\n            clb_cum[:, N_chunks*chunks_size:N_samples] *= inv_sum_diag[N_chunks*chunks_size:N_samples]\n            max_entries[N_chunks*chunks_size:N_samples] = np.amax(clb_cum[:, N_chunks*chunks_size:N_samples], axis = 0)\n\n    cluster_labels = np.zeros(N_samples, dtype = int)\n    winner_probabilities = np.zeros(N_samples)\n    \n    chunks_size = get_chunk_size(N_samples, 2)\n    for i in reversed(range(0, N_consensus, chunks_size)):\n        ind = np.where(np.tile(max_entries, (min(chunks_size, N_consensus - i), 1)) == clb_cum[i:min(i+chunks_size, N_consensus)])\n        cluster_labels[ind[1]] = i + ind[0]\n        winner_probabilities[ind[1]] = clb_cum[(ind[0] + i, ind[1])]       \n\n    # Done with competing for objects.\n\n    cluster_labels = one_to_max(cluster_labels)\n\n    print(\"INFO: Cluster_Ensembles: MCLA: delivering \"\n          \"{} clusters.\".format(np.unique(cluster_labels).size))\n    print(\"INFO: Cluster_Ensembles: MCLA: average posterior \"\n          \"probability is {}\".format(np.mean(winner_probabilities)))\n    if cluster_labels.size <= 7:\n        print(\"INFO: Cluster_Ensembles: MCLA: the winning posterior probabilities are:\")\n        print(winner_probabilities)\n        print(\"'INFO: Cluster_Ensembles: MCLA: the full posterior probabilities are:\")\n        print(clb_cum)\n\n    fileh.remove_node(fileh.root.consensus_group, \"clb_cum\")\n    fileh.close()\n\n    return cluster_labels"}
{"func_code_string": "def create_membership_matrix(cluster_run):\n    \"\"\"For a label vector represented by cluster_run, constructs the binary \n        membership indicator matrix. Such matrices, when concatenated, contribute \n        to the adjacency matrix for a hypergraph representation of an \n        ensemble of clusterings.\n    \n    Parameters\n    ----------\n    cluster_run : array of shape (n_partitions, n_samples)\n    \n    Returns\n    -------\n    An adjacnecy matrix in compressed sparse row form.\n    \"\"\"\n\n    cluster_run = np.asanyarray(cluster_run)\n\n    if reduce(operator.mul, cluster_run.shape, 1) != max(cluster_run.shape):\n        raise ValueError(\"\\nERROR: Cluster_Ensembles: create_membership_matrix: \"\n                         \"problem in dimensions of the cluster label vector \"\n                         \"under consideration.\")\n    else:\n        cluster_run = cluster_run.reshape(cluster_run.size)\n\n        cluster_ids = np.unique(np.compress(np.isfinite(cluster_run), cluster_run))\n      \n        indices = np.empty(0, dtype = np.int32)\n        indptr = np.zeros(1, dtype = np.int32)\n\n        for elt in cluster_ids:\n            indices = np.append(indices, np.where(cluster_run == elt)[0])\n            indptr = np.append(indptr, indices.size)\n\n        data = np.ones(indices.size, dtype = int)\n\n        return scipy.sparse.csr_matrix((data, indices, indptr), shape = (cluster_ids.size, cluster_run.size))"}
{"func_code_string": "def metis(hdf5_file_name, N_clusters_max):\n    \"\"\"METIS algorithm by Karypis and Kumar. Partitions the induced similarity graph \n        passed by CSPA.\n\n    Parameters\n    ----------\n    hdf5_file_name : string or file handle\n    \n    N_clusters_max : int\n    \n    Returns\n    -------\n    labels : array of shape (n_samples,)\n        A vector of labels denoting the cluster to which each sample has been assigned\n        as a result of the CSPA heuristics for consensus clustering.\n    \n    Reference\n    ---------\n    G. Karypis and V. Kumar, \"A Fast and High Quality Multilevel Scheme for\n    Partitioning Irregular Graphs\"\n    In: SIAM Journal on Scientific Computing, Vol. 20, No. 1, pp. 359-392, 1999.\n    \"\"\"\n\n    file_name = wgraph(hdf5_file_name)\n    labels = sgraph(N_clusters_max, file_name)\n    subprocess.call(['rm', file_name])\n\n    return labels"}
{"func_code_string": "def hmetis(hdf5_file_name, N_clusters_max, w = None):\n    \"\"\"Gives cluster labels ranging from 1 to N_clusters_max for \n        hypergraph partitioning required for HGPA.\n\n    Parameters\n    ----------\n    hdf5_file_name : file handle or string\n    \n    N_clusters_max : int\n    \n    w : array, optional (default = None)\n    \n    Returns\n    -------\n    labels : array of shape (n_samples,)\n        A vector of labels denoting the cluster to which each sample has been assigned\n        as a result of the HGPA approximation algorithm for consensus clustering.\n    \n    Reference\n    ---------\n    G. Karypis, R. Aggarwal, V. Kumar and S. Shekhar, \"Multilevel hypergraph\n    partitioning: applications in VLSI domain\" \n    In: IEEE Transactions on Very Large Scale Integration (VLSI) Systems, \n    Vol. 7, No. 1, pp. 69-79, 1999.\n    \"\"\"\n\n    if w is None:\n        file_name = wgraph(hdf5_file_name, None, 2)\n    else:\n        file_name = wgraph(hdf5_file_name, w, 3)\n    labels = sgraph(N_clusters_max, file_name)\n    labels = one_to_max(labels)\n\n    subprocess.call(['rm', file_name])\n\n    return labels"}
{"func_code_string": "def wgraph(hdf5_file_name, w = None, method = 0):\n    \"\"\"Write a graph file in a format apposite to later use by METIS or HMETIS.\n    \n    Parameters\n    ----------\n    hdf5_file_name : file handle or string\n    \n    w : list or array, optional (default = None)\n    \n    method : int, optional (default = 0)\n    \n    Returns\n    -------\n    file_name : string\n    \"\"\"\n\n    print('\\n#')\n\n    if method == 0:\n        fileh = tables.open_file(hdf5_file_name, 'r+')\n        e_mat = fileh.root.consensus_group.similarities_CSPA\n        file_name = 'wgraph_CSPA'\n    elif method == 1:\n        fileh = tables.open_file(hdf5_file_name, 'r+')\n        e_mat = fileh.root.consensus_group.similarities_MCLA\n        file_name = 'wgraph_MCLA'\n    elif method in {2, 3}:\n        hypergraph_adjacency = load_hypergraph_adjacency(hdf5_file_name)\n        e_mat = hypergraph_adjacency.copy().transpose()\n        file_name = 'wgraph_HGPA'\n        fileh = tables.open_file(hdf5_file_name, 'r+')\n    else:\n        raise ValueError(\"\\nERROR: Cluster_Ensembles: wgraph: \"\n                         \"invalid code for choice of method; \"\n                         \"choose either 0, 1, 2 or 3.\")\n\n    if w is None:\n        w = []\n\n    N_rows = e_mat.shape[0]\n    N_cols = e_mat.shape[1]\n\n    if method in {0, 1}:\n        diag_ind = np.diag_indices(N_rows)\n        e_mat[diag_ind] = 0\n\n    if method == 1:\n        scale_factor = 100.0\n        w_sum_before = np.sum(w)\n        w *= scale_factor\n        w = np.rint(w)\n\n    with open(file_name, 'w') as file:\n        print(\"INFO: Cluster_Ensembles: wgraph: writing {}.\".format(file_name))\n\n        if method == 0:\n            sz = float(np.sum(e_mat[:] > 0)) / 2\n            if int(sz) == 0:\n                return 'DO_NOT_PROCESS'\n            else:\n                file.write('{} {} 1\\n'.format(N_rows, int(sz)))\n        elif method == 1:\n            chunks_size = get_chunk_size(N_cols, 2)\n            N_chunks, remainder = divmod(N_rows, chunks_size)\n            if N_chunks == 0:\n                sz = float(np.sum(e_mat[:] > 0)) / 2\n            else:\n                sz = 0\n                for i in range(N_chunks):\n                    M = e_mat[i*chunks_size:(i+1)*chunks_size]\n                    sz += float(np.sum(M > 0))\n                if remainder != 0:\n                    M = e_mat[N_chunks*chunks_size:N_rows]\n                    sz += float(np.sum(M > 0))\n                sz = float(sz) / 2 \n            file.write('{} {} 11\\n'.format(N_rows, int(sz)))\n        else:\n            file.write('{} {} 1\\n'.format(N_cols, N_rows))\n                    \n        if method in {0, 1}:\n            chunks_size = get_chunk_size(N_cols, 2)\n            for i in range(0, N_rows, chunks_size):\n                M = e_mat[i:min(i+chunks_size, N_rows)]\n\n                for j in range(M.shape[0]):\n                    edges = np.where(M[j] > 0)[0]\n                    weights = M[j, edges]\n\n                    if method == 0:\n                        interlaced = np.zeros(2 * edges.size, dtype = int)\n                        # METIS and hMETIS have vertices numbering starting from 1:\n                        interlaced[::2] = edges + 1 \n                        interlaced[1::2] = weights\n                    else:\n                        interlaced = np.zeros(1 + 2 * edges.size, dtype = int)\n                        interlaced[0] = w[i + j]\n                        # METIS and hMETIS have vertices numbering starting from 1:\n                        interlaced[1::2] = edges + 1 \n                        interlaced[2::2] = weights\n\n                    for elt in interlaced:\n                        file.write('{} '.format(int(elt)))\n                    file.write('\\n')  \n        else:\n            print(\"INFO: Cluster_Ensembles: wgraph: {N_rows} vertices and {N_cols} \"\n                  \"non-zero hyper-edges.\".format(**locals()))\n\n            chunks_size = get_chunk_size(N_rows, 2)\n            for i in range(0, N_cols, chunks_size):\n                M = np.asarray(e_mat[:, i:min(i+chunks_size, N_cols)].todense())\n                for j in range(M.shape[1]):\n                    edges = np.where(M[:, j] > 0)[0]\n                    if method == 2:\n                        weight = np.array(M[:, j].sum(), dtype = int)\n                    else:\n                        weight = w[i + j]\n                    # METIS and hMETIS require vertices numbering starting from 1:\n                    interlaced = np.append(weight, edges + 1) \n               \n                    for elt in interlaced:\n                        file.write('{} '.format(int(elt)))\n                    file.write('\\n')\n    \n    if method in {0, 1}:\n        fileh.remove_node(fileh.root.consensus_group, e_mat.name)\n\n    fileh.close()\n\n    print('#')\n\n    return file_name"}
{"func_code_string": "def sgraph(N_clusters_max, file_name):\n    \"\"\"Runs METIS or hMETIS and returns the labels found by those \n        (hyper-)graph partitioning algorithms.\n        \n    Parameters\n    ----------\n    N_clusters_max : int\n    \n    file_name : string\n    \n    Returns\n    -------\n    labels : array of shape (n_samples,)\n        A vector of labels denoting the cluster to which each sample has been assigned\n        as a result of any of three approximation algorithms for consensus clustering \n        (either of CSPA, HGPA or MCLA).\n    \"\"\"\n\n    if file_name == 'DO_NOT_PROCESS':\n        return []\n\n    print('\\n#')\n\n    k = str(N_clusters_max)\n    out_name = file_name + '.part.' + k\n    if file_name == 'wgraph_HGPA':\n        print(\"INFO: Cluster_Ensembles: sgraph: \"\n              \"calling shmetis for hypergraph partitioning.\")\n        \n        if sys.platform.startswith('linux'):\n            shmetis_path = pkg_resources.resource_filename(__name__, \n                                         'Hypergraph_Partitioning/hmetis-1.5-linux/shmetis')\n        elif sys.platform.startswith('darwin'):\n            shmetis_path = pkg_resources.resource_filename(__name__, \n                                      'Hypergraph_Partitioning/hmetis-1.5-osx-i686/shmetis')\n        else:\n            print(\"ERROR: Cluster_Ensembles: sgraph:\\n\"\n                  \"your platform is not supported. Some code required for graph partition \"\n                  \"is only available for Linux distributions and OS X.\")\n            sys.exit(1)\n        \n        args = \"{0} ./\".format(shmetis_path) + file_name + \" \" + k + \" 15\"\n        subprocess.call(args, shell = True)\n    elif file_name == 'wgraph_CSPA' or file_name == 'wgraph_MCLA':\n        print(\"INFO: Cluster_Ensembles: sgraph: \"\n              \"calling gpmetis for graph partitioning.\")\n        args = \"gpmetis ./\" + file_name + \" \" + k\n        subprocess.call(args, shell = True)\n    else:\n        raise NameError(\"ERROR: Cluster_Ensembles: sgraph: {} is not an acceptable \"\n                        \"file-name.\".format(file_name))\n\n    labels = np.empty(0, dtype = int)\n    with open(out_name, 'r') as file:\n        print(\"INFO: Cluster_Ensembles: sgraph: (hyper)-graph partitioning completed; \"\n              \"loading {}\".format(out_name))\n        labels = np.loadtxt(out_name, dtype = int)\n        labels = labels.reshape(labels.size)\n    labels = one_to_max(labels)            \n\n    subprocess.call(['rm', out_name])\n\n    print('#')\n\n    return labels"}
{"func_code_string": "def overlap_matrix(hdf5_file_name, consensus_labels, cluster_runs):\n    \"\"\"Writes on disk (in an HDF5 file whose handle is provided as the first\n       argument to this function) a stack of matrices, each describing\n       for a particular run the overlap of cluster ID's that are matching \n       each of the cluster ID's stored in 'consensus_labels' \n       (the vector of labels obtained by ensemble clustering). \n       Returns also the adjacency matrix for consensus clustering \n       and a vector of mutual informations between each of the clusterings \n       from the ensemble and their consensus.\n       \n    Parameters\n    ----------\n    hdf5_file_name : file handle or string\n    \n    consensus_labels : array of shape (n_samples,)\n    \n    cluster_runs : array of shape (n_partitions, n_samples)\n    \n    Returns\n    -------\n    cluster_dims_list : \n    \n    mutual_info_list :\n    \n    consensus_adjacency :\n    \"\"\"\n\n    if reduce(operator.mul, cluster_runs.shape, 1) == max(cluster_runs.shape):\n        cluster_runs = cluster_runs.reshape(1, -1)\n\n    N_runs, N_samples = cluster_runs.shape\n    N_consensus_labels = np.unique(consensus_labels).size\n\n    indices_consensus_adjacency = np.empty(0, dtype = np.int32)\n    indptr_consensus_adjacency = np.zeros(1, dtype = np.int64)\n\n    for k in range(N_consensus_labels):\n        indices_consensus_adjacency = np.append(indices_consensus_adjacency, np.where(consensus_labels == k)[0])\n        indptr_consensus_adjacency = np.append(indptr_consensus_adjacency, indices_consensus_adjacency.size)\n\n    data_consensus_adjacency = np.ones(indices_consensus_adjacency.size, dtype = int) \n\n    consensus_adjacency = scipy.sparse.csr_matrix((data_consensus_adjacency, indices_consensus_adjacency, indptr_consensus_adjacency), \n                                                  shape = (N_consensus_labels, N_samples))\n\n    fileh = tables.open_file(hdf5_file_name, 'r+')\n    \n    FILTERS = get_compression_filter(4 * N_consensus_labels * N_runs)\n\n    overlap_matrix = fileh.create_earray(fileh.root.consensus_group, 'overlap_matrix',\n                                         tables.Float32Atom(), (0, N_consensus_labels), \n                                         \"Matrix of overlaps between each run and \"\n                                         \"the consensus labellings\", filters = FILTERS,\n                                         expectedrows = N_consensus_labels * N_runs)\n\n    mutual_info_list = []\n    cluster_dims_list =  [0]\n\n    for i in range(N_runs):\n        M = cluster_runs[i]\n\n        mutual_info_list.append(ceEvalMutual(M, consensus_labels))\n\n        finite_indices = np.where(np.isfinite(M))[0]\n        positive_indices = np.where(M >= 0)[0]\n        selected_indices = np.intersect1d(finite_indices, positive_indices, assume_unique = True)\n        cluster_ids = np.unique(M[selected_indices])\n        n_ids = cluster_ids.size\n\n        cluster_dims_list.append(n_ids)\n\n        unions = np.zeros((n_ids, N_consensus_labels), dtype = float)\n\n        indices = np.empty(0, dtype = int)\n        indptr = [0]\n\n        c = 0\n        for elt in cluster_ids:\n            indices = np.append(indices, np.where(M == elt)[0])\n            indptr.append(indices.size)\n\n            for k in range(N_consensus_labels):\n                x = indices_consensus_adjacency[indptr_consensus_adjacency[k]:indptr_consensus_adjacency[k+1]]\n                unions[c, k] = np.union1d(indices, x).size \n \n            c += 1 \n\n        data = np.ones(indices.size, dtype = int)\n    \n        I = scipy.sparse.csr_matrix((data, indices, indptr), shape = (n_ids, N_samples))\n\n        intersections = I.dot(consensus_adjacency.transpose())\n        intersections = np.squeeze(np.asarray(intersections.todense()))\n\n        overlap_matrix.append(np.divide(intersections, unions))\n\n    fileh.close()\n\n    return cluster_dims_list, mutual_info_list, consensus_adjacency"}
{"func_code_string": "def obfuscate(p, action):\n    \"\"\"Obfuscate the auth details to avoid easy snatching.\n\n    It's best to use a throw away account for these alerts to avoid having\n    your authentication put at risk by storing it locally.\n    \"\"\"\n    key = \"ru7sll3uQrGtDPcIW3okutpFLo6YYtd5bWSpbZJIopYQ0Du0a1WlhvJOaZEH\"\n    s = list()\n    if action == 'store':\n        if PY2:\n            for i in range(len(p)):\n                kc = key[i % len(key)]\n                ec = chr((ord(p[i]) + ord(kc)) % 256)\n                s.append(ec)\n            return base64.urlsafe_b64encode(\"\".join(s))\n        else:\n            return base64.urlsafe_b64encode(p.encode()).decode()\n    else:\n        if PY2:\n            e = base64.urlsafe_b64decode(p)\n            for i in range(len(e)):\n                kc = key[i % len(key)]\n                dc = chr((256 + ord(e[i]) - ord(kc)) % 256)\n                s.append(dc)\n            return \"\".join(s)\n        else:\n            e = base64.urlsafe_b64decode(p)\n            return e.decode()"}
{"func_code_string": "def _config_bootstrap(self):\n        \"\"\"Go through and establish the defaults on the file system.\n\n        The approach here was stolen from the CLI tool provided with the\n        module. Idea being that the user should not always need to provide a\n        username and password in order to run the script. If the configuration\n        file is already present with valid data, then lets use it.\n        \"\"\"\n        if not os.path.exists(CONFIG_PATH):\n            os.makedirs(CONFIG_PATH)\n        if not os.path.exists(CONFIG_FILE):\n            json.dump(CONFIG_DEFAULTS, open(CONFIG_FILE, 'w'), indent=4,\n                      separators=(',', ': '))\n        config = CONFIG_DEFAULTS\n        if self._email and self._password:\n            #  Save the configuration locally to pull later on\n            config['email'] = self._email\n            config['password'] = str(obfuscate(self._password, 'store'))\n            self._log.debug(\"Caching authentication in config file\")\n            json.dump(config, open(CONFIG_FILE, 'w'), indent=4,\n                      separators=(',', ': '))\n        else:\n            #  Load the config file and override the class\n            config = json.load(open(CONFIG_FILE))\n            if config.get('py2', PY2) != PY2:\n                raise Exception(\"Python versions have changed. Please run `setup` again to reconfigure the client.\")\n            if config['email'] and config['password']:\n                self._email = config['email']\n                self._password = obfuscate(str(config['password']), 'fetch')\n                self._log.debug(\"Loaded authentication from config file\")"}
{"func_code_string": "def _session_check(self):\n        \"\"\"Attempt to authenticate the user through a session file.\n\n        This process is done to avoid having to authenticate the user every\n        single time. It uses a session file that is saved when a valid session\n        is captured and then reused. Because sessions can expire, we need to\n        test the session prior to calling the user authenticated. Right now\n        that is done with a test string found in an unauthenticated session.\n        This approach is not an ideal method, but it works.\n        \"\"\"\n        if not os.path.exists(SESSION_FILE):\n            self._log.debug(\"Session file does not exist\")\n            return False\n        with open(SESSION_FILE, 'rb') as f:\n            cookies = requests.utils.cookiejar_from_dict(pickle.load(f))\n            self._session.cookies = cookies\n            self._log.debug(\"Loaded cookies from session file\")\n        response = self._session.get(url=self.TEST_URL, headers=self.HEADERS)\n        if self.TEST_KEY in str(response.content):\n            self._log.debug(\"Session file appears invalid\")\n            return False\n        self._is_authenticated = True\n        self._process_state()\n        return True"}
{"func_code_string": "def _logger(self):\n        \"\"\"Create a logger to be used between processes.\n\n        :returns: Logging instance.\n        \"\"\"\n        logger = logging.getLogger(self.NAME)\n        logger.setLevel(self.LOG_LEVEL)\n        shandler = logging.StreamHandler(sys.stdout)\n        fmt = '\\033[1;32m%(levelname)-5s %(module)s:%(funcName)s():'\n        fmt += '%(lineno)d %(asctime)s\\033[0m| %(message)s'\n        shandler.setFormatter(logging.Formatter(fmt))\n        logger.addHandler(shandler)\n        return logger"}
{"func_code_string": "def set_log_level(self, level):\n        \"\"\"Override the default log level of the class\"\"\"\n        if level == 'info':\n            level = logging.INFO\n        if level == 'debug':\n            level = logging.DEBUG\n        if level == 'error':\n            level = logging.ERROR\n        self._log.setLevel(level)"}
{"func_code_string": "def _process_state(self):\n        \"\"\"Process the application state configuration.\n\n        Google Alerts manages the account information and alert data through\n        some custom state configuration. Not all values have been completely\n        enumerated.\n        \"\"\"\n        self._log.debug(\"Capturing state from the request\")\n        response = self._session.get(url=self.ALERTS_URL, headers=self.HEADERS)\n        soup = BeautifulSoup(response.content, \"html.parser\")\n        for i in soup.findAll('script'):\n            if i.text.find('window.STATE') == -1:\n                continue\n            state = json.loads(i.text[15:-1])\n            if state != \"\":\n                self._state = state\n                self._log.debug(\"State value set: %s\" % self._state)\n        return self._state"}
{"func_code_string": "def authenticate(self):\n        \"\"\"Authenticate the user and setup our state.\"\"\"\n        valid = self._session_check()\n        if self._is_authenticated and valid:\n            self._log.debug(\"[!] User has already authenticated\")\n            return\n        init = self._session.get(url=self.LOGIN_URL, headers=self.HEADERS)\n        soup = BeautifulSoup(init.content, \"html.parser\")\n        soup_login = soup.find('form').find_all('input')\n        post_data = dict()\n        for u in soup_login:\n            if u.has_attr('name') and u.has_attr('value'):\n                post_data[u['name']] = u['value']\n        post_data['Email'] = self._email\n        post_data['Passwd'] = self._password\n        response = self._session.post(url=self.AUTH_URL, data=post_data,\n                                      headers=self.HEADERS)\n        if self.CAPTCHA_KEY in str(response.content):\n            raise AccountCaptcha('Google is forcing a CAPTCHA. To get around this issue, run the google-alerts with the seed option to open an interactive authentication session. Once authenticated, this module will cache your session and load that in the future')\n        cookies = [x.name for x in response.cookies]\n        if 'SIDCC' not in cookies:\n            raise InvalidCredentials(\"Email or password was incorrect.\")\n        with open(SESSION_FILE, 'wb') as f:\n            cookies = requests.utils.dict_from_cookiejar(self._session.cookies)\n            pickle.dump(cookies, f, protocol=2)\n            self._log.debug(\"Saved session to disk for future reference\")\n        self._log.debug(\"User successfully authenticated\")\n        self._is_authenticated = True\n        self._process_state()\n        return"}
{"func_code_string": "def list(self, term=None):\n        \"\"\"List alerts configured for the account.\"\"\"\n        if not self._state:\n            raise InvalidState(\"State was not properly obtained from the app\")\n        self._process_state()\n        if not self._state[1]:\n            self._log.info(\"No monitors have been created yet.\")\n            return list()\n\n        monitors = list()\n        for monitor in self._state[1][1]:\n            obj = dict()\n            obj['monitor_id'] = monitor[1]\n            obj['user_id'] = monitor[-1]\n            obj['term'] = monitor[2][3][1]\n            if term and obj['term'] != term:\n                continue\n            obj['language'] = monitor[2][3][3][1]\n            obj['region'] = monitor[2][3][3][2]\n            obj['delivery'] = self.DELIVERY[monitor[2][6][0][1]]\n            obj['match_type'] = self.MONITOR_MATCH_TYPE[monitor[2][5]]\n            if obj['delivery'] == 'MAIL':\n                obj['alert_frequency'] = self.ALERT_FREQ[monitor[2][6][0][4]]\n                obj['email_address'] = monitor[2][6][0][2]\n            else:\n                rss_id = monitor[2][6][0][11]\n                url = \"https://google.com/alerts/feeds/{uid}/{fid}\"\n                obj['rss_link'] = url.format(uid=obj['user_id'], fid=rss_id)\n            monitors.append(obj)\n        return monitors"}
{"func_code_string": "def create(self, term, options):\n        \"\"\"Create a monitor using passed configuration.\"\"\"\n        if not self._state:\n            raise InvalidState(\"State was not properly obtained from the app\")\n        options['action'] = 'CREATE'\n        payload = self._build_payload(term, options)\n        url = self.ALERTS_CREATE_URL.format(requestX=self._state[3])\n        self._log.debug(\"Creating alert using: %s\" % url)\n        params = json.dumps(payload, separators=(',', ':'))\n        data = {'params': params}\n        response = self._session.post(url, data=data, headers=self.HEADERS)\n        if response.status_code != 200:\n            raise ActionError(\"Failed to create monitor: %s\"\n                              % response.content)\n        if options.get('exact', False):\n            term = \"\\\"%s\\\"\" % term\n        return self.list(term)"}
{"func_code_string": "def modify(self, monitor_id, options):\n        \"\"\"Create a monitor using passed configuration.\"\"\"\n        if not self._state:\n            raise InvalidState(\"State was not properly obtained from the app\")\n        monitors = self.list()  # Get the latest set of monitors\n        obj = None\n        for monitor in monitors:\n            if monitor_id != monitor['monitor_id']:\n                continue\n            obj = monitor\n        if not monitor_id:\n            raise MonitorNotFound(\"No monitor was found with that term.\")\n        options['action'] = 'MODIFY'\n        options.update(obj)\n        payload = self._build_payload(obj['term'], options)\n        url = self.ALERTS_MODIFY_URL.format(requestX=self._state[3])\n        self._log.debug(\"Modifying alert using: %s\" % url)\n        params = json.dumps(payload, separators=(',', ':'))\n        data = {'params': params}\n        response = self._session.post(url, data=data, headers=self.HEADERS)\n        if response.status_code != 200:\n            raise ActionError(\"Failed to create monitor: %s\"\n                              % response.content)\n        return self.list()"}
{"func_code_string": "def delete(self, monitor_id):\n        \"\"\"Delete a monitor by ID.\"\"\"\n        if not self._state:\n            raise InvalidState(\"State was not properly obtained from the app\")\n        monitors = self.list()  # Get the latest set of monitors\n        bit = None\n        for monitor in monitors:\n            if monitor_id != monitor['monitor_id']:\n                continue\n            bit = monitor['monitor_id']\n        if not bit:\n            raise MonitorNotFound(\"No monitor was found with that term.\")\n        url = self.ALERTS_DELETE_URL.format(requestX=self._state[3])\n        self._log.debug(\"Deleting alert using: %s\" % url)\n        payload = [None, monitor_id]\n        params = json.dumps(payload, separators=(',', ':'))\n        data = {'params': params}\n        response = self._session.post(url, data=data, headers=self.HEADERS)\n        if response.status_code != 200:\n            raise ActionError(\"Failed to delete by ID: %s\"\n                              % response.content)\n        return True"}
{"func_code_string": "def main():\n    \"\"\"Run the core.\"\"\"\n    parser = ArgumentParser()\n    subs = parser.add_subparsers(dest='cmd')\n    setup_parser = subs.add_parser('setup')\n    setup_parser.add_argument('-e', '--email', dest='email', required=True,\n                              help='Email of the Google user.', type=str)\n    setup_parser.add_argument('-p', '--password', dest='pwd', required=True,\n                              help='Password of the Google user.', type=str)\n    setup_parser = subs.add_parser('seed')\n    setup_parser.add_argument('-d', '--driver', dest='driver',\n                              required=True, type=str,\n                              help='Location of the Chrome driver. This can be downloaded by visiting http://chromedriver.chromium.org/downloads',)\n    setup_parser = subs.add_parser('list')\n    setup_parser = subs.add_parser('create')\n    setup_parser.add_argument('-t', '--term', dest='term', required=True,\n                              help='Term to store.', type=str)\n    setup_parser.add_argument('--exact', dest='exact', action='store_true',\n                              help='Exact matches only for term.')\n    setup_parser.add_argument('-d', '--delivery', dest='delivery',\n                              required=True, choices=['rss', 'mail'],\n                              help='Delivery method of results.')\n    setup_parser.add_argument('-f', '--frequency', dest='frequency',\n                              default=\"realtime\", choices=['realtime', 'daily', 'weekly'],\n                              help='Frequency to send results. RSS only allows for realtime alerting.')\n    setup_parser = subs.add_parser('delete')\n    setup_parser.add_argument('--id', dest='term_id', required=True,\n                              help='ID of the term to find for deletion.',\n                              type=str)\n    args = parser.parse_args()\n\n    if args.cmd == 'setup':\n        if not os.path.exists(CONFIG_PATH):\n            os.makedirs(CONFIG_PATH)\n        if not os.path.exists(CONFIG_FILE):\n            json.dump(CONFIG_DEFAULTS, open(CONFIG_FILE, 'w'), indent=4,\n                      separators=(',', ': '))\n        config = CONFIG_DEFAULTS\n        config['email'] = args.email\n        config['password'] = str(obfuscate(args.pwd, 'store'))\n        json.dump(config, open(CONFIG_FILE, 'w'), indent=4,\n                  separators=(',', ': '))\n\n    config = json.load(open(CONFIG_FILE))\n    if config.get('py2', PY2) != PY2:\n        raise Exception(\"Python versions have changed. Please run `setup` again to reconfigure the client.\")\n    if config['password'] == '':\n        raise Exception(\"Run setup before any other actions!\")\n\n    if args.cmd == 'seed':\n        config['password'] = obfuscate(str(config['password']), 'fetch')\n        ga = GoogleAlerts(config['email'], config['password'])\n        with contextlib.closing(webdriver.Chrome(args.driver)) as driver:\n            driver.get(ga.LOGIN_URL)\n            wait = ui.WebDriverWait(driver, 10) # timeout after 10 seconds\n            inputElement = driver.find_element_by_name('Email')\n            inputElement.send_keys(config['email'])\n            inputElement.submit()\n            time.sleep(3)\n            inputElement = driver.find_element_by_id('Passwd')\n            inputElement.send_keys(config['password'])\n            inputElement.submit()\n            print(\"[!] Waiting 15 seconds for authentication to complete\")\n            time.sleep(15)\n            cookies = driver.get_cookies()\n            collected = dict()\n            for cookie in cookies:\n                collected[str(cookie['name'])] = str(cookie['value'])\n            with open(SESSION_FILE, 'wb') as f:\n                pickle.dump(collected, f, protocol=2)\n        print(\"Session has been seeded.\")\n\n    if args.cmd == 'list':\n        config['password'] = obfuscate(str(config['password']), 'fetch')\n        ga = GoogleAlerts(config['email'], config['password'])\n        ga.authenticate()\n        print(json.dumps(ga.list(), indent=4))\n\n    if args.cmd == 'create':\n        config['password'] = obfuscate(str(config['password']), 'fetch')\n        ga = GoogleAlerts(config['email'], config['password'])\n        ga.authenticate()\n        alert_frequency = 'as_it_happens'\n        if args.frequency == 'realtime':\n            alert_frequency = 'as_it_happens'\n        elif args.frequency == 'daily':\n            alert_frequency = 'at_most_once_a_day'\n        else:\n            alert_frequency = 'at_most_once_a_week'\n\n        monitor = ga.create(args.term, {'delivery': args.delivery.upper(),\n                                        'alert_frequency': alert_frequency.upper(),\n                                        'exact': args.exact})\n        print(json.dumps(monitor, indent=4))\n\n    if args.cmd == 'delete':\n        config['password'] = obfuscate(str(config['password']), 'fetch')\n        ga = GoogleAlerts(config['email'], config['password'])\n        ga.authenticate()\n        result = ga.delete(args.term_id)\n        if result:\n            print(\"%s was deleted\" % args.term_id)"}
{"func_code_string": "def search_packages_info(query):\n    \"\"\"\n    Gather details from installed distributions. Print distribution name,\n    version, location, and installed files. Installed files requires a\n    pip generated 'installed-files.txt' in the distributions '.egg-info'\n    directory.\n    \"\"\"\n    installed = {}\n    for p in pkg_resources.working_set:\n        installed[canonicalize_name(p.project_name)] = p\n\n    query_names = [canonicalize_name(name) for name in query]\n\n    for dist in [installed[pkg] for pkg in query_names if pkg in installed]:\n        package = {\n            'name': dist.project_name,\n            'version': dist.version,\n            'location': dist.location,\n            'requires': [dep.project_name for dep in dist.requires()],\n        }\n        file_list = None\n        if isinstance(dist, pkg_resources.DistInfoDistribution):\n            # RECORDs should be part of .dist-info metadatas\n            if dist.has_metadata('RECORD'):\n                lines = dist.get_metadata_lines('RECORD')\n                paths = [l.split(',')[0] for l in lines]\n                paths = [os.path.join(dist.location, p) for p in paths]\n                file_list = [os.path.relpath(p, dist.location) for p in paths]\n        else:\n            # Otherwise use pip's log for .egg-info's\n            if dist.has_metadata('installed-files.txt'):\n                paths = dist.get_metadata_lines('installed-files.txt')\n                paths = [os.path.join(dist.egg_info, p) for p in paths]\n                file_list = [os.path.relpath(p, dist.location) for p in paths]\n\n        if file_list:\n            package['files'] = sorted(file_list)\n        yield package"}
{"func_code_string": "def process_view(self, request, view_func, view_args, view_kwargs):\n        \"\"\"\n        Collect data on Class-Based Views\n        \"\"\"\n\n        # Purge data in view method cache\n        # Python 3's keys() method returns an iterator, so force evaluation before iterating.\n        view_keys = list(VIEW_METHOD_DATA.keys())\n        for key in view_keys:\n            del VIEW_METHOD_DATA[key]\n\n        self.view_data = {}\n\n        try:\n            cbv = view_func.view_class\n        except AttributeError:\n            cbv = False\n\n        if cbv:\n\n            self.view_data['cbv'] = True\n            klass = view_func.view_class\n            self.view_data['bases'] = [base.__name__ for base in inspect.getmro(klass)]\n            # Inject with drugz\n\n            for member in inspect.getmembers(view_func.view_class):\n                # Check that we are interested in capturing data for this method\n                # and ensure that a decorated method is not decorated multiple times.\n                if member[0] in VIEW_METHOD_WHITEIST and member[0] not in PATCHED_METHODS[klass]:\n                    decorate_method(klass, member[0])\n                    PATCHED_METHODS[klass].append(member[0])"}
{"func_code_string": "def process_response(self, request, response):\n        \"\"\"Let's handle old-style response processing here, as usual.\"\"\"\n\n        # For debug only.\n        if not settings.DEBUG:\n            return response\n\n        # Check for responses where the data can't be inserted.\n        content_encoding = response.get('Content-Encoding', '')\n        content_type = response.get('Content-Type', '').split(';')[0]\n        if any((getattr(response, 'streaming', False),\n                'gzip' in content_encoding,\n                content_type not in _HTML_TYPES)):\n            return response\n\n        content = force_text(response.content, encoding=settings.DEFAULT_CHARSET)\n\n        pattern = re.escape('</body>')\n        bits = re.split(pattern, content, flags=re.IGNORECASE)\n\n        if len(bits) > 1:\n            bits[-2] += debug_payload(request, response, self.view_data)\n            response.content = \"</body>\".join(bits)\n            if response.get('Content-Length', None):\n                response['Content-Length'] = len(response.content)\n\n        return response"}
{"func_code_string": "def get_job_class(klass_str):\n    \"\"\"\n    Return the job class\n    \"\"\"\n    mod_name, klass_name = klass_str.rsplit('.', 1)\n    try:\n        mod = importlib.import_module(mod_name)\n    except ImportError as e:\n        logger.error(\"Error importing job module %s: '%s'\", mod_name, e)\n        return\n    try:\n        klass = getattr(mod, klass_name)\n    except AttributeError:\n        logger.error(\"Module '%s' does not define a '%s' class\", mod_name, klass_name)\n        return\n    return klass"}
{"func_code_string": "def get(self, *raw_args, **raw_kwargs):\n        \"\"\"\n        Return the data for this function (using the cache if possible).\n\n        This method is not intended to be overidden\n        \"\"\"\n        # We pass args and kwargs through a filter to allow them to be\n        # converted into values that can be pickled.\n        args = self.prepare_args(*raw_args)\n        kwargs = self.prepare_kwargs(**raw_kwargs)\n\n        # Build the cache key and attempt to fetch the cached item\n        key = self.key(*args, **kwargs)\n        item = self.cache.get(key)\n\n        call = Call(args=raw_args, kwargs=raw_kwargs)\n\n        if item is None:\n            # Cache MISS - we can either:\n            # a) fetch the data immediately, blocking execution until\n            #    the fetch has finished, or\n            # b) trigger an async refresh and return an empty result\n            if self.should_missing_item_be_fetched_synchronously(*args, **kwargs):\n                logger.debug((\"Job %s with key '%s' - cache MISS - running \"\n                              \"synchronous refresh\"),\n                             self.class_path, key)\n                result = self.refresh(*args, **kwargs)\n                return self.process_result(\n                    result, call=call, cache_status=self.MISS, sync_fetch=True)\n\n            else:\n                logger.debug((\"Job %s with key '%s' - cache MISS - triggering \"\n                              \"async refresh and returning empty result\"),\n                             self.class_path, key)\n                # To avoid cache hammering (ie lots of identical tasks\n                # to refresh the same cache item), we reset the cache with an\n                # empty result which will be returned until the cache is\n                # refreshed.\n                result = self.empty()\n                self.store(key, self.timeout(*args, **kwargs), result)\n                self.async_refresh(*args, **kwargs)\n                return self.process_result(\n                    result, call=call, cache_status=self.MISS,\n                    sync_fetch=False)\n\n        expiry, data = item\n        delta = time.time() - expiry\n        if delta > 0:\n            # Cache HIT but STALE expiry - we can either:\n            # a) fetch the data immediately, blocking execution until\n            #    the fetch has finished, or\n            # b) trigger a refresh but allow the stale result to be\n            #    returned this time.  This is normally acceptable.\n            if self.should_stale_item_be_fetched_synchronously(\n                    delta, *args, **kwargs):\n                logger.debug(\n                    (\"Job %s with key '%s' - STALE cache hit - running \"\n                     \"synchronous refresh\"),\n                    self.class_path, key)\n                result = self.refresh(*args, **kwargs)\n                return self.process_result(\n                    result, call=call, cache_status=self.STALE,\n                    sync_fetch=True)\n\n            else:\n                logger.debug(\n                    (\"Job %s with key '%s' - STALE cache hit - triggering \"\n                     \"async refresh and returning stale result\"),\n                    self.class_path, key)\n                # We replace the item in the cache with a 'timeout' expiry - this\n                # prevents cache hammering but guards against a 'limbo' situation\n                # where the refresh task fails for some reason.\n                timeout = self.timeout(*args, **kwargs)\n                self.store(key, timeout, data)\n                self.async_refresh(*args, **kwargs)\n                return self.process_result(\n                    data, call=call, cache_status=self.STALE, sync_fetch=False)\n        else:\n            logger.debug(\"Job %s with key '%s' - cache HIT\", self.class_path, key)\n            return self.process_result(data, call=call, cache_status=self.HIT)"}
{"func_code_string": "def invalidate(self, *raw_args, **raw_kwargs):\n        \"\"\"\n        Mark a cached item invalid and trigger an asynchronous\n        job to refresh the cache\n        \"\"\"\n        args = self.prepare_args(*raw_args)\n        kwargs = self.prepare_kwargs(**raw_kwargs)\n        key = self.key(*args, **kwargs)\n        item = self.cache.get(key)\n        if item is not None:\n            expiry, data = item\n            self.store(key, self.timeout(*args, **kwargs), data)\n            self.async_refresh(*args, **kwargs)"}
{"func_code_string": "def delete(self, *raw_args, **raw_kwargs):\n        \"\"\"\n        Remove an item from the cache\n        \"\"\"\n        args = self.prepare_args(*raw_args)\n        kwargs = self.prepare_kwargs(**raw_kwargs)\n        key = self.key(*args, **kwargs)\n        item = self.cache.get(key)\n        if item is not None:\n            self.cache.delete(key)"}
{"func_code_string": "def raw_get(self, *raw_args, **raw_kwargs):\n        \"\"\"\n        Retrieve the item (tuple of value and expiry) that is actually in the cache,\n        without causing a refresh.\n        \"\"\"\n\n        args = self.prepare_args(*raw_args)\n        kwargs = self.prepare_kwargs(**raw_kwargs)\n\n        key = self.key(*args, **kwargs)\n\n        return self.cache.get(key)"}
{"func_code_string": "def set(self, *raw_args, **raw_kwargs):\n        \"\"\"\n        Manually set the cache value with its appropriate expiry.\n        \"\"\"\n        if self.set_data_kwarg in raw_kwargs:\n            data = raw_kwargs.pop(self.set_data_kwarg)\n        else:\n            raw_args = list(raw_args)\n            data = raw_args.pop()\n\n        args = self.prepare_args(*raw_args)\n        kwargs = self.prepare_kwargs(**raw_kwargs)\n\n        key = self.key(*args, **kwargs)\n\n        expiry = self.expiry(*args, **kwargs)\n\n        logger.debug(\"Setting %s cache with key '%s', args '%r', kwargs '%r', expiry '%r'\",\n                     self.class_path, key, args, kwargs, expiry)\n\n        self.store(key, expiry, data)"}
{"func_code_string": "def store(self, key, expiry, data):\n        \"\"\"\n        Add a result to the cache\n\n        :key: Cache key to use\n        :expiry: The expiry timestamp after which the result is stale\n        :data: The data to cache\n        \"\"\"\n        self.cache.set(key, (expiry, data), self.cache_ttl)\n\n        if getattr(settings, 'CACHEBACK_VERIFY_CACHE_WRITE', True):\n            # We verify that the item was cached correctly.  This is to avoid a\n            # Memcache problem where some values aren't cached correctly\n            # without warning.\n            __, cached_data = self.cache.get(key, (None, None))\n            if data is not None and cached_data is None:\n                raise RuntimeError(\n                    \"Unable to save data of type %s to cache\" % (\n                        type(data)))"}
{"func_code_string": "def refresh(self, *args, **kwargs):\n        \"\"\"\n        Fetch the result SYNCHRONOUSLY and populate the cache\n        \"\"\"\n        result = self.fetch(*args, **kwargs)\n        self.store(self.key(*args, **kwargs), self.expiry(*args, **kwargs), result)\n        return result"}
{"func_code_string": "def async_refresh(self, *args, **kwargs):\n        \"\"\"\n        Trigger an asynchronous job to refresh the cache\n        \"\"\"\n        # We trigger the task with the class path to import as well as the\n        # (a) args and kwargs for instantiating the class\n        # (b) args and kwargs for calling the 'refresh' method\n\n        try:\n            enqueue_task(\n                dict(\n                    klass_str=self.class_path,\n                    obj_args=self.get_init_args(),\n                    obj_kwargs=self.get_init_kwargs(),\n                    call_args=args,\n                    call_kwargs=kwargs\n                ),\n                task_options=self.task_options\n            )\n        except Exception:\n            # Handle exceptions from talking to RabbitMQ - eg connection\n            # refused.  When this happens, we try to run the task\n            # synchronously.\n            logger.error(\"Unable to trigger task asynchronously - failing \"\n                         \"over to synchronous refresh\", exc_info=True)\n            try:\n                return self.refresh(*args, **kwargs)\n            except Exception as e:\n                # Something went wrong while running the task\n                logger.error(\"Unable to refresh data synchronously: %s\", e,\n                             exc_info=True)\n            else:\n                logger.debug(\"Failover synchronous refresh completed successfully\")"}
{"func_code_string": "def should_stale_item_be_fetched_synchronously(self, delta, *args, **kwargs):\n        \"\"\"\n        Return whether to refresh an item synchronously when it is found in the\n        cache but stale\n        \"\"\"\n        if self.fetch_on_stale_threshold is None:\n            return False\n        return delta > (self.fetch_on_stale_threshold - self.lifetime)"}
{"func_code_string": "def key(self, *args, **kwargs):\n        \"\"\"\n        Return the cache key to use.\n\n        If you're passing anything but primitive types to the ``get`` method,\n        it's likely that you'll need to override this method.\n        \"\"\"\n        if not args and not kwargs:\n            return self.class_path\n        try:\n            if args and not kwargs:\n                return \"%s:%s\" % (self.class_path, self.hash(args))\n            # The line might break if your passed values are un-hashable.  If\n            # it does, you need to override this method and implement your own\n            # key algorithm.\n            return \"%s:%s:%s:%s\" % (self.class_path,\n                                    self.hash(args),\n                                    self.hash([k for k in sorted(kwargs)]),\n                                    self.hash([kwargs[k] for k in sorted(kwargs)]))\n        except TypeError:\n            raise RuntimeError(\n                \"Unable to generate cache key due to unhashable\"\n                \"args or kwargs - you need to implement your own\"\n                \"key generation method to avoid this problem\")"}
{"func_code_string": "def hash(self, value):\n        \"\"\"\n        Generate a hash of the given iterable.\n\n        This is for use in a cache key.\n        \"\"\"\n        if is_iterable(value):\n            value = tuple(to_bytestring(v) for v in value)\n        return hashlib.md5(six.b(':').join(value)).hexdigest()"}
{"func_code_string": "def perform_async_refresh(cls, klass_str, obj_args, obj_kwargs, call_args, call_kwargs):\n        \"\"\"\n        Re-populate cache using the given job class.\n\n        The job class is instantiated with the passed constructor args and the\n        refresh method is called with the passed call args.  That is::\n\n            data = klass(*obj_args, **obj_kwargs).refresh(\n                *call_args, **call_kwargs)\n\n        :klass_str: String repr of class (eg 'apps.twitter.jobs.FetchTweetsJob')\n        :obj_args: Constructor args\n        :obj_kwargs: Constructor kwargs\n        :call_args: Refresh args\n        :call_kwargs: Refresh kwargs\n        \"\"\"\n        klass = get_job_class(klass_str)\n        if klass is None:\n            logger.error(\"Unable to construct %s with args %r and kwargs %r\",\n                         klass_str, obj_args, obj_kwargs)\n            return\n\n        logger.info(\"Using %s with constructor args %r and kwargs %r\",\n                    klass_str, obj_args, obj_kwargs)\n        logger.info(\"Calling refresh with args %r and kwargs %r\", call_args,\n                    call_kwargs)\n        start = time.time()\n        try:\n            klass(*obj_args, **obj_kwargs).refresh(\n                *call_args, **call_kwargs)\n        except Exception as e:\n            logger.exception(\"Error running job: '%s'\", e)\n        else:\n            duration = time.time() - start\n            logger.info(\"Refreshed cache in %.6f seconds\", duration)"}
{"func_code_string": "def cacheback(lifetime=None, fetch_on_miss=None, cache_alias=None,\n              job_class=None, task_options=None, **job_class_kwargs):\n    \"\"\"\n    Decorate function to cache its return value.\n\n    :lifetime: How long to cache items for\n    :fetch_on_miss: Whether to perform a synchronous fetch when no cached\n                    result is found\n    :cache_alias: The Django cache alias to store the result into.\n    :job_class: The class to use for running the cache refresh job.  Defaults\n                using the FunctionJob.\n    :job_class_kwargs: Any extra kwargs to pass to job_class constructor.\n                       Useful with custom job_class implementations.\n    \"\"\"\n    if job_class is None:\n        job_class = FunctionJob\n    job = job_class(lifetime=lifetime, fetch_on_miss=fetch_on_miss,\n                    cache_alias=cache_alias, task_options=task_options,\n                    **job_class_kwargs)\n\n    def _wrapper(fn):\n        # using available_attrs to work around http://bugs.python.org/issue3445\n        @wraps(fn, assigned=available_attrs(fn))\n        def __wrapper(*args, **kwargs):\n            return job.get(fn, *args, **kwargs)\n        # Assign reference to unwrapped function so that we can access it\n        # later without descending into infinite regress.\n        __wrapper.fn = fn\n        # Assign reference to job so we can use the full Job API\n        __wrapper.job = job\n        return __wrapper\n\n    return _wrapper"}
{"func_code_string": "def angle(v1, v2):\n    \"\"\"Return the angle in radians between vectors 'v1' and 'v2'.\"\"\"\n    v1_u = unit_vector(v1)\n    v2_u = unit_vector(v2)\n    return np.arccos(np.clip(np.dot(v1_u, v2_u), -1.0, 1.0))"}
{"func_code_string": "def keep_high_angle(vertices, min_angle_deg):\n    \"\"\"Keep vertices with angles higher then given minimum.\"\"\"\n    accepted = []\n    v = vertices\n    v1 = v[1] - v[0]\n    accepted.append((v[0][0], v[0][1]))\n    for i in range(1, len(v) - 2):\n        v2 = v[i + 1] - v[i - 1]\n        diff_angle = np.fabs(angle(v1, v2) * 180.0 / np.pi)\n        if diff_angle > min_angle_deg:\n            accepted.append((v[i][0], v[i][1]))\n            v1 = v[i] - v[i - 1]\n    accepted.append((v[-1][0], v[-1][1]))\n    return np.array(accepted, dtype=vertices.dtype)"}
{"func_code_string": "def set_contourf_properties(stroke_width, fcolor, fill_opacity, contour_levels, contourf_idx, unit):\n    \"\"\"Set property values for Polygon.\"\"\"\n    return {\n        \"stroke\": fcolor,\n        \"stroke-width\": stroke_width,\n        \"stroke-opacity\": 1,\n        \"fill\": fcolor,\n        \"fill-opacity\": fill_opacity,\n        \"title\": \"%.2f\" % contour_levels[contourf_idx] + ' ' + unit\n    }"}
{"func_code_string": "def contour_to_geojson(contour, geojson_filepath=None, min_angle_deg=None,\n                       ndigits=5, unit='', stroke_width=1, geojson_properties=None, strdump=False,\n                       serialize=True):\n    \"\"\"Transform matplotlib.contour to geojson.\"\"\"\n    collections = contour.collections\n    contour_index = 0\n    line_features = []\n    for collection in collections:\n        color = collection.get_edgecolor()\n        for path in collection.get_paths():\n            v = path.vertices\n            if len(v) < 3:\n                continue\n            coordinates = keep_high_angle(v, min_angle_deg)\n            if ndigits:\n                coordinates = np.around(coordinates, ndigits)\n            line = LineString(coordinates.tolist())\n            properties = {\n                \"stroke-width\": stroke_width,\n                \"stroke\": rgb2hex(color[0]),\n                \"title\": \"%.2f\" % contour.levels[contour_index] + ' ' + unit,\n                \"level-value\": float(\"%.6f\" % contour.levels[contour_index]),\n                \"level-index\": contour_index\n            }\n            if geojson_properties:\n                properties.update(geojson_properties)\n            line_features.append(Feature(geometry=line, properties=properties))\n        contour_index += 1\n    feature_collection = FeatureCollection(line_features)\n    return _render_feature_collection(feature_collection, geojson_filepath, strdump, serialize)"}
{"func_code_string": "def contourf_to_geojson_overlap(contourf, geojson_filepath=None, min_angle_deg=None,\n                                ndigits=5, unit='', stroke_width=1, fill_opacity=.9,\n                                geojson_properties=None, strdump=False, serialize=True):\n    \"\"\"Transform matplotlib.contourf to geojson with overlapping filled contours.\"\"\"\n    polygon_features = []\n    contourf_idx = 0\n    for collection in contourf.collections:\n        color = collection.get_facecolor()\n        for path in collection.get_paths():\n            for coord in path.to_polygons():\n                if min_angle_deg:\n                    coord = keep_high_angle(coord, min_angle_deg)\n                coord = np.around(coord, ndigits) if ndigits else coord\n                polygon = Polygon(coordinates=[coord.tolist()])\n                fcolor = rgb2hex(color[0])\n                properties = set_contourf_properties(stroke_width, fcolor, fill_opacity, contourf.levels, contourf_idx, unit)\n                if geojson_properties:\n                    properties.update(geojson_properties)\n                feature = Feature(geometry=polygon, properties=properties)\n                polygon_features.append(feature)\n        contourf_idx += 1\n    feature_collection = FeatureCollection(polygon_features)\n    return _render_feature_collection(feature_collection, geojson_filepath, strdump, serialize)"}
{"func_code_string": "def contourf_to_geojson(contourf, geojson_filepath=None, min_angle_deg=None,\n                        ndigits=5, unit='', stroke_width=1, fill_opacity=.9,\n                        geojson_properties=None, strdump=False, serialize=True):\n    \"\"\"Transform matplotlib.contourf to geojson with MultiPolygons.\"\"\"\n    polygon_features = []\n    mps = []\n    contourf_idx = 0\n    for coll in contourf.collections:\n        color = coll.get_facecolor()\n        for path in coll.get_paths():\n            for coord in path.to_polygons():\n                if min_angle_deg:\n                    coord = keep_high_angle(coord, min_angle_deg)\n                coord = np.around(coord, ndigits) if ndigits else coord\n                op = MP(contourf.levels[contourf_idx], rgb2hex(color[0]))\n                if op in mps:\n                    for i, k in enumerate(mps):\n                        if k == op:\n                            mps[i].add_coords(coord.tolist())\n                else:\n                    op.add_coords(coord.tolist())\n                    mps.append(op)\n        contourf_idx += 1\n    # starting here the multipolys will be extracted\n    contourf_idx = 0\n    for muli in mps:\n        polygon = muli.mpoly()\n        fcolor = muli.color\n        properties = set_contourf_properties(stroke_width, fcolor, fill_opacity, contourf.levels, contourf_idx, unit)\n        if geojson_properties:\n            properties.update(geojson_properties)\n        feature = Feature(geometry=polygon, properties=properties)\n        polygon_features.append(feature)\n        contourf_idx += 1\n    feature_collection = FeatureCollection(polygon_features)\n    return _render_feature_collection(feature_collection, geojson_filepath, strdump, serialize)"}
{"func_code_string": "def get_authorize_callback(endpoint, provider_id):\n    \"\"\"Get a qualified URL for the provider to return to upon authorization\n\n    param: endpoint: Absolute path to append to the application's host\n    \"\"\"\n    endpoint_prefix = config_value('BLUEPRINT_NAME')\n    url = url_for(endpoint_prefix + '.' + endpoint, provider_id=provider_id)\n    return request.url_root[:-1] + url"}
{"func_code_string": "def delete_connection(self, **kwargs):\n        \"\"\"Remove a single connection to a provider for the specified user.\"\"\"\n        conn = self.find_connection(**kwargs)\n        if not conn:\n            return False\n        self.delete(conn)\n        return True"}
{"func_code_string": "def delete_connections(self, **kwargs):\n        \"\"\"Remove a single connection to a provider for the specified user.\"\"\"\n        rv = False\n        for c in self.find_connections(**kwargs):\n            self.delete(c)\n            rv = True\n        return rv"}
{"func_code_string": "def login(provider_id):\n    \"\"\"Starts the provider login OAuth flow\"\"\"\n    provider = get_provider_or_404(provider_id)\n    callback_url = get_authorize_callback('login', provider_id)\n    post_login = request.form.get('next', get_post_login_redirect())\n    session[config_value('POST_OAUTH_LOGIN_SESSION_KEY')] = post_login\n    return provider.authorize(callback_url)"}
{"func_code_string": "def connect(provider_id):\n    \"\"\"Starts the provider connection OAuth flow\"\"\"\n    provider = get_provider_or_404(provider_id)\n    callback_url = get_authorize_callback('connect', provider_id)\n    allow_view = get_url(config_value('CONNECT_ALLOW_VIEW'))\n    pc = request.form.get('next', allow_view)\n    session[config_value('POST_OAUTH_CONNECT_SESSION_KEY')] = pc\n    return provider.authorize(callback_url)"}
{"func_code_string": "def remove_all_connections(provider_id):\n    \"\"\"Remove all connections for the authenticated user to the\n    specified provider\n    \"\"\"\n    provider = get_provider_or_404(provider_id)\n\n    ctx = dict(provider=provider.name, user=current_user)\n\n    deleted = _datastore.delete_connections(user_id=current_user.get_id(),\n                                            provider_id=provider_id)\n    if deleted:\n        after_this_request(_commit)\n        msg = ('All connections to %s removed' % provider.name, 'info')\n        connection_removed.send(current_app._get_current_object(),\n                                user=current_user._get_current_object(),\n                                provider_id=provider_id)\n    else:\n        msg = ('Unable to remove connection to %(provider)s' % ctx, 'error')\n\n    do_flash(*msg)\n    return redirect(request.referrer)"}
{"func_code_string": "def remove_connection(provider_id, provider_user_id):\n    \"\"\"Remove a specific connection for the authenticated user to the\n    specified provider\n    \"\"\"\n    provider = get_provider_or_404(provider_id)\n\n    ctx = dict(provider=provider.name, user=current_user,\n               provider_user_id=provider_user_id)\n\n    deleted = _datastore.delete_connection(user_id=current_user.get_id(),\n                                           provider_id=provider_id,\n                                           provider_user_id=provider_user_id)\n\n    if deleted:\n        after_this_request(_commit)\n        msg = ('Connection to %(provider)s removed' % ctx, 'info')\n        connection_removed.send(current_app._get_current_object(),\n                                user=current_user._get_current_object(),\n                                provider_id=provider_id)\n    else:\n        msg = ('Unabled to remove connection to %(provider)s' % ctx, 'error')\n\n    do_flash(*msg)\n    return redirect(request.referrer or get_post_login_redirect())"}
{"func_code_string": "def connect_handler(cv, provider):\n    \"\"\"Shared method to handle the connection process\n\n    :param connection_values: A dictionary containing the connection values\n    :param provider_id: The provider ID the connection shoudl be made to\n    \"\"\"\n    cv.setdefault('user_id', current_user.get_id())\n    connection = _datastore.find_connection(\n        provider_id=cv['provider_id'], provider_user_id=cv['provider_user_id'])\n\n    if connection is None:\n        after_this_request(_commit)\n        connection = _datastore.create_connection(**cv)\n        msg = ('Connection established to %s' % provider.name, 'success')\n        connection_created.send(current_app._get_current_object(),\n                                user=current_user._get_current_object(),\n                                connection=connection)\n    else:\n        msg = ('A connection is already established with %s '\n               'to your account' % provider.name, 'notice')\n        connection_failed.send(current_app._get_current_object(),\n                               user=current_user._get_current_object())\n\n    redirect_url = session.pop(config_value('POST_OAUTH_CONNECT_SESSION_KEY'),\n                               get_url(config_value('CONNECT_ALLOW_VIEW')))\n\n    do_flash(*msg)\n    return redirect(redirect_url)"}
{"func_code_string": "def login_handler(response, provider, query):\n    \"\"\"Shared method to handle the signin process\"\"\"\n\n    connection = _datastore.find_connection(**query)\n\n    if connection:\n        after_this_request(_commit)\n        token_pair = get_token_pair_from_oauth_response(provider, response)\n        if (token_pair['access_token'] != connection.access_token or\n            token_pair['secret'] != connection.secret):\n            connection.access_token = token_pair['access_token']\n            connection.secret = token_pair['secret']\n            _datastore.put(connection)\n        user = connection.user\n        login_user(user)\n        key = _social.post_oauth_login_session_key\n        redirect_url = session.pop(key, get_post_login_redirect())\n\n        login_completed.send(current_app._get_current_object(),\n                             provider=provider, user=user)\n\n        return redirect(redirect_url)\n\n    login_failed.send(current_app._get_current_object(),\n                      provider=provider,\n                      oauth_response=response)\n\n    next = get_url(_security.login_manager.login_view)\n    msg = '%s account not associated with an existing user' % provider.name\n    do_flash(msg, 'error')\n    return redirect(next)"}
{"func_code_string": "def init_app(self, app, datastore=None):\n        \"\"\"Initialize the application with the Social extension\n\n        :param app: The Flask application\n        :param datastore: Connection datastore instance\n        \"\"\"\n\n        datastore = datastore or self.datastore\n\n        for key, value in default_config.items():\n            app.config.setdefault(key, value)\n\n        providers = dict()\n\n        for key, config in app.config.items():\n            if not key.startswith('SOCIAL_') or config is None or key in default_config:\n                continue\n\n            suffix = key.lower().replace('social_', '')\n            default_module_name = 'flask_social.providers.%s' % suffix\n            module_name = config.get('module', default_module_name)\n            module = import_module(module_name)\n            config = update_recursive(module.config, config)\n\n            providers[config['id']] = OAuthRemoteApp(**config)\n            providers[config['id']].tokengetter(_get_token)\n\n        state = _get_state(app, datastore, providers)\n\n        app.register_blueprint(create_blueprint(state, __name__))\n        app.extensions['social'] = state\n\n        return state"}
{"func_code_string": "def guess(filename, fallback='application/octet-stream'):\n    \"\"\"\n    Using the mimetypes library, guess the mimetype and\n    encoding for a given *filename*. If the mimetype\n    cannot be guessed, *fallback* is assumed instead.\n\n    :param filename: Filename- can be absolute path.\n    :param fallback: A fallback mimetype.\n    \"\"\"\n    guessed, encoding = mimetypes.guess_type(filename, strict=False)\n    if guessed is None:\n        return fallback, encoding\n    return guessed, encoding"}
{"func_code_string": "def format_addresses(addrs):\n    \"\"\"\n    Given an iterable of addresses or name-address\n    tuples *addrs*, return a header value that joins\n    all of them together with a space and a comma.\n    \"\"\"\n    return ', '.join(\n        formataddr(item) if isinstance(item, tuple) else item\n        for item in addrs\n    )"}
{"func_code_string": "def stringify_address(addr, encoding='utf-8'):\n    \"\"\"\n    Given an email address *addr*, try to encode\n    it with ASCII. If it's not possible, encode\n    the *local-part* with the *encoding* and the\n    *domain* with IDNA.\n\n    The result is a unicode string with the domain\n    encoded as idna.\n    \"\"\"\n    if isinstance(addr, bytes_type):\n        return addr\n    try:\n        addr = addr.encode('ascii')\n    except UnicodeEncodeError:\n        if '@' in addr:\n            localpart, domain = addr.split('@', 1)\n            addr = b'@'.join([\n                localpart.encode(encoding),\n                domain.encode('idna'),\n            ])\n        else:\n            addr = addr.encode(encoding)\n    return addr.decode('utf-8')"}
{"func_code_string": "def email(sender=None, receivers=(), cc=(), bcc=(),\n          subject=None, content=None, encoding='utf8',\n          attachments=()):\n    \"\"\"\n    Creates a Collection object with a HTML *content*,\n    and *attachments*.\n\n    :param content: HTML content.\n    :param encoding: Encoding of the email.\n    :param attachments: List of filenames to\n        attach to the email.\n    \"\"\"\n    enclosure = [HTML(content, encoding)]\n    enclosure.extend(Attachment(k) for k in attachments)\n    return Collection(\n        *enclosure,\n        headers=[\n            headers.subject(subject),\n            headers.sender(sender),\n            headers.to(*receivers),\n            headers.cc(*cc),\n            headers.bcc(*bcc),\n            headers.date(),\n            headers.message_id(),\n        ]\n    )"}
{"func_code_string": "def postman(host, port=587, auth=(None, None),\n            force_tls=False, options=None):\n    \"\"\"\n    Creates a Postman object with TLS and Auth\n    middleware. TLS is placed before authentication\n    because usually authentication happens and is\n    accepted only after TLS is enabled.\n\n    :param auth: Tuple of (username, password) to\n        be used to ``login`` to the server.\n    :param force_tls: Whether TLS should be forced.\n    :param options: Dictionary of keyword arguments\n        to be used when the SMTP class is called.\n    \"\"\"\n    return Postman(\n        host=host,\n        port=port,\n        middlewares=[\n            middleware.tls(force=force_tls),\n            middleware.auth(*auth),\n        ],\n        **options\n    )"}
{"func_code_string": "def mime(self):\n        \"\"\"\n        Returns the finalised mime object, after\n        applying the internal headers. Usually this\n        is not to be overriden.\n        \"\"\"\n        mime = self.mime_object()\n        self.headers.prepare(mime)\n        return mime"}
{"func_code_string": "def send(self, envelope):\n        \"\"\"\n        Send an *envelope* which may be an envelope\n        or an enclosure-like object, see\n        :class:`~mailthon.enclosure.Enclosure` and\n        :class:`~mailthon.envelope.Envelope`, and\n        returns a :class:`~mailthon.response.SendmailResponse`\n        object.\n        \"\"\"\n        rejected = self.conn.sendmail(\n            stringify_address(envelope.sender),\n            [stringify_address(k) for k in envelope.receivers],\n            envelope.string(),\n        )\n        status_code, reason = self.conn.noop()\n        return SendmailResponse(\n            status_code,\n            reason,\n            rejected,\n        )"}
{"func_code_string": "def connection(self):\n        \"\"\"\n        A context manager that returns a connection\n        to the server using some *session*.\n        \"\"\"\n        conn = self.session(**self.options)\n        try:\n            for item in self.middlewares:\n                item(conn)\n            yield conn\n        finally:\n            conn.teardown()"}
{"func_code_string": "def sender(self):\n        \"\"\"\n        Returns the sender, respecting the Resent-*\n        headers. In any case, prefer Sender over From,\n        meaning that if Sender is present then From is\n        ignored, as per the RFC.\n        \"\"\"\n        to_fetch = (\n            ['Resent-Sender', 'Resent-From'] if self.resent else\n            ['Sender', 'From']\n        )\n        for item in to_fetch:\n            if item in self:\n                _, addr = getaddresses([self[item]])[0]\n                return addr"}
{"func_code_string": "def receivers(self):\n        \"\"\"\n        Returns a list of receivers, obtained from the\n        To, Cc, and Bcc headers, respecting the Resent-*\n        headers if the email was resent.\n        \"\"\"\n        attrs = (\n            ['Resent-To', 'Resent-Cc', 'Resent-Bcc'] if self.resent else\n            ['To', 'Cc', 'Bcc']\n        )\n        addrs = (v for v in (self.get(k) for k in attrs) if v)\n        return [addr for _, addr in getaddresses(addrs)]"}
{"func_code_string": "def prepare(self, mime):\n        \"\"\"\n        Prepares a MIME object by applying the headers\n        to the *mime* object. Ignores any Bcc or\n        Resent-Bcc headers.\n        \"\"\"\n        for key in self:\n            if key == 'Bcc' or key == 'Resent-Bcc':\n                continue\n            del mime[key]\n            # Python 3.* email's compatibility layer will handle\n            # unicode field values in proper way but Python 2\n            # won't (it will encode not only additional field\n            # values but also all header values)\n            parsed_header, additional_fields = parse_header(\n                self[key] if IS_PY3 else\n                self[key].encode(\"utf-8\")\n            )\n            mime.add_header(key, parsed_header, **additional_fields)"}
{"func_code_string": "def tls(force=False):\n    \"\"\"\n    Middleware implementing TLS for SMTP connections. By\n    default this is not forced- TLS is only used if\n    STARTTLS is available. If the *force* parameter is set\n    to True, it will not query the server for TLS features\n    before upgrading to TLS.\n    \"\"\"\n    def middleware(conn):\n        if force or conn.has_extn('STARTTLS'):\n            conn.starttls()\n            conn.ehlo()\n    return middleware"}
{"func_code_string": "def auth(username, password):\n    \"\"\"\n    Middleware implementing authentication via LOGIN.\n    Most of the time this middleware needs to be placed\n    *after* TLS.\n\n    :param username: Username to login with.\n    :param password: Password of the user.\n    \"\"\"\n    def middleware(conn):\n        conn.login(username, password)\n    return middleware"}
{"func_code_string": "def get_existing_model(model_name):\n    \"\"\" Try to find existing model class named `model_name`.\n\n    :param model_name: String name of the model class.\n    \"\"\"\n    try:\n        model_cls = engine.get_document_cls(model_name)\n        log.debug('Model `{}` already exists. Using existing one'.format(\n            model_name))\n        return model_cls\n    except ValueError:\n        log.debug('Model `{}` does not exist'.format(model_name))"}
{"func_code_string": "def prepare_relationship(config, model_name, raml_resource):\n    \"\"\" Create referenced model if it doesn't exist.\n\n    When preparing a relationship, we check to see if the model that will be\n    referenced already exists. If not, it is created so that it will be possible\n    to use it in a relationship. Thus the first usage of this model in RAML file\n    must provide its schema in POST method resource body schema.\n\n    :param model_name: Name of model which should be generated.\n    :param raml_resource: Instance of ramlfications.raml.ResourceNode for\n        which :model_name: will be defined.\n    \"\"\"\n    if get_existing_model(model_name) is None:\n        plural_route = '/' + pluralize(model_name.lower())\n        route = '/' + model_name.lower()\n        for res in raml_resource.root.resources:\n            if res.method.upper() != 'POST':\n                continue\n            if res.path.endswith(plural_route) or res.path.endswith(route):\n                break\n        else:\n            raise ValueError('Model `{}` used in relationship is not '\n                             'defined'.format(model_name))\n        setup_data_model(config, res, model_name)"}
{"func_code_string": "def generate_model_cls(config, schema, model_name, raml_resource,\n                       es_based=True):\n    \"\"\" Generate model class.\n\n    Engine DB field types are determined using `type_fields` and only those\n    types may be used.\n\n    :param schema: Model schema dict parsed from RAML.\n    :param model_name: String that is used as new model's name.\n    :param raml_resource: Instance of ramlfications.raml.ResourceNode.\n    :param es_based: Boolean indicating if generated model should be a\n        subclass of Elasticsearch-based document class or not.\n        It True, ESBaseDocument is used; BaseDocument is used otherwise.\n        Defaults to True.\n    \"\"\"\n    from nefertari.authentication.models import AuthModelMethodsMixin\n    base_cls = engine.ESBaseDocument if es_based else engine.BaseDocument\n    model_name = str(model_name)\n    metaclass = type(base_cls)\n    auth_model = schema.get('_auth_model', False)\n\n    bases = []\n    if config.registry.database_acls:\n        from nefertari_guards import engine as guards_engine\n        bases.append(guards_engine.DocumentACLMixin)\n    if auth_model:\n        bases.append(AuthModelMethodsMixin)\n    bases.append(base_cls)\n\n    attrs = {\n        '__tablename__': model_name.lower(),\n        '_public_fields': schema.get('_public_fields') or [],\n        '_auth_fields': schema.get('_auth_fields') or [],\n        '_hidden_fields': schema.get('_hidden_fields') or [],\n        '_nested_relationships': schema.get('_nested_relationships') or [],\n    }\n    if '_nesting_depth' in schema:\n        attrs['_nesting_depth'] = schema.get('_nesting_depth')\n\n    # Generate fields from properties\n    properties = schema.get('properties', {})\n    for field_name, props in properties.items():\n        if field_name in attrs:\n            continue\n\n        db_settings = props.get('_db_settings')\n        if db_settings is None:\n            continue\n\n        field_kwargs = db_settings.copy()\n        field_kwargs['required'] = bool(field_kwargs.get('required'))\n\n        for default_attr_key in ('default', 'onupdate'):\n            value = field_kwargs.get(default_attr_key)\n            if is_callable_tag(value):\n                field_kwargs[default_attr_key] = resolve_to_callable(value)\n\n        type_name = (\n            field_kwargs.pop('type', 'string') or 'string').lower()\n        if type_name not in type_fields:\n            raise ValueError('Unknown type: {}'.format(type_name))\n\n        field_cls = type_fields[type_name]\n\n        if field_cls is engine.Relationship:\n            prepare_relationship(\n                config, field_kwargs['document'],\n                raml_resource)\n        if field_cls is engine.ForeignKeyField:\n            key = 'ref_column_type'\n            field_kwargs[key] = type_fields[field_kwargs[key]]\n        if field_cls is engine.ListField:\n            key = 'item_type'\n            field_kwargs[key] = type_fields[field_kwargs[key]]\n\n        attrs[field_name] = field_cls(**field_kwargs)\n\n    # Update model definition with methods and variables defined in registry\n    attrs.update(registry.mget(model_name))\n\n    # Generate new model class\n    model_cls = metaclass(model_name, tuple(bases), attrs)\n    setup_model_event_subscribers(config, model_cls, schema)\n    setup_fields_processors(config, model_cls, schema)\n    return model_cls, auth_model"}
{"func_code_string": "def setup_data_model(config, raml_resource, model_name):\n    \"\"\" Setup storage/data model and return generated model class.\n\n    Process follows these steps:\n      * Resource schema is found and restructured by `resource_schema`.\n      * Model class is generated from properties dict using util function\n        `generate_model_cls`.\n\n    :param raml_resource: Instance of ramlfications.raml.ResourceNode.\n    :param model_name: String representing model name.\n    \"\"\"\n    model_cls = get_existing_model(model_name)\n    schema = resource_schema(raml_resource)\n\n    if not schema:\n        raise Exception('Missing schema for model `{}`'.format(model_name))\n\n    if model_cls is not None:\n        return model_cls, schema.get('_auth_model', False)\n\n    log.info('Generating model class `{}`'.format(model_name))\n    return generate_model_cls(\n        config,\n        schema=schema,\n        model_name=model_name,\n        raml_resource=raml_resource,\n    )"}
{"func_code_string": "def handle_model_generation(config, raml_resource):\n    \"\"\" Generates model name and runs `setup_data_model` to get\n    or generate actual model class.\n\n    :param raml_resource: Instance of ramlfications.raml.ResourceNode.\n    \"\"\"\n    model_name = generate_model_name(raml_resource)\n    try:\n        return setup_data_model(config, raml_resource, model_name)\n    except ValueError as ex:\n        raise ValueError('{}: {}'.format(model_name, str(ex)))"}
{"func_code_string": "def setup_model_event_subscribers(config, model_cls, schema):\n    \"\"\" Set up model event subscribers.\n\n    :param config: Pyramid Configurator instance.\n    :param model_cls: Model class for which handlers should be connected.\n    :param schema: Dict of model JSON schema.\n    \"\"\"\n    events_map = get_events_map()\n    model_events = schema.get('_event_handlers', {})\n    event_kwargs = {'model': model_cls}\n\n    for event_tag, subscribers in model_events.items():\n        type_, action = event_tag.split('_')\n        event_objects = events_map[type_][action]\n\n        if not isinstance(event_objects, list):\n            event_objects = [event_objects]\n\n        for sub_name in subscribers:\n            sub_func = resolve_to_callable(sub_name)\n            config.subscribe_to_events(\n                sub_func, event_objects, **event_kwargs)"}
{"func_code_string": "def setup_fields_processors(config, model_cls, schema):\n    \"\"\" Set up model fields' processors.\n\n    :param config: Pyramid Configurator instance.\n    :param model_cls: Model class for field of which processors should be\n        set up.\n    :param schema: Dict of model JSON schema.\n    \"\"\"\n    properties = schema.get('properties', {})\n    for field_name, props in properties.items():\n        if not props:\n            continue\n\n        processors = props.get('_processors')\n        backref_processors = props.get('_backref_processors')\n\n        if processors:\n            processors = [resolve_to_callable(val) for val in processors]\n            setup_kwargs = {'model': model_cls, 'field': field_name}\n            config.add_field_processors(processors, **setup_kwargs)\n\n        if backref_processors:\n            db_settings = props.get('_db_settings', {})\n            is_relationship = db_settings.get('type') == 'relationship'\n            document = db_settings.get('document')\n            backref_name = db_settings.get('backref_name')\n            if not (is_relationship and document and backref_name):\n                continue\n\n            backref_processors = [\n                resolve_to_callable(val) for val in backref_processors]\n            setup_kwargs = {\n                'model': engine.get_document_cls(document),\n                'field': backref_name\n            }\n            config.add_field_processors(\n                backref_processors, **setup_kwargs)"}
{"func_code_string": "def _setup_ticket_policy(config, params):\n    \"\"\" Setup Pyramid AuthTktAuthenticationPolicy.\n\n    Notes:\n      * Initial `secret` params value is considered to be a name of config\n        param that represents a cookie name.\n      * `auth_model.get_groups_by_userid` is used as a `callback`.\n      * Also connects basic routes to perform authentication actions.\n\n    :param config: Pyramid Configurator instance.\n    :param params: Nefertari dictset which contains security scheme\n        `settings`.\n    \"\"\"\n    from nefertari.authentication.views import (\n        TicketAuthRegisterView, TicketAuthLoginView,\n        TicketAuthLogoutView)\n\n    log.info('Configuring Pyramid Ticket Authn policy')\n    if 'secret' not in params:\n        raise ValueError(\n            'Missing required security scheme settings: secret')\n    params['secret'] = config.registry.settings[params['secret']]\n\n    auth_model = config.registry.auth_model\n    params['callback'] = auth_model.get_groups_by_userid\n\n    config.add_request_method(\n        auth_model.get_authuser_by_userid, 'user', reify=True)\n\n    policy = AuthTktAuthenticationPolicy(**params)\n\n    RegisterViewBase = TicketAuthRegisterView\n    if config.registry.database_acls:\n        class RegisterViewBase(ACLAssignRegisterMixin,\n                               TicketAuthRegisterView):\n            pass\n\n    class RamsesTicketAuthRegisterView(RegisterViewBase):\n        Model = config.registry.auth_model\n\n    class RamsesTicketAuthLoginView(TicketAuthLoginView):\n        Model = config.registry.auth_model\n\n    class RamsesTicketAuthLogoutView(TicketAuthLogoutView):\n        Model = config.registry.auth_model\n\n    common_kw = {\n        'prefix': 'auth',\n        'factory': 'nefertari.acl.AuthenticationACL',\n    }\n\n    root = config.get_root_resource()\n    root.add('register', view=RamsesTicketAuthRegisterView, **common_kw)\n    root.add('login', view=RamsesTicketAuthLoginView, **common_kw)\n    root.add('logout', view=RamsesTicketAuthLogoutView, **common_kw)\n\n    return policy"}
{"func_code_string": "def _setup_apikey_policy(config, params):\n    \"\"\" Setup `nefertari.ApiKeyAuthenticationPolicy`.\n\n    Notes:\n      * User may provide model name in :params['user_model']: do define\n        the name of the user model.\n      * `auth_model.get_groups_by_token` is used to perform username and\n        token check\n      * `auth_model.get_token_credentials` is used to get username and\n        token from userid\n      * Also connects basic routes to perform authentication actions.\n\n    Arguments:\n        :config: Pyramid Configurator instance.\n        :params: Nefertari dictset which contains security scheme `settings`.\n    \"\"\"\n    from nefertari.authentication.views import (\n        TokenAuthRegisterView, TokenAuthClaimView,\n        TokenAuthResetView)\n    log.info('Configuring ApiKey Authn policy')\n\n    auth_model = config.registry.auth_model\n    params['check'] = auth_model.get_groups_by_token\n    params['credentials_callback'] = auth_model.get_token_credentials\n    params['user_model'] = auth_model\n    config.add_request_method(\n        auth_model.get_authuser_by_name, 'user', reify=True)\n\n    policy = ApiKeyAuthenticationPolicy(**params)\n\n    RegisterViewBase = TokenAuthRegisterView\n    if config.registry.database_acls:\n        class RegisterViewBase(ACLAssignRegisterMixin,\n                               TokenAuthRegisterView):\n            pass\n\n    class RamsesTokenAuthRegisterView(RegisterViewBase):\n        Model = auth_model\n\n    class RamsesTokenAuthClaimView(TokenAuthClaimView):\n        Model = auth_model\n\n    class RamsesTokenAuthResetView(TokenAuthResetView):\n        Model = auth_model\n\n    common_kw = {\n        'prefix': 'auth',\n        'factory': 'nefertari.acl.AuthenticationACL',\n    }\n\n    root = config.get_root_resource()\n    root.add('register', view=RamsesTokenAuthRegisterView, **common_kw)\n    root.add('token', view=RamsesTokenAuthClaimView, **common_kw)\n    root.add('reset_token', view=RamsesTokenAuthResetView, **common_kw)\n\n    return policy"}
{"func_code_string": "def setup_auth_policies(config, raml_root):\n    \"\"\" Setup authentication, authorization policies.\n\n    Performs basic validation to check all the required values are present\n    and performs authentication, authorization policies generation using\n    generator functions from `AUTHENTICATION_POLICIES`.\n\n    :param config: Pyramid Configurator instance.\n    :param raml_root: Instance of ramlfications.raml.RootNode.\n    \"\"\"\n    log.info('Configuring auth policies')\n    secured_by_all = raml_root.secured_by or []\n    secured_by = [item for item in secured_by_all if item]\n    if not secured_by:\n        log.info('API is not secured. `secured_by` attribute '\n                 'value missing.')\n        return\n    secured_by = secured_by[0]\n\n    schemes = {scheme.name: scheme\n               for scheme in raml_root.security_schemes}\n    if secured_by not in schemes:\n        raise ValueError(\n            'Undefined security scheme used in `secured_by`: {}'.format(\n                secured_by))\n\n    scheme = schemes[secured_by]\n    if scheme.type not in AUTHENTICATION_POLICIES:\n        raise ValueError('Unsupported security scheme type: {}'.format(\n            scheme.type))\n\n    # Setup Authentication policy\n    policy_generator = AUTHENTICATION_POLICIES[scheme.type]\n    params = dictset(scheme.settings or {})\n    authn_policy = policy_generator(config, params)\n    config.set_authentication_policy(authn_policy)\n\n    # Setup Authorization policy\n    authz_policy = ACLAuthorizationPolicy()\n    config.set_authorization_policy(authz_policy)"}
{"func_code_string": "def get_authuser_model():\n    \"\"\" Define and return AuthUser model using nefertari base classes \"\"\"\n    from nefertari.authentication.models import AuthUserMixin\n    from nefertari import engine\n\n    class AuthUser(AuthUserMixin, engine.BaseDocument):\n        __tablename__ = 'ramses_authuser'\n\n    return AuthUser"}
{"func_code_string": "def validate_permissions(perms):\n    \"\"\" Validate :perms: contains valid permissions.\n\n    :param perms: List of permission names or ALL_PERMISSIONS.\n    \"\"\"\n    if not isinstance(perms, (list, tuple)):\n        perms = [perms]\n    valid_perms = set(PERMISSIONS.values())\n    if ALL_PERMISSIONS in perms:\n        return perms\n    if set(perms) - valid_perms:\n        raise ValueError(\n            'Invalid ACL permission names. Valid permissions '\n            'are: {}'.format(', '.join(valid_perms)))\n    return perms"}
{"func_code_string": "def parse_permissions(perms):\n    \"\"\" Parse permissions (\"perms\") which are either exact permission\n    names or the keyword 'all'.\n\n    :param perms: List or comma-separated string of nefertari permission\n        names, or 'all'\n    \"\"\"\n    if isinstance(perms, six.string_types):\n        perms = perms.split(',')\n    perms = [perm.strip().lower() for perm in perms]\n    if 'all' in perms:\n        return ALL_PERMISSIONS\n    return validate_permissions(perms)"}
{"func_code_string": "def parse_acl(acl_string):\n    \"\"\" Parse raw string :acl_string: of RAML-defined ACLs.\n\n    If :acl_string: is blank or None, all permissions are given.\n    Values of ACL action and principal are parsed using `actions` and\n    `special_principals` maps and are looked up after `strip()` and\n    `lower()`.\n\n    ACEs in :acl_string: may be separated by newlines or semicolons.\n    Action, principal and permission lists must be separated by spaces.\n    Permissions must be comma-separated.\n    E.g. 'allow everyone view,create,update' and 'deny authenticated delete'\n\n    :param acl_string: Raw RAML string containing defined ACEs.\n    \"\"\"\n    if not acl_string:\n        return [ALLOW_ALL]\n\n    aces_list = acl_string.replace('\\n', ';').split(';')\n    aces_list = [ace.strip().split(' ', 2) for ace in aces_list if ace]\n    aces_list = [(a, b, c.split(',')) for a, b, c in aces_list]\n    result_acl = []\n\n    for action_str, princ_str, perms in aces_list:\n        # Process action\n        action_str = action_str.strip().lower()\n        action = actions.get(action_str)\n        if action is None:\n            raise ValueError(\n                'Unknown ACL action: {}. Valid actions: {}'.format(\n                    action_str, list(actions.keys())))\n\n        # Process principal\n        princ_str = princ_str.strip().lower()\n        if princ_str in special_principals:\n            principal = special_principals[princ_str]\n        elif is_callable_tag(princ_str):\n            principal = resolve_to_callable(princ_str)\n        else:\n            principal = princ_str\n\n        # Process permissions\n        permissions = parse_permissions(perms)\n\n        result_acl.append((action, principal, permissions))\n\n    return result_acl"}
{"func_code_string": "def generate_acl(config, model_cls, raml_resource, es_based=True):\n    \"\"\" Generate an ACL.\n\n    Generated ACL class has a `item_model` attribute set to\n    :model_cls:.\n\n    ACLs used for collection and item access control are generated from a\n    first security scheme with type `x-ACL`.\n    If :raml_resource: has no x-ACL security schemes defined then ALLOW_ALL\n    ACL is used.\n    If the `collection` or `item` settings are empty, then ALLOW_ALL ACL\n    is used.\n\n    :param model_cls: Generated model class\n    :param raml_resource: Instance of ramlfications.raml.ResourceNode\n        for which ACL is being generated\n    :param es_based: Boolean inidicating whether ACL should query ES or\n        not when getting an object\n    \"\"\"\n    schemes = raml_resource.security_schemes or []\n    schemes = [sch for sch in schemes if sch.type == 'x-ACL']\n\n    if not schemes:\n        collection_acl = item_acl = []\n        log.debug('No ACL scheme applied. Using ACL: {}'.format(item_acl))\n    else:\n        sec_scheme = schemes[0]\n        log.debug('{} ACL scheme applied'.format(sec_scheme.name))\n        settings = sec_scheme.settings or {}\n        collection_acl = parse_acl(acl_string=settings.get('collection'))\n        item_acl = parse_acl(acl_string=settings.get('item'))\n\n    class GeneratedACLBase(object):\n        item_model = model_cls\n\n        def __init__(self, request, es_based=es_based):\n            super(GeneratedACLBase, self).__init__(request=request)\n            self.es_based = es_based\n            self._collection_acl = collection_acl\n            self._item_acl = item_acl\n\n    bases = [GeneratedACLBase]\n    if config.registry.database_acls:\n        from nefertari_guards.acl import DatabaseACLMixin as GuardsMixin\n        bases += [DatabaseACLMixin, GuardsMixin]\n    bases.append(BaseACL)\n\n    return type('GeneratedACL', tuple(bases), {})"}
{"func_code_string": "def _apply_callables(self, acl, obj=None):\n        \"\"\" Iterate over ACEs from :acl: and apply callable principals\n        if any.\n\n        Principals are passed 3 arguments on call:\n            :ace: Single ACE object that looks like (action, callable,\n                permission or [permission])\n            :request: Current request object\n            :obj: Object instance to be accessed via the ACL\n        Principals must return a single ACE or a list of ACEs.\n\n        :param acl: Sequence of valid Pyramid ACEs which will be processed\n        :param obj: Object to be accessed via the ACL\n        \"\"\"\n        new_acl = []\n        for i, ace in enumerate(acl):\n            principal = ace[1]\n            if six.callable(principal):\n                ace = principal(ace=ace, request=self.request, obj=obj)\n                if not ace:\n                    continue\n                if not isinstance(ace[0], (list, tuple)):\n                    ace = [ace]\n                ace = [(a, b, validate_permissions(c)) for a, b, c in ace]\n            else:\n                ace = [ace]\n            new_acl += ace\n        return tuple(new_acl)"}
{"func_code_string": "def item_acl(self, item):\n        \"\"\" Objectify ACL if ES is used or call item.get_acl() if\n        db is used.\n        \"\"\"\n        if self.es_based:\n            from nefertari_guards.elasticsearch import get_es_item_acl\n            return get_es_item_acl(item)\n        return super(DatabaseACLMixin, self).item_acl(item)"}
{"func_code_string": "def getitem_es(self, key):\n        \"\"\" Override to support ACL filtering.\n\n        To do so: passes `self.request` to `get_item` and uses\n        `ACLFilterES`.\n        \"\"\"\n        from nefertari_guards.elasticsearch import ACLFilterES\n        es = ACLFilterES(self.item_model.__name__)\n        params = {\n            'id': key,\n            'request': self.request,\n        }\n        obj = es.get_item(**params)\n        obj.__acl__ = self.item_acl(obj)\n        obj.__parent__ = self\n        obj.__name__ = key\n        return obj"}
{"func_code_string": "def convert_schema(raml_schema, mime_type):\n    \"\"\" Restructure `raml_schema` to a dictionary that has 'properties'\n    as well as other schema keys/values.\n\n    The resulting dictionary looks like this::\n\n    {\n        \"properties\": {\n            \"field1\": {\n                \"required\": boolean,\n                \"type\": ...,\n                ...more field options\n            },\n            ...more properties\n        },\n        \"public_fields\": [...],\n        \"auth_fields\": [...],\n        ...more schema options\n    }\n\n    :param raml_schema: RAML request body schema.\n    :param mime_type: ContentType of the schema as a string from RAML\n        file. Only JSON is currently supported.\n    \"\"\"\n    if mime_type == ContentTypes.JSON:\n        if not isinstance(raml_schema, dict):\n            raise TypeError(\n                'Schema is not a valid JSON. Please check your '\n                'schema syntax.\\n{}...'.format(str(raml_schema)[:60]))\n        return raml_schema\n    if mime_type == ContentTypes.TEXT_XML:\n        # Process XML schema\n        pass"}
{"func_code_string": "def generate_model_name(raml_resource):\n    \"\"\" Generate model name.\n\n    :param raml_resource: Instance of ramlfications.raml.ResourceNode.\n    \"\"\"\n    resource_uri = get_resource_uri(raml_resource).strip('/')\n    resource_uri = re.sub('\\W', ' ', resource_uri)\n    model_name = inflection.titleize(resource_uri)\n    return inflection.singularize(model_name).replace(' ', '')"}
{"func_code_string": "def dynamic_part_name(raml_resource, route_name, pk_field):\n    \"\"\" Generate a dynamic part for a resource :raml_resource:.\n\n    A dynamic part is generated using 2 parts: :route_name: of the\n    resource and the dynamic part of first dynamic child resources. If\n    :raml_resource: has no dynamic child resources, 'id' is used as the\n    2nd part.\n    E.g. if your dynamic part on route 'stories' is named 'superId' then\n    dynamic part will be 'stories_superId'.\n\n    :param raml_resource: Instance of ramlfications.raml.ResourceNode for\n        which dynamic part name is being generated.\n    :param route_name: Cleaned name of :raml_resource:\n    :param pk_field: Model Primary Key field name.\n    \"\"\"\n    subresources = get_resource_children(raml_resource)\n    dynamic_uris = [res.path for res in subresources\n                    if is_dynamic_uri(res.path)]\n    if dynamic_uris:\n        dynamic_part = extract_dynamic_part(dynamic_uris[0])\n    else:\n        dynamic_part = pk_field\n    return '_'.join([route_name, dynamic_part])"}
{"func_code_string": "def extract_dynamic_part(uri):\n    \"\"\" Extract dynamic url part from :uri: string.\n\n    :param uri: URI string that may contain dynamic part.\n    \"\"\"\n    for part in uri.split('/'):\n        part = part.strip()\n        if part.startswith('{') and part.endswith('}'):\n            return clean_dynamic_uri(part)"}
{"func_code_string": "def resource_view_attrs(raml_resource, singular=False):\n    \"\"\" Generate view method names needed for `raml_resource` view.\n\n    Collects HTTP method names from resource siblings and dynamic children\n    if exist. Collected methods are then translated  to\n    `nefertari.view.BaseView` method names, each of which is used to\n    process a particular HTTP method request.\n\n    Maps of {HTTP_method: view_method} `collection_methods` and\n    `item_methods` are used to convert collection and item methods\n    respectively.\n\n    :param raml_resource: Instance of ramlfications.raml.ResourceNode\n    :param singular: Boolean indicating if resource is singular or not\n    \"\"\"\n    from .views import collection_methods, item_methods\n    # Singular resource doesn't have collection methods though\n    # it looks like a collection\n    if singular:\n        collection_methods = item_methods\n\n    siblings = get_resource_siblings(raml_resource)\n    http_methods = [sibl.method.lower() for sibl in siblings]\n    attrs = [collection_methods.get(method) for method in http_methods]\n\n    # Check if resource has dynamic child resource like collection/{id}\n    # If dynamic child resource exists, add its siblings' methods to attrs,\n    # as both resources are handled by a single view\n    children = get_resource_children(raml_resource)\n    http_submethods = [child.method.lower() for child in children\n                       if is_dynamic_uri(child.path)]\n    attrs += [item_methods.get(method) for method in http_submethods]\n\n    return set(filter(bool, attrs))"}
{"func_code_string": "def resource_schema(raml_resource):\n    \"\"\" Get schema properties of RAML resource :raml_resource:.\n\n    Must be called with RAML resource that defines body schema. First\n    body that defines schema is used. Schema is converted on return using\n    'convert_schema'.\n\n    :param raml_resource: Instance of ramlfications.raml.ResourceNode of\n        POST method.\n    \"\"\"\n    # NOTE: Must be called with resource that defines body schema\n    log.info('Searching for model schema')\n    if not raml_resource.body:\n        raise ValueError('RAML resource has no body to setup database '\n                         'schema from')\n\n    for body in raml_resource.body:\n        if body.schema:\n            return convert_schema(body.schema, body.mime_type)\n    log.debug('No model schema found.')"}
{"func_code_string": "def get_static_parent(raml_resource, method=None):\n    \"\"\" Get static parent resource of :raml_resource: with HTTP\n    method :method:.\n\n    :param raml_resource:Instance of ramlfications.raml.ResourceNode.\n    :param method: HTTP method name which matching static resource\n        must have.\n    \"\"\"\n    parent = raml_resource.parent\n    while is_dynamic_resource(parent):\n        parent = parent.parent\n\n    if parent is None:\n        return parent\n\n    match_method = method is not None\n    if match_method:\n        if parent.method.upper() == method.upper():\n            return parent\n    else:\n        return parent\n\n    for res in parent.root.resources:\n        if res.path == parent.path:\n            if res.method.upper() == method.upper():\n                return res"}
{"func_code_string": "def attr_subresource(raml_resource, route_name):\n    \"\"\" Determine if :raml_resource: is an attribute subresource.\n\n    :param raml_resource: Instance of ramlfications.raml.ResourceNode.\n    :param route_name: Name of the :raml_resource:.\n    \"\"\"\n    static_parent = get_static_parent(raml_resource, method='POST')\n    if static_parent is None:\n        return False\n    schema = resource_schema(static_parent) or {}\n    properties = schema.get('properties', {})\n    if route_name in properties:\n        db_settings = properties[route_name].get('_db_settings', {})\n        return db_settings.get('type') in ('dict', 'list')\n    return False"}
{"func_code_string": "def singular_subresource(raml_resource, route_name):\n    \"\"\" Determine if :raml_resource: is a singular subresource.\n\n    :param raml_resource: Instance of ramlfications.raml.ResourceNode.\n    :param route_name: Name of the :raml_resource:.\n    \"\"\"\n    static_parent = get_static_parent(raml_resource, method='POST')\n    if static_parent is None:\n        return False\n    schema = resource_schema(static_parent) or {}\n    properties = schema.get('properties', {})\n    if route_name not in properties:\n        return False\n\n    db_settings = properties[route_name].get('_db_settings', {})\n    is_obj = db_settings.get('type') == 'relationship'\n    single_obj = not db_settings.get('uselist', True)\n    return is_obj and single_obj"}
{"func_code_string": "def is_callable_tag(tag):\n    \"\"\" Determine whether :tag: is a valid callable string tag.\n\n    String is assumed to be valid callable if it starts with '{{'\n    and ends with '}}'.\n\n    :param tag: String name of tag.\n    \"\"\"\n    return (isinstance(tag, six.string_types) and\n            tag.strip().startswith('{{') and\n            tag.strip().endswith('}}'))"}
{"func_code_string": "def resolve_to_callable(callable_name):\n    \"\"\" Resolve string :callable_name: to a callable.\n\n    :param callable_name: String representing callable name as registered\n        in ramses registry or dotted import path of callable. Can be\n        wrapped in double curly brackets, e.g. '{{my_callable}}'.\n    \"\"\"\n    from . import registry\n    clean_callable_name = callable_name.replace(\n        '{{', '').replace('}}', '').strip()\n    try:\n        return registry.get(clean_callable_name)\n    except KeyError:\n        try:\n            from zope.dottedname.resolve import resolve\n            return resolve(clean_callable_name)\n        except ImportError:\n            raise ImportError(\n                'Failed to load callable `{}`'.format(clean_callable_name))"}
{"func_code_string": "def get_resource_siblings(raml_resource):\n    \"\"\" Get siblings of :raml_resource:.\n\n    :param raml_resource: Instance of ramlfications.raml.ResourceNode.\n    \"\"\"\n    path = raml_resource.path\n    return [res for res in raml_resource.root.resources\n            if res.path == path]"}
{"func_code_string": "def get_resource_children(raml_resource):\n    \"\"\" Get children of :raml_resource:.\n\n    :param raml_resource: Instance of ramlfications.raml.ResourceNode.\n    \"\"\"\n    path = raml_resource.path\n    return [res for res in raml_resource.root.resources\n            if res.parent and res.parent.path == path]"}
{"func_code_string": "def get_events_map():\n    \"\"\" Prepare map of event subscribers.\n\n    * Extends copies of BEFORE_EVENTS and AFTER_EVENTS maps with\n        'set' action.\n    * Returns map of {before/after: {action: event class(es)}}\n    \"\"\"\n    from nefertari import events\n    set_keys = ('create', 'update', 'replace', 'update_many', 'register')\n    before_events = events.BEFORE_EVENTS.copy()\n    before_events['set'] = [before_events[key] for key in set_keys]\n    after_events = events.AFTER_EVENTS.copy()\n    after_events['set'] = [after_events[key] for key in set_keys]\n    return {\n        'before': before_events,\n        'after': after_events,\n    }"}
{"func_code_string": "def patch_view_model(view_cls, model_cls):\n    \"\"\" Patches view_cls.Model with model_cls.\n\n    :param view_cls: View class \"Model\" param of which should be\n        patched\n    :param model_cls: Model class which should be used to patch\n        view_cls.Model\n    \"\"\"\n    original_model = view_cls.Model\n    view_cls.Model = model_cls\n\n    try:\n        yield\n    finally:\n        view_cls.Model = original_model"}
{"func_code_string": "def get_route_name(resource_uri):\n    \"\"\" Get route name from RAML resource URI.\n\n    :param resource_uri: String representing RAML resource URI.\n    :returns string: String with route name, which is :resource_uri:\n        stripped of non-word characters.\n    \"\"\"\n    resource_uri = resource_uri.strip('/')\n    resource_uri = re.sub('\\W', '', resource_uri)\n    return resource_uri"}
{"func_code_string": "def generate_resource(config, raml_resource, parent_resource):\n    \"\"\" Perform complete one resource configuration process\n\n    This function generates: ACL, view, route, resource, database\n    model for a given `raml_resource`. New nefertari resource is\n    attached to `parent_resource` class which is an instance of\n    `nefertari.resource.Resource`.\n\n    Things to consider:\n      * Top-level resources must be collection names.\n      * No resources are explicitly created for dynamic (ending with '}')\n        RAML resources as they are implicitly processed by parent collection\n        resources.\n      * Resource nesting must look like collection/id/collection/id/...\n      * Only part of resource path after last '/' is taken into account,\n        thus each level of resource nesting should add one more path\n        element. E.g. /stories -> /stories/{id} and not\n        /stories -> /stories/mystories/{id}. Latter route will be generated\n        at /stories/{id}.\n\n    :param raml_resource: Instance of ramlfications.raml.ResourceNode.\n    :param parent_resource: Parent nefertari resource object.\n    \"\"\"\n    from .models import get_existing_model\n\n    # Don't generate resources for dynamic routes as they are already\n    # generated by their parent\n    resource_uri = get_resource_uri(raml_resource)\n    if is_dynamic_uri(resource_uri):\n        if parent_resource.is_root:\n            raise Exception(\"Top-level resources can't be dynamic and must \"\n                            \"represent collections instead\")\n        return\n\n    route_name = get_route_name(resource_uri)\n    log.info('Configuring resource: `{}`. Parent: `{}`'.format(\n        route_name, parent_resource.uid or 'root'))\n\n    # Get DB model. If this is an attribute or singular resource,\n    # we don't need to get model\n    is_singular = singular_subresource(raml_resource, route_name)\n    is_attr_res = attr_subresource(raml_resource, route_name)\n    if not parent_resource.is_root and (is_attr_res or is_singular):\n        model_cls = parent_resource.view.Model\n    else:\n        model_name = generate_model_name(raml_resource)\n        model_cls = get_existing_model(model_name)\n\n    resource_kwargs = {}\n\n    # Generate ACL\n    log.info('Generating ACL for `{}`'.format(route_name))\n    resource_kwargs['factory'] = generate_acl(\n        config,\n        model_cls=model_cls,\n        raml_resource=raml_resource)\n\n    # Generate dynamic part name\n    if not is_singular:\n        resource_kwargs['id_name'] = dynamic_part_name(\n            raml_resource=raml_resource,\n            route_name=route_name,\n            pk_field=model_cls.pk_field())\n\n    # Generate REST view\n    log.info('Generating view for `{}`'.format(route_name))\n    view_attrs = resource_view_attrs(raml_resource, is_singular)\n    resource_kwargs['view'] = generate_rest_view(\n        config,\n        model_cls=model_cls,\n        attrs=view_attrs,\n        attr_view=is_attr_res,\n        singular=is_singular,\n    )\n\n    # In case of singular resource, model still needs to be generated,\n    # but we store it on a different view attribute\n    if is_singular:\n        model_name = generate_model_name(raml_resource)\n        view_cls = resource_kwargs['view']\n        view_cls._parent_model = view_cls.Model\n        view_cls.Model = get_existing_model(model_name)\n\n    # Create new nefertari resource\n    log.info('Creating new resource for `{}`'.format(route_name))\n    clean_uri = resource_uri.strip('/')\n    resource_args = (singularize(clean_uri),)\n    if not is_singular:\n        resource_args += (clean_uri,)\n\n    return parent_resource.add(*resource_args, **resource_kwargs)"}
{"func_code_string": "def generate_server(raml_root, config):\n    \"\"\" Handle server generation process.\n\n    :param raml_root: Instance of ramlfications.raml.RootNode.\n    :param config: Pyramid Configurator instance.\n    \"\"\"\n    log.info('Server generation started')\n\n    if not raml_root.resources:\n        return\n\n    root_resource = config.get_root_resource()\n    generated_resources = {}\n\n    for raml_resource in raml_root.resources:\n        if raml_resource.path in generated_resources:\n            continue\n\n        # Get Nefertari parent resource\n        parent_resource = _get_nefertari_parent_resource(\n            raml_resource, generated_resources, root_resource)\n\n        # Get generated resource and store it\n        new_resource = generate_resource(\n            config, raml_resource, parent_resource)\n        if new_resource is not None:\n            generated_resources[raml_resource.path] = new_resource"}
{"func_code_string": "def generate_models(config, raml_resources):\n    \"\"\" Generate model for each resource in :raml_resources:\n\n    The DB model name is generated using singular titled version of current\n    resource's url. E.g. for resource under url '/stories', model with\n    name 'Story' will be generated.\n\n    :param config: Pyramid Configurator instance.\n    :param raml_resources: List of ramlfications.raml.ResourceNode.\n    \"\"\"\n    from .models import handle_model_generation\n    if not raml_resources:\n        return\n    for raml_resource in raml_resources:\n        # No need to generate models for dynamic resource\n        if is_dynamic_uri(raml_resource.path):\n            continue\n\n        # Since POST resource must define schema use only POST\n        # resources to generate models\n        if raml_resource.method.upper() != 'POST':\n            continue\n\n        # Generate DB model\n        # If this is an attribute resource we don't need to generate model\n        resource_uri = get_resource_uri(raml_resource)\n        route_name = get_route_name(resource_uri)\n        if not attr_subresource(raml_resource, route_name):\n            log.info('Configuring model for route `{}`'.format(route_name))\n            model_cls, is_auth_model = handle_model_generation(\n                config, raml_resource)\n            if is_auth_model:\n                config.registry.auth_model = model_cls"}
{"func_code_string": "def generate_rest_view(config, model_cls, attrs=None, es_based=True,\n                       attr_view=False, singular=False):\n    \"\"\" Generate REST view for a model class.\n\n    :param model_cls: Generated DB model class.\n    :param attr: List of strings that represent names of view methods, new\n        generated view should support. Not supported methods are replaced\n        with property that raises AttributeError to display MethodNotAllowed\n        error.\n    :param es_based: Boolean indicating if generated view should read from\n        elasticsearch. If True - collection reads are performed from\n        elasticsearch. Database is used for reads otherwise.\n        Defaults to True.\n    :param attr_view: Boolean indicating if ItemAttributeView should be\n        used as a base class for generated view.\n    :param singular: Boolean indicating if ItemSingularView should be\n        used as a base class for generated view.\n    \"\"\"\n    valid_attrs = (list(collection_methods.values()) +\n                   list(item_methods.values()))\n    missing_attrs = set(valid_attrs) - set(attrs)\n\n    if singular:\n        bases = [ItemSingularView]\n    elif attr_view:\n        bases = [ItemAttributeView]\n    elif es_based:\n        bases = [ESCollectionView]\n    else:\n        bases = [CollectionView]\n\n    if config.registry.database_acls:\n        from nefertari_guards.view import ACLFilterViewMixin\n        bases = [SetObjectACLMixin] + bases + [ACLFilterViewMixin]\n    bases.append(NefertariBaseView)\n\n    RESTView = type('RESTView', tuple(bases), {'Model': model_cls})\n\n    def _attr_error(*args, **kwargs):\n        raise AttributeError\n\n    for attr in missing_attrs:\n        setattr(RESTView, attr, property(_attr_error))\n\n    return RESTView"}
{"func_code_string": "def set_object_acl(self, obj):\n        \"\"\" Set object ACL on creation if not already present. \"\"\"\n        if not obj._acl:\n            from nefertari_guards import engine as guards_engine\n            acl = self._factory(self.request).generate_item_acl(obj)\n            obj._acl = guards_engine.ACLField.stringify_acl(acl)"}
{"func_code_string": "def resolve_kw(self, kwargs):\n        \"\"\" Resolve :kwargs: like `story_id: 1` to the form of `id: 1`.\n\n        \"\"\"\n        resolved = {}\n        for key, value in kwargs.items():\n            split = key.split('_', 1)\n            if len(split) > 1:\n                key = split[1]\n            resolved[key] = value\n        return resolved"}
{"func_code_string": "def _location(self, obj):\n        \"\"\" Get location of the `obj`\n\n        Arguments:\n            :obj: self.Model instance.\n        \"\"\"\n        field_name = self.clean_id_name\n        return self.request.route_url(\n            self._resource.uid,\n            **{self._resource.id_name: getattr(obj, field_name)})"}
{"func_code_string": "def _parent_queryset(self):\n        \"\"\" Get queryset of parent view.\n\n        Generated queryset is used to run queries in the current level view.\n        \"\"\"\n        parent = self._resource.parent\n        if hasattr(parent, 'view'):\n            req = self.request.blank(self.request.path)\n            req.registry = self.request.registry\n            req.matchdict = {\n                parent.id_name: self.request.matchdict.get(parent.id_name)}\n            parent_view = parent.view(parent.view._factory, req)\n            obj = parent_view.get_item(**req.matchdict)\n            if isinstance(self, ItemSubresourceBaseView):\n                return\n            prop = self._resource.collection_name\n            return getattr(obj, prop, None)"}
{"func_code_string": "def get_collection(self, **kwargs):\n        \"\"\" Get objects collection taking into account generated queryset\n        of parent view.\n\n        This method allows working with nested resources properly. Thus a\n        queryset returned by this method will be a subset of its parent\n        view's queryset, thus filtering out objects that don't belong to\n        the parent object.\n        \"\"\"\n        self._query_params.update(kwargs)\n        objects = self._parent_queryset()\n        if objects is not None:\n            return self.Model.filter_objects(\n                objects, **self._query_params)\n        return self.Model.get_collection(**self._query_params)"}
{"func_code_string": "def get_item(self, **kwargs):\n        \"\"\" Get collection item taking into account generated queryset\n        of parent view.\n\n        This method allows working with nested resources properly. Thus an item\n        returned by this method will belong to its parent view's queryset, thus\n        filtering out objects that don't belong to the parent object.\n\n        Returns an object from the applicable ACL. If ACL wasn't applied, it is\n        applied explicitly.\n        \"\"\"\n        if six.callable(self.context):\n            self.reload_context(es_based=False, **kwargs)\n\n        objects = self._parent_queryset()\n        if objects is not None and self.context not in objects:\n            raise JHTTPNotFound('{}({}) not found'.format(\n                self.Model.__name__,\n                self._get_context_key(**kwargs)))\n\n        return self.context"}
{"func_code_string": "def reload_context(self, es_based, **kwargs):\n        \"\"\" Reload `self.context` object into a DB or ES object.\n\n        A reload is performed by getting the object ID from :kwargs: and then\n        getting a context key item from the new instance of `self._factory`\n        which is an ACL class used by the current view.\n\n        Arguments:\n            :es_based: Boolean. Whether to init ACL ac es-based or not. This\n                affects the backend which will be queried - either DB or ES\n            :kwargs: Kwargs that contain value for current resource 'id_name'\n                key\n        \"\"\"\n        from .acl import BaseACL\n        key = self._get_context_key(**kwargs)\n        kwargs = {'request': self.request}\n        if issubclass(self._factory, BaseACL):\n            kwargs['es_based'] = es_based\n\n        acl = self._factory(**kwargs)\n        if acl.item_model is None:\n            acl.item_model = self.Model\n\n        self.context = acl[key]"}
{"func_code_string": "def _parent_queryset_es(self):\n        \"\"\" Get queryset (list of object IDs) of parent view.\n\n        The generated queryset is used to run queries in the current level's\n        view.\n        \"\"\"\n        parent = self._resource.parent\n        if hasattr(parent, 'view'):\n            req = self.request.blank(self.request.path)\n            req.registry = self.request.registry\n            req.matchdict = {\n                parent.id_name: self.request.matchdict.get(parent.id_name)}\n            parent_view = parent.view(parent.view._factory, req)\n            obj = parent_view.get_item_es(**req.matchdict)\n            prop = self._resource.collection_name\n            objects_ids = getattr(obj, prop, None)\n            return objects_ids"}
{"func_code_string": "def get_es_object_ids(self, objects):\n        \"\"\" Return IDs of :objects: if they are not IDs already. \"\"\"\n        id_field = self.clean_id_name\n        ids = [getattr(obj, id_field, obj) for obj in objects]\n        return list(set(str(id_) for id_ in ids))"}
{"func_code_string": "def get_collection_es(self):\n        \"\"\" Get ES objects collection taking into account the generated\n        queryset of parent view.\n\n        This method allows working with nested resources properly. Thus a\n        queryset returned by this method will be a subset of its parent view's\n        queryset, thus filtering out objects that don't belong to the parent\n        object.\n        \"\"\"\n        objects_ids = self._parent_queryset_es()\n\n        if objects_ids is not None:\n            objects_ids = self.get_es_object_ids(objects_ids)\n            if not objects_ids:\n                return []\n            self._query_params['id'] = objects_ids\n\n        return super(ESBaseView, self).get_collection_es()"}
{"func_code_string": "def get_item_es(self, **kwargs):\n        \"\"\" Get ES collection item taking into account generated queryset\n        of parent view.\n\n        This method allows working with nested resources properly. Thus an item\n        returned by this method will belong to its parent view's queryset, thus\n        filtering out objects that don't belong to the parent object.\n\n        Returns an object retrieved from the applicable ACL. If an ACL wasn't\n        applied, it is applied explicitly.\n        \"\"\"\n        item_id = self._get_context_key(**kwargs)\n        objects_ids = self._parent_queryset_es()\n        if objects_ids is not None:\n            objects_ids = self.get_es_object_ids(objects_ids)\n\n        if six.callable(self.context):\n            self.reload_context(es_based=True, **kwargs)\n\n        if (objects_ids is not None) and (item_id not in objects_ids):\n            raise JHTTPNotFound('{}(id={}) resource not found'.format(\n                self.Model.__name__, item_id))\n\n        return self.context"}
{"func_code_string": "def update(self, **kwargs):\n        \"\"\" Explicitly reload context with DB usage to get access\n        to complete DB object.\n        \"\"\"\n        self.reload_context(es_based=False, **kwargs)\n        return super(ESCollectionView, self).update(**kwargs)"}
{"func_code_string": "def delete(self, **kwargs):\n        \"\"\" Explicitly reload context with DB usage to get access\n        to complete DB object.\n        \"\"\"\n        self.reload_context(es_based=False, **kwargs)\n        return super(ESCollectionView, self).delete(**kwargs)"}
{"func_code_string": "def get_dbcollection_with_es(self, **kwargs):\n        \"\"\" Get DB objects collection by first querying ES. \"\"\"\n        es_objects = self.get_collection_es()\n        db_objects = self.Model.filter_objects(es_objects)\n        return db_objects"}
{"func_code_string": "def delete_many(self, **kwargs):\n        \"\"\" Delete multiple objects from collection.\n\n        First ES is queried, then the results are used to query the DB.\n        This is done to make sure deleted objects are those filtered\n        by ES in the 'index' method (so user deletes what he saw).\n        \"\"\"\n        db_objects = self.get_dbcollection_with_es(**kwargs)\n        return self.Model._delete_many(db_objects, self.request)"}
{"func_code_string": "def update_many(self, **kwargs):\n        \"\"\" Update multiple objects from collection.\n\n        First ES is queried, then the results are used to query DB.\n        This is done to make sure updated objects are those filtered\n        by ES in the 'index' method (so user updates what he saw).\n        \"\"\"\n        db_objects = self.get_dbcollection_with_es(**kwargs)\n        return self.Model._update_many(\n            db_objects, self._json_params, self.request)"}
{"func_code_string": "def _get_context_key(self, **kwargs):\n        \"\"\" Get value of `self._resource.parent.id_name` from :kwargs: \"\"\"\n        return str(kwargs.get(self._resource.parent.id_name))"}
{"func_code_string": "def get_item(self, **kwargs):\n        \"\"\" Reload context on each access. \"\"\"\n        self.reload_context(es_based=False, **kwargs)\n        return super(ItemSubresourceBaseView, self).get_item(**kwargs)"}
{"func_code_string": "def setup(app):\n    \"\"\"Allow this module to be used as sphinx extension.\n    This attaches the Sphinx hooks.\n\n    :type app: sphinx.application.Sphinx\n    \"\"\"\n    import sphinxcontrib_django.docstrings\n    import sphinxcontrib_django.roles\n\n    # Setup both modules at once. They can also be separately imported to\n    # use only fragments of this package.\n    sphinxcontrib_django.docstrings.setup(app)\n    sphinxcontrib_django.roles.setup(app)"}
{"func_code_string": "def patch_django_for_autodoc():\n    \"\"\"Fix the appearance of some classes in autodoc.\n\n    This avoids query evaluation.\n    \"\"\"\n    # Fix Django's manager appearance\n    ManagerDescriptor.__get__ = lambda self, *args, **kwargs: self.manager\n\n    # Stop Django from executing DB queries\n    models.QuerySet.__repr__ = lambda self: self.__class__.__name__"}
{"func_code_string": "def setup(app):\n    \"\"\"Allow this package to be used as Sphinx extension.\n    This is also called from the top-level ``__init__.py``.\n\n    :type app: sphinx.application.Sphinx\n    \"\"\"\n    from .patches import patch_django_for_autodoc\n\n    # When running, make sure Django doesn't execute querysets\n    patch_django_for_autodoc()\n\n    # Generate docstrings for Django model fields\n    # Register the docstring processor with sphinx\n    app.connect('autodoc-process-docstring', improve_model_docstring)\n\n    # influence skip rules\n    app.connect(\"autodoc-skip-member\", autodoc_skip)"}
{"func_code_string": "def autodoc_skip(app, what, name, obj, skip, options):\n    \"\"\"Hook that tells autodoc to include or exclude certain fields.\n\n    Sadly, it doesn't give a reference to the parent object,\n    so only the ``name`` can be used for referencing.\n\n    :type app: sphinx.application.Sphinx\n    :param what: The parent type, ``class`` or ``module``\n    :type what: str\n    :param name: The name of the child method/attribute.\n    :type name: str\n    :param obj: The child value (e.g. a method, dict, or module reference)\n    :param options: The current autodoc settings.\n    :type options: dict\n\n    .. seealso:: http://www.sphinx-doc.org/en/stable/ext/autodoc.html#event-autodoc-skip-member\n    \"\"\"\n    if name in config.EXCLUDE_MEMBERS:\n        return True\n\n    if name in config.INCLUDE_MEMBERS:\n        return False\n\n    return skip"}
{"func_code_string": "def improve_model_docstring(app, what, name, obj, options, lines):\n    \"\"\"Hook that improves the autodoc docstrings for Django models.\n\n    :type app: sphinx.application.Sphinx\n    :param what: The parent type, ``class`` or ``module``\n    :type what: str\n    :param name: The dotted path to the child method/attribute.\n    :type name: str\n    :param obj: The Python object that i s being documented.\n    :param options: The current autodoc settings.\n    :type options: dict\n    :param lines: The current documentation lines\n    :type lines: list\n    \"\"\"\n    if what == 'class':\n        _improve_class_docs(app, obj, lines)\n    elif what == 'attribute':\n        _improve_attribute_docs(obj, name, lines)\n    elif what == 'method':\n        _improve_method_docs(obj, name, lines)\n\n    # Return the extended docstring\n    return lines"}
{"func_code_string": "def _improve_class_docs(app, cls, lines):\n    \"\"\"Improve the documentation of a class.\"\"\"\n    if issubclass(cls, models.Model):\n        _add_model_fields_as_params(app, cls, lines)\n    elif issubclass(cls, forms.Form):\n        _add_form_fields(cls, lines)"}
{"func_code_string": "def _add_model_fields_as_params(app, obj, lines):\n    \"\"\"Improve the documentation of a Django model subclass.\n\n    This adds all model fields as parameters to the ``__init__()`` method.\n\n    :type app: sphinx.application.Sphinx\n    :type lines: list\n    \"\"\"\n    for field in obj._meta.get_fields():\n        try:\n            help_text = strip_tags(force_text(field.help_text))\n            verbose_name = force_text(field.verbose_name).capitalize()\n        except AttributeError:\n            # e.g. ManyToOneRel\n            continue\n\n        # Add parameter\n        if help_text:\n            lines.append(u':param %s: %s' % (field.name, help_text))\n        else:\n            lines.append(u':param %s: %s' % (field.name, verbose_name))\n\n        # Add type\n        lines.append(_get_field_type(field))\n\n    if 'sphinx.ext.inheritance_diagram' in app.extensions and \\\n            'sphinx.ext.graphviz' in app.extensions and \\\n            not any('inheritance-diagram::' in line for line in lines):\n        lines.append('.. inheritance-diagram::')"}
{"func_code_string": "def _add_form_fields(obj, lines):\n    \"\"\"Improve the documentation of a Django Form class.\n\n    This highlights the available fields in the form.\n    \"\"\"\n    lines.append(\"**Form fields:**\")\n    lines.append(\"\")\n    for name, field in obj.base_fields.items():\n        field_type = \"{}.{}\".format(field.__class__.__module__, field.__class__.__name__)\n        tpl = \"* ``{name}``: {label} (:class:`~{field_type}`)\"\n        lines.append(tpl.format(\n            name=name,\n            field=field,\n            label=field.label or name.replace('_', ' ').title(),\n            field_type=field_type\n        ))"}
{"func_code_string": "def _improve_attribute_docs(obj, name, lines):\n    \"\"\"Improve the documentation of various attributes.\n    This improves the navigation between related objects.\n\n    :param obj: the instance of the object to document.\n    :param name: full dotted path to the object.\n    :param lines: expected documentation lines.\n    \"\"\"\n    if obj is None:\n        # Happens with form attributes.\n        return\n\n    if isinstance(obj, DeferredAttribute):\n        # This only points to a field name, not a field.\n        # Get the field by importing the name.\n        cls_path, field_name = name.rsplit('.', 1)\n        model = import_string(cls_path)\n        field = model._meta.get_field(obj.field_name)\n\n        del lines[:]  # lines.clear() is Python 3 only\n        lines.append(\"**Model field:** {label}\".format(\n            label=field.verbose_name\n        ))\n    elif isinstance(obj, _FIELD_DESCRIPTORS):\n        # These\n        del lines[:]\n        lines.append(\"**Model field:** {label}\".format(\n            label=obj.field.verbose_name\n        ))\n\n        if isinstance(obj, FileDescriptor):\n            lines.append(\"**Return type:** :class:`~django.db.models.fields.files.FieldFile`\")\n        elif PhoneNumberDescriptor is not None and isinstance(obj, PhoneNumberDescriptor):\n            lines.append(\"**Return type:** :class:`~phonenumber_field.phonenumber.PhoneNumber`\")\n    elif isinstance(obj, related_descriptors.ForwardManyToOneDescriptor):\n        # Display a reasonable output for forward descriptors.\n        related_model = obj.field.remote_field.model\n        if isinstance(related_model, str):\n            cls_path = related_model\n        else:\n            cls_path = \"{}.{}\".format(related_model.__module__, related_model.__name__)\n        del lines[:]\n        lines.append(\"**Model field:** {label}, \"\n                     \"accesses the :class:`~{cls_path}` model.\".format(\n                         label=obj.field.verbose_name, cls_path=cls_path\n                     ))\n    elif isinstance(obj, related_descriptors.ReverseOneToOneDescriptor):\n        related_model = obj.related.related_model\n        if isinstance(related_model, str):\n            cls_path = related_model\n        else:\n            cls_path = \"{}.{}\".format(related_model.__module__, related_model.__name__)\n        del lines[:]\n        lines.append(\"**Model field:** {label}, \"\n                     \"accesses the :class:`~{cls_path}` model.\".format(\n                         label=obj.related.field.verbose_name, cls_path=cls_path\n                     ))\n    elif isinstance(obj, related_descriptors.ReverseManyToOneDescriptor):\n        related_model = obj.rel.related_model\n        if isinstance(related_model, str):\n            cls_path = related_model\n        else:\n            cls_path = \"{}.{}\".format(related_model.__module__, related_model.__name__)\n        del lines[:]\n        lines.append(\"**Model field:** {label}, \"\n                     \"accesses the M2M :class:`~{cls_path}` model.\".format(\n                         label=obj.field.verbose_name, cls_path=cls_path\n                     ))\n    elif isinstance(obj, (models.Manager, ManagerDescriptor)):\n        # Somehow the 'objects' manager doesn't pass through the docstrings.\n        module, cls_name, field_name = name.rsplit('.', 2)\n        lines.append(\"Django manager to access the ORM\")\n        tpl = \"Use ``{cls_name}.objects.all()`` to fetch all objects.\"\n        lines.append(tpl.format(cls_name=cls_name))"}
{"func_code_string": "def _improve_method_docs(obj, name, lines):\n    \"\"\"Improve the documentation of various methods.\n\n    :param obj: the instance of the method to document.\n    :param name: full dotted path to the object.\n    :param lines: expected documentation lines.\n    \"\"\"\n    if not lines:\n        # Not doing obj.__module__ lookups to avoid performance issues.\n        if name.endswith('_display'):\n            match = RE_GET_FOO_DISPLAY.search(name)\n            if match is not None:\n                # Django get_..._display method\n                lines.append(\"**Autogenerated:** Shows the label of the :attr:`{field}`\".format(\n                    field=match.group('field')\n                ))\n        elif '.get_next_by_' in name:\n            match = RE_GET_NEXT_BY.search(name)\n            if match is not None:\n                lines.append(\"**Autogenerated:** Finds next instance\"\n                             \" based on :attr:`{field}`.\".format(\n                                 field=match.group('field')\n                             ))\n        elif '.get_previous_by_' in name:\n            match = RE_GET_PREVIOUS_BY.search(name)\n            if match is not None:\n                lines.append(\"**Autogenerated:** Finds previous instance\"\n                             \" based on :attr:`{field}`.\".format(\n                                 field=match.group('field')\n                             ))"}
{"func_code_string": "def attr_names(cls) -> List[str]:\n        \"\"\"\n        Returns annotated attribute names\n        :return: List[str]\n        \"\"\"\n        return [k for k, v in cls.attr_types().items()]"}
{"func_code_string": "def elliptic_fourier_descriptors(contour, order=10, normalize=False):\n    \"\"\"Calculate elliptical Fourier descriptors for a contour.\n\n    :param numpy.ndarray contour: A contour array of size ``[M x 2]``.\n    :param int order: The order of Fourier coefficients to calculate.\n    :param bool normalize: If the coefficients should be normalized;\n        see references for details.\n    :return: A ``[order x 4]`` array of Fourier coefficients.\n    :rtype: :py:class:`numpy.ndarray`\n\n    \"\"\"\n    dxy = np.diff(contour, axis=0)\n    dt = np.sqrt((dxy ** 2).sum(axis=1))\n    t = np.concatenate([([0., ]), np.cumsum(dt)])\n    T = t[-1]\n\n    phi = (2 * np.pi * t) / T\n\n    coeffs = np.zeros((order, 4))\n    for n in _range(1, order + 1):\n        const = T / (2 * n * n * np.pi * np.pi)\n        phi_n = phi * n\n        d_cos_phi_n = np.cos(phi_n[1:]) - np.cos(phi_n[:-1])\n        d_sin_phi_n = np.sin(phi_n[1:]) - np.sin(phi_n[:-1])\n        a_n = const * np.sum((dxy[:, 0] / dt) * d_cos_phi_n)\n        b_n = const * np.sum((dxy[:, 0] / dt) * d_sin_phi_n)\n        c_n = const * np.sum((dxy[:, 1] / dt) * d_cos_phi_n)\n        d_n = const * np.sum((dxy[:, 1] / dt) * d_sin_phi_n)\n        coeffs[n - 1, :] = a_n, b_n, c_n, d_n\n\n    if normalize:\n        coeffs = normalize_efd(coeffs)\n\n    return coeffs"}
{"func_code_string": "def normalize_efd(coeffs, size_invariant=True):\n    \"\"\"Normalizes an array of Fourier coefficients.\n\n    See [#a]_ and [#b]_ for details.\n\n    :param numpy.ndarray coeffs: A ``[n x 4]`` Fourier coefficient array.\n    :param bool size_invariant: If size invariance normalizing should be done as well.\n        Default is ``True``.\n    :return: The normalized ``[n x 4]`` Fourier coefficient array.\n    :rtype: :py:class:`numpy.ndarray`\n\n    \"\"\"\n    # Make the coefficients have a zero phase shift from\n    # the first major axis. Theta_1 is that shift angle.\n    theta_1 = 0.5 * np.arctan2(\n        2 * ((coeffs[0, 0] * coeffs[0, 1]) + (coeffs[0, 2] * coeffs[0, 3])),\n        ((coeffs[0, 0] ** 2) - (coeffs[0, 1] ** 2) + (coeffs[0, 2] ** 2) - (coeffs[0, 3] ** 2)))\n    # Rotate all coefficients by theta_1.\n    for n in _range(1, coeffs.shape[0] + 1):\n        coeffs[n - 1, :] = np.dot(\n            np.array([[coeffs[n - 1, 0], coeffs[n - 1, 1]],\n                      [coeffs[n - 1, 2], coeffs[n - 1, 3]]]),\n            np.array([[np.cos(n * theta_1), -np.sin(n * theta_1)],\n                      [np.sin(n * theta_1), np.cos(n * theta_1)]])).flatten()\n\n    # Make the coefficients rotation invariant by rotating so that\n    # the semi-major axis is parallel to the x-axis.\n    psi_1 = np.arctan2(coeffs[0, 2], coeffs[0, 0])\n    psi_rotation_matrix = np.array([[np.cos(psi_1), np.sin(psi_1)],\n                                    [-np.sin(psi_1), np.cos(psi_1)]])\n    # Rotate all coefficients by -psi_1.\n    for n in _range(1, coeffs.shape[0] + 1):\n        coeffs[n - 1, :] = psi_rotation_matrix.dot(\n            np.array([[coeffs[n - 1, 0], coeffs[n - 1, 1]],\n                      [coeffs[n - 1, 2], coeffs[n - 1, 3]]])).flatten()\n\n    if size_invariant:\n        # Obtain size-invariance by normalizing.\n        coeffs /= np.abs(coeffs[0, 0])\n\n    return coeffs"}
{"func_code_string": "def calculate_dc_coefficients(contour):\n    \"\"\"Calculate the :math:`A_0` and :math:`C_0` coefficients of the elliptic Fourier series.\n\n    :param numpy.ndarray contour: A contour array of size ``[M x 2]``.\n    :return: The :math:`A_0` and :math:`C_0` coefficients.\n    :rtype: tuple\n\n    \"\"\"\n    dxy = np.diff(contour, axis=0)\n    dt = np.sqrt((dxy ** 2).sum(axis=1))\n    t = np.concatenate([([0., ]), np.cumsum(dt)])\n    T = t[-1]\n\n    xi = np.cumsum(dxy[:, 0]) - (dxy[:, 0] / dt) * t[1:]\n    A0 = (1 / T) * np.sum(((dxy[:, 0] / (2 * dt)) * np.diff(t ** 2)) + xi * dt)\n    delta = np.cumsum(dxy[:, 1]) - (dxy[:, 1] / dt) * t[1:]\n    C0 = (1 / T) * np.sum(((dxy[:, 1] / (2 * dt)) * np.diff(t ** 2)) + delta * dt)\n\n    # A0 and CO relate to the first point of the contour array as origin.\n    # Adding those values to the coefficients to make them relate to true origin.\n    return contour[0, 0] + A0, contour[0, 1] + C0"}
{"func_code_string": "def plot_efd(coeffs, locus=(0., 0.), image=None, contour=None, n=300):\n    \"\"\"Plot a ``[2 x (N / 2)]`` grid of successive truncations of the series.\n\n    .. note::\n\n        Requires `matplotlib <http://matplotlib.org/>`_!\n\n    :param numpy.ndarray coeffs: ``[N x 4]`` Fourier coefficient array.\n    :param list, tuple or numpy.ndarray locus:\n        The :math:`A_0` and :math:`C_0` elliptic locus in [#a]_ and [#b]_.\n    :param int n: Number of points to use for plotting of Fourier series.\n\n    \"\"\"\n    try:\n        import matplotlib.pyplot as plt\n    except ImportError:\n        print(\"Cannot plot: matplotlib was not installed.\")\n        return\n\n    N = coeffs.shape[0]\n    N_half = int(np.ceil(N / 2))\n    n_rows = 2\n\n    t = np.linspace(0, 1.0, n)\n    xt = np.ones((n,)) * locus[0]\n    yt = np.ones((n,)) * locus[1]\n\n    for n in _range(coeffs.shape[0]):\n        xt += (coeffs[n, 0] * np.cos(2 * (n + 1) * np.pi * t)) + \\\n              (coeffs[n, 1] * np.sin(2 * (n + 1) * np.pi * t))\n        yt += (coeffs[n, 2] * np.cos(2 * (n + 1) * np.pi * t)) + \\\n              (coeffs[n, 3] * np.sin(2 * (n + 1) * np.pi * t))\n        ax = plt.subplot2grid((n_rows, N_half), (n // N_half, n % N_half))\n        ax.set_title(str(n + 1))\n        if contour is not None:\n            ax.plot(contour[:, 1], contour[:, 0], 'c--', linewidth=2)\n        ax.plot(yt, xt, 'r', linewidth=2)\n        if image is not None:\n            ax.imshow(image, plt.cm.gray)\n\n    plt.show()"}
{"func_code_string": "def _errcheck(result, func, arguments):\n    \"\"\"\n    Error checker for functions returning an integer indicating\n    success (0) / failure (1).\n\n    Raises a XdoException in case of error, otherwise just\n    returns ``None`` (returning the original code, 0, would be\n    useless anyways..)\n    \"\"\"\n\n    if result != 0:\n        raise XdoException(\n            'Function {0} returned error code {1}'\n            .format(func.__name__, result))\n    return None"}
{"func_code_string": "def _gen_input_mask(mask):\n    \"\"\"Generate input mask from bytemask\"\"\"\n    return input_mask(\n        shift=bool(mask & MOD_Shift),\n        lock=bool(mask & MOD_Lock),\n        control=bool(mask & MOD_Control),\n        mod1=bool(mask & MOD_Mod1),\n        mod2=bool(mask & MOD_Mod2),\n        mod3=bool(mask & MOD_Mod3),\n        mod4=bool(mask & MOD_Mod4),\n        mod5=bool(mask & MOD_Mod5))"}
{"func_code_string": "def move_mouse(self, x, y, screen=0):\n        \"\"\"\n        Move the mouse to a specific location.\n\n        :param x: the target X coordinate on the screen in pixels.\n        :param y: the target Y coordinate on the screen in pixels.\n        :param screen: the screen (number) you want to move on.\n        \"\"\"\n        # todo: apparently the \"screen\" argument is not behaving properly\n        #       and sometimes even making the interpreter crash..\n        #       Figure out why (changed API / using wrong header?)\n\n        # >>> xdo.move_mouse(3000,200,1)\n\n        # X Error of failed request:  BadWindow (invalid Window parameter)\n        #   Major opcode of failed request:  41 (X_WarpPointer)\n        #   Resource id in failed request:  0x2a4fca0\n        #   Serial number of failed request:  25\n        #   Current serial number in output stream:  26\n\n        # Just to be safe..\n        # screen = 0\n\n        x = ctypes.c_int(x)\n        y = ctypes.c_int(y)\n        screen = ctypes.c_int(screen)\n\n        _libxdo.xdo_move_mouse(self._xdo, x, y, screen)"}
{"func_code_string": "def move_mouse_relative_to_window(self, window, x, y):\n        \"\"\"\n        Move the mouse to a specific location relative to the top-left corner\n        of a window.\n\n        :param x: the target X coordinate on the screen in pixels.\n        :param y: the target Y coordinate on the screen in pixels.\n        \"\"\"\n        _libxdo.xdo_move_mouse_relative_to_window(\n            self._xdo, ctypes.c_ulong(window), x, y)"}
{"func_code_string": "def move_mouse_relative(self, x, y):\n        \"\"\"\n        Move the mouse relative to it's current position.\n\n        :param x: the distance in pixels to move on the X axis.\n        :param y: the distance in pixels to move on the Y axis.\n        \"\"\"\n        _libxdo.xdo_move_mouse_relative(self._xdo, x, y)"}
{"func_code_string": "def mouse_down(self, window, button):\n        \"\"\"\n        Send a mouse press (aka mouse down) for a given button at\n        the current mouse location.\n\n        :param window:\n            The window you want to send the event to or CURRENTWINDOW\n        :param button:\n            The mouse button. Generally, 1 is left, 2 is middle, 3 is\n            right, 4 is wheel up, 5 is wheel down.\n        \"\"\"\n        _libxdo.xdo_mouse_down(\n            self._xdo, ctypes.c_ulong(window), ctypes.c_int(button))"}
{"func_code_string": "def mouse_up(self, window, button):\n        \"\"\"\n        Send a mouse release (aka mouse up) for a given button at\n        the current mouse location.\n\n        :param window:\n            The window you want to send the event to or CURRENTWINDOW\n        :param button:\n            The mouse button. Generally, 1 is left, 2 is middle, 3 is\n            right, 4 is wheel up, 5 is wheel down.\n        \"\"\"\n        _libxdo.xdo_mouse_up(\n            self._xdo, ctypes.c_ulong(window), ctypes.c_int(button))"}
{"func_code_string": "def get_mouse_location(self):\n        \"\"\"\n        Get the current mouse location (coordinates and screen number).\n\n        :return: a namedtuple with ``x``, ``y`` and ``screen_num`` fields\n        \"\"\"\n        x = ctypes.c_int(0)\n        y = ctypes.c_int(0)\n        screen_num = ctypes.c_int(0)\n        _libxdo.xdo_get_mouse_location(\n            self._xdo, ctypes.byref(x), ctypes.byref(y),\n            ctypes.byref(screen_num))\n        return mouse_location(x.value, y.value, screen_num.value)"}
{"func_code_string": "def get_window_at_mouse(self):\n        \"\"\"\n        Get the window the mouse is currently over\n        \"\"\"\n        window_ret = ctypes.c_ulong(0)\n        _libxdo.xdo_get_window_at_mouse(self._xdo, ctypes.byref(window_ret))\n        return window_ret.value"}
{"func_code_string": "def get_mouse_location2(self):\n        \"\"\"\n        Get all mouse location-related data.\n\n        :return: a namedtuple with ``x``, ``y``, ``screen_num``\n            and ``window`` fields\n        \"\"\"\n        x = ctypes.c_int(0)\n        y = ctypes.c_int(0)\n        screen_num_ret = ctypes.c_ulong(0)\n        window_ret = ctypes.c_ulong(0)\n        _libxdo.xdo_get_mouse_location2(\n            self._xdo, ctypes.byref(x), ctypes.byref(y),\n            ctypes.byref(screen_num_ret), ctypes.byref(window_ret))\n        return mouse_location2(x.value, y.value, screen_num_ret.value,\n                               window_ret.value)"}
{"func_code_string": "def wait_for_mouse_move_from(self, origin_x, origin_y):\n        \"\"\"\n        Wait for the mouse to move from a location. This function will block\n        until the condition has been satisified.\n\n        :param origin_x: the X position you expect the mouse to move from\n        :param origin_y: the Y position you expect the mouse to move from\n        \"\"\"\n        _libxdo.xdo_wait_for_mouse_move_from(self._xdo, origin_x, origin_y)"}
{"func_code_string": "def wait_for_mouse_move_to(self, dest_x, dest_y):\n        \"\"\"\n        Wait for the mouse to move to a location. This function will block\n        until the condition has been satisified.\n\n        :param dest_x: the X position you expect the mouse to move to\n        :param dest_y: the Y position you expect the mouse to move to\n        \"\"\"\n        _libxdo.xdo_wait_for_mouse_move_from(self._xdo, dest_x, dest_y)"}
{"func_code_string": "def click_window(self, window, button):\n        \"\"\"\n        Send a click for a specific mouse button at the current mouse location.\n\n        :param window:\n            The window you want to send the event to or CURRENTWINDOW\n        :param button:\n            The mouse button. Generally, 1 is left, 2 is middle, 3 is\n            right, 4 is wheel up, 5 is wheel down.\n        \"\"\"\n        _libxdo.xdo_click_window(self._xdo, window, button)"}
{"func_code_string": "def click_window_multiple(self, window, button, repeat=2, delay=100000):\n        \"\"\"\n        Send a one or more clicks for a specific mouse button at the\n        current mouse location.\n\n        :param window:\n            The window you want to send the event to or CURRENTWINDOW\n        :param button:\n            The mouse button. Generally, 1 is left, 2 is middle, 3 is\n            right, 4 is wheel up, 5 is wheel down.\n        :param repeat: number of repetitions (default: 2)\n        :param delay: delay between clicks, in microseconds (default: 100k)\n        \"\"\"\n        _libxdo.xdo_click_window_multiple(\n            self._xdo, window, button, repeat, delay)"}
{"func_code_string": "def enter_text_window(self, window, string, delay=12000):\n        \"\"\"\n        Type a string to the specified window.\n\n        If you want to send a specific key or key sequence, such as\n        \"alt+l\", you want instead ``send_keysequence_window(...)``.\n\n        :param window:\n            The window you want to send keystrokes to or CURRENTWINDOW\n        :param string:\n            The string to type, like \"Hello world!\"\n        :param delay:\n            The delay between keystrokes in microseconds.\n            12000 is a decent choice if you don't have other plans.\n        \"\"\"\n        return _libxdo.xdo_enter_text_window(self._xdo, window, string, delay)"}
{"func_code_string": "def send_keysequence_window(self, window, keysequence, delay=12000):\n        \"\"\"\n        Send a keysequence to the specified window.\n\n        This allows you to send keysequences by symbol name. Any combination\n        of X11 KeySym names separated by '+' are valid. Single KeySym names\n        are valid, too.\n\n        Examples:\n          \"l\"\n          \"semicolon\"\n          \"alt+Return\"\n          \"Alt_L+Tab\"\n\n        If you want to type a string, such as \"Hello world.\" you want to\n        instead use xdo_enter_text_window.\n\n        :param window: The window you want to send the keysequence to or\n          CURRENTWINDOW\n        :param keysequence: The string keysequence to send.\n        :param delay: The delay between keystrokes in microseconds.\n        \"\"\"\n        _libxdo.xdo_send_keysequence_window(\n            self._xdo, window, keysequence, delay)"}
{"func_code_string": "def send_keysequence_window_up(self, window, keysequence, delay=12000):\n        \"\"\"Send key release (up) events for the given key sequence\"\"\"\n        _libxdo.xdo_send_keysequence_window_up(\n            self._xdo, window, keysequence, ctypes.c_ulong(delay))"}
{"func_code_string": "def send_keysequence_window_down(self, window, keysequence, delay=12000):\n        \"\"\"Send key press (down) events for the given key sequence\"\"\"\n        _libxdo.xdo_send_keysequence_window_down(\n            self._xdo, window, keysequence, ctypes.c_ulong(delay))"}
{"func_code_string": "def send_keysequence_window_list_do(\n            self, window, keys, pressed=1, modifier=None, delay=120000):\n        \"\"\"\n        Send a series of keystrokes.\n\n        :param window: The window to send events to or CURRENTWINDOW\n        :param keys: The array of charcodemap_t entities to send.\n        :param pressed: 1 for key press, 0 for key release.\n        :param modifier:\n            Pointer to integer to record the modifiers\n            activated by the keys being pressed. If NULL, we don't save\n            the modifiers.\n        :param delay:\n            The delay between keystrokes in microseconds.\n        \"\"\"\n        # todo: how to properly use charcodes_t in a nice way?\n        _libxdo.xdo_send_keysequence_window_list_do(\n            self._xdo, window, keys, len(keys), pressed, modifier, delay)"}
{"func_code_string": "def get_active_keys_to_keycode_list(self):\n        \"\"\"Get a list of active keys. Uses XQueryKeymap\"\"\"\n\n        try:\n            _libxdo.xdo_get_active_keys_to_keycode_list\n        except AttributeError:\n            # Apparently, this was implemented in a later version..\n            raise NotImplementedError()\n\n        keys = POINTER(charcodemap_t)\n        nkeys = ctypes.c_int(0)\n        _libxdo.xdo_get_active_keys_to_keycode_list(\n            self._xdo, ctypes.byref(keys), ctypes.byref(nkeys))\n\n        # todo: make sure this returns a list of charcodemap_t!\n        return keys.value"}
{"func_code_string": "def wait_for_window_map_state(self, window, state):\n        \"\"\"\n        Wait for a window to have a specific map state.\n\n        State possibilities:\n          IsUnmapped - window is not displayed.\n          IsViewable - window is mapped and shown (though may be\n              clipped by windows on top of it)\n          IsUnviewable - window is mapped but a parent window is unmapped.\n\n        :param window: the window you want to wait for.\n        :param state: the state to wait for.\n        \"\"\"\n        _libxdo.xdo_wait_for_window_map_state(self._xdo, window, state)"}
{"func_code_string": "def move_window(self, window, x, y):\n        \"\"\"\n        Move a window to a specific location.\n\n        The top left corner of the window will be moved to the x,y coordinate.\n\n        :param wid: the window to move\n        :param x: the X coordinate to move to.\n        :param y: the Y coordinate to move to.\n        \"\"\"\n        _libxdo.xdo_move_window(self._xdo, window, x, y)"}
{"func_code_string": "def translate_window_with_sizehint(self, window, width, height):\n        \"\"\"\n        Apply a window's sizing hints (if any) to a given width and height.\n\n        This function wraps XGetWMNormalHints() and applies any\n        resize increment and base size to your given width and height values.\n\n        :param window: the window to use\n        :param width: the unit width you want to translate\n        :param height: the unit height you want to translate\n        :return: (width, height)\n        \"\"\"\n        width_ret = ctypes.c_uint(0)\n        height_ret = ctypes.c_uint(0)\n        _libxdo.xdo_translate_window_with_sizehint(\n            self._xdo, window, width, height,\n            ctypes.byref(width_ret),\n            ctypes.byref(height_ret))\n        return width_ret.value, height_ret.value"}
{"func_code_string": "def set_window_size(self, window, w, h, flags=0):\n        \"\"\"\n        Change the window size.\n\n        :param wid: the window to resize\n        :param w: the new desired width\n        :param h: the new desired height\n        :param flags: if 0, use pixels for units. If SIZE_USEHINTS, then\n            the units will be relative to the window size hints.\n        \"\"\"\n        _libxdo.xdo_set_window_size(self._xdo, window, w, h, flags)"}
{"func_code_string": "def set_window_property(self, window, name, value):\n        \"\"\"\n        Change a window property.\n\n        Example properties you can change are WM_NAME, WM_ICON_NAME, etc.\n\n        :param wid: The window to change a property of.\n        :param name: the string name of the property.\n        :param value: the string value of the property.\n        \"\"\"\n        _libxdo.xdo_set_window_property(self._xdo, window, name, value)"}
{"func_code_string": "def set_window_class(self, window, name, class_):\n        \"\"\"\n        Change the window's classname and or class.\n\n        :param name: The new class name. If ``None``, no change.\n        :param class_: The new class. If ``None``, no change.\n        \"\"\"\n        _libxdo.xdo_set_window_class(self._xdo, window, name, class_)"}
{"func_code_string": "def set_window_urgency(self, window, urgency):\n        \"\"\"Sets the urgency hint for a window\"\"\"\n        _libxdo.xdo_set_window_urgency(self._xdo, window, urgency)"}
{"func_code_string": "def set_window_override_redirect(self, window, override_redirect):\n        \"\"\"\n        Set the override_redirect value for a window. This generally means\n        whether or not a window manager will manage this window.\n\n        If you set it to 1, the window manager will usually not draw\n        borders on the window, etc. If you set it to 0, the window manager\n         will see it like a normal application window.\n        \"\"\"\n        _libxdo.xdo_set_window_override_redirect(\n            self._xdo, window, override_redirect)"}
{"func_code_string": "def get_focused_window(self):\n        \"\"\"\n        Get the window currently having focus.\n\n        :param window_ret:\n        Pointer to a window where the currently-focused window\n        will be stored.\n        \"\"\"\n        window_ret = window_t(0)\n        _libxdo.xdo_get_focused_window(self._xdo, ctypes.byref(window_ret))\n        return window_ret.value"}
{"func_code_string": "def wait_for_window_focus(self, window, want_focus):\n        \"\"\"\n        Wait for a window to have or lose focus.\n\n        :param window: The window to wait on\n        :param want_focus: If 1, wait for focus. If 0, wait for loss of focus.\n        \"\"\"\n        _libxdo.xdo_wait_for_window_focus(self._xdo, window, want_focus)"}
{"func_code_string": "def get_focused_window_sane(self):\n        \"\"\"\n        Like xdo_get_focused_window, but return the first ancestor-or-self\n        window * having a property of WM_CLASS. This allows you to get\n        the \"real\" or top-level-ish window having focus rather than something\n        you may not expect to be the window having focused.\n\n        :param window_ret:\n            Pointer to a window where the currently-focused window\n            will be stored.\n        \"\"\"\n        window_ret = window_t(0)\n        _libxdo.xdo_get_focused_window_sane(\n            self._xdo, ctypes.byref(window_ret))\n        return window_ret.value"}
{"func_code_string": "def wait_for_window_active(self, window, active=1):\n        \"\"\"\n        Wait for a window to be active or not active.\n\n        Requires your window manager to support this.\n        Uses _NET_ACTIVE_WINDOW from the EWMH spec.\n\n        :param window: the window to wait on\n        :param active: If 1, wait for active. If 0, wait for inactive.\n        \"\"\"\n        _libxdo.xdo_wait_for_window_active(self._xdo, window, active)"}
{"func_code_string": "def reparent_window(self, window_source, window_target):\n        \"\"\"\n        Reparents a window\n\n        :param wid_source: the window to reparent\n        :param wid_target: the new parent window\n        \"\"\"\n        _libxdo.xdo_reparent_window(self._xdo, window_source, window_target)"}
{"func_code_string": "def get_window_location(self, window):\n        \"\"\"\n        Get a window's location.\n        \"\"\"\n        screen_ret = Screen()\n        x_ret = ctypes.c_int(0)\n        y_ret = ctypes.c_int(0)\n        _libxdo.xdo_get_window_location(\n            self._xdo, window, ctypes.byref(x_ret), ctypes.byref(y_ret),\n            ctypes.byref(screen_ret))\n        return window_location(x_ret.value, y_ret.value, screen_ret)"}
{"func_code_string": "def get_window_size(self, window):\n        \"\"\"\n        Get a window's size.\n        \"\"\"\n        w_ret = ctypes.c_uint(0)\n        h_ret = ctypes.c_uint(0)\n        _libxdo.xdo_get_window_size(self._xdo, window, ctypes.byref(w_ret),\n                                    ctypes.byref(h_ret))\n        return window_size(w_ret.value, h_ret.value)"}
{"func_code_string": "def get_active_window(self):\n        \"\"\"\n        Get the currently-active window.\n        Requires your window manager to support this.\n        Uses ``_NET_ACTIVE_WINDOW`` from the EWMH spec.\n        \"\"\"\n        window_ret = window_t(0)\n        _libxdo.xdo_get_active_window(self._xdo, ctypes.byref(window_ret))\n        return window_ret.value"}
{"func_code_string": "def select_window_with_click(self):\n        \"\"\"\n        Get a window ID by clicking on it.\n        This function blocks until a selection is made.\n        \"\"\"\n        window_ret = window_t(0)\n        _libxdo.xdo_select_window_with_click(\n            self._xdo, ctypes.byref(window_ret))\n        return window_ret.value"}
{"func_code_string": "def get_number_of_desktops(self):\n        \"\"\"\n        Get the current number of desktops.\n        Uses ``_NET_NUMBER_OF_DESKTOPS`` of the EWMH spec.\n\n        :param ndesktops:\n            pointer to long where the current number of desktops is stored\n        \"\"\"\n        ndesktops = ctypes.c_long(0)\n        _libxdo.xdo_get_number_of_desktops(self._xdo, ctypes.byref(ndesktops))\n        return ndesktops.value"}
{"func_code_string": "def get_current_desktop(self):\n        \"\"\"\n        Get the current desktop.\n        Uses ``_NET_CURRENT_DESKTOP`` of the EWMH spec.\n        \"\"\"\n        desktop = ctypes.c_long(0)\n        _libxdo.xdo_get_current_desktop(self._xdo, ctypes.byref(desktop))\n        return desktop.value"}
{"func_code_string": "def set_desktop_for_window(self, window, desktop):\n        \"\"\"\n        Move a window to another desktop\n        Uses _NET_WM_DESKTOP of the EWMH spec.\n\n        :param wid: the window to move\n        :param desktop: the desktop destination for the window\n        \"\"\"\n        _libxdo.xdo_set_desktop_for_window(self._xdo, window, desktop)"}
{"func_code_string": "def get_desktop_for_window(self, window):\n        \"\"\"\n        Get the desktop a window is on.\n        Uses _NET_WM_DESKTOP of the EWMH spec.\n\n        If your desktop does not support ``_NET_WM_DESKTOP``, then '*desktop'\n        remains unmodified.\n\n        :param wid: the window to query\n        \"\"\"\n        desktop = ctypes.c_long(0)\n        _libxdo.xdo_get_desktop_for_window(\n            self._xdo, window, ctypes.byref(desktop))\n        return desktop.value"}
{"func_code_string": "def search_windows(\n            self, winname=None, winclass=None, winclassname=None,\n            pid=None, only_visible=False, screen=None, require=False,\n            searchmask=0, desktop=None, limit=0, max_depth=-1):\n        \"\"\"\n        Search for windows.\n\n        :param winname:\n            Regexp to be matched against window name\n        :param winclass:\n            Regexp to be matched against window class\n        :param winclassname:\n            Regexp to be matched against window class name\n        :param pid:\n            Only return windows from this PID\n        :param only_visible:\n            If True, only return visible windows\n        :param screen:\n            Search only windows on this screen\n        :param require:\n            If True, will match ALL conditions. Otherwise, windows matching\n            ANY condition will be returned.\n        :param searchmask:\n            Search mask, for advanced usage. Leave this alone if you\n            don't kwnow what you are doing.\n        :param limit:\n            Maximum number of windows to list. Zero means no limit.\n        :param max_depth:\n            Maximum depth to return. Defaults to -1, meaning \"no limit\".\n        :return:\n            A list of window ids matching query.\n        \"\"\"\n        windowlist_ret = ctypes.pointer(window_t(0))\n        nwindows_ret = ctypes.c_uint(0)\n\n        search = xdo_search_t(searchmask=searchmask)\n\n        if winname is not None:\n            search.winname = winname\n            search.searchmask |= SEARCH_NAME\n\n        if winclass is not None:\n            search.winclass = winclass\n            search.searchmask |= SEARCH_CLASS\n\n        if winclassname is not None:\n            search.winclassname = winclassname\n            search.searchmask |= SEARCH_CLASSNAME\n\n        if pid is not None:\n            search.pid = pid\n            search.searchmask |= SEARCH_PID\n\n        if only_visible:\n            search.only_visible = True\n            search.searchmask |= SEARCH_ONLYVISIBLE\n\n        if screen is not None:\n            search.screen = screen\n            search.searchmask |= SEARCH_SCREEN\n\n        if screen is not None:\n            search.screen = desktop\n            search.searchmask |= SEARCH_DESKTOP\n\n        search.limit = limit\n        search.max_depth = max_depth\n\n        _libxdo.xdo_search_windows(\n            self._xdo, search,\n            ctypes.byref(windowlist_ret),\n            ctypes.byref(nwindows_ret))\n\n        return [windowlist_ret[i] for i in range(nwindows_ret.value)]"}
{"func_code_string": "def get_symbol_map(self):\n        \"\"\"\n        If you need the symbol map, use this method.\n\n        The symbol map is an array of string pairs mapping common tokens\n        to X Keysym strings, such as \"alt\" to \"Alt_L\"\n\n        :return: array of strings.\n        \"\"\"\n        # todo: make sure we return a list of strings!\n        sm = _libxdo.xdo_get_symbol_map()\n\n        # Return value is like:\n        # ['alt', 'Alt_L', ..., None, None, None, ...]\n        # We want to return only values up to the first None.\n        # todo: any better solution than this?\n        i = 0\n        ret = []\n        while True:\n            c = sm[i]\n            if c is None:\n                return ret\n            ret.append(c)\n            i += 1"}
{"func_code_string": "def get_active_modifiers(self):\n        \"\"\"\n        Get a list of active keys. Uses XQueryKeymap.\n\n        :return: list of charcodemap_t instances\n        \"\"\"\n        keys = ctypes.pointer(charcodemap_t())\n        nkeys = ctypes.c_int(0)\n\n        _libxdo.xdo_get_active_modifiers(\n            self._xdo, ctypes.byref(keys), ctypes.byref(nkeys))\n        return [keys[i] for i in range(nkeys.value)]"}
{"func_code_string": "def get_window_name(self, win_id):\n        \"\"\"\n        Get a window's name, if any.\n        \"\"\"\n        window = window_t(win_id)\n        name_ptr = ctypes.c_char_p()\n        name_len = ctypes.c_int(0)\n        name_type = ctypes.c_int(0)\n        _libxdo.xdo_get_window_name(\n            self._xdo, window, ctypes.byref(name_ptr),\n            ctypes.byref(name_len), ctypes.byref(name_type))\n        name = name_ptr.value\n        _libX11.XFree(name_ptr)  # Free the string allocated by Xlib\n        return name"}
{"func_code_string": "def import_metadata(module_paths):\n    \"\"\"Import all the given modules\"\"\"\n    cwd = os.getcwd()\n    if cwd not in sys.path:\n        sys.path.insert(0, cwd)\n    modules = []\n    try:\n        for path in module_paths:\n            modules.append(import_module(path))\n    except ImportError as e:\n        err = RuntimeError('Could not import {}: {}'.format(path, str(e)))\n        raise_from(err, e)\n    return modules"}
{"func_code_string": "def load_metadata(stream):\n    \"\"\"Load JSON metadata from opened stream.\"\"\"\n    try:\n        metadata = json.load(\n            stream, encoding='utf8', object_pairs_hook=OrderedDict)\n    except json.JSONDecodeError as e:\n        err = RuntimeError('Error parsing {}: {}'.format(stream.name, e))\n        raise_from(err, e)\n    else:\n        # convert changelog keys back to ints for sorting\n        for group in metadata:\n            if group == '$version':\n                continue\n            apis = metadata[group]['apis']\n            for api in apis.values():\n                int_changelog = OrderedDict()\n                for version, log in api.get('changelog', {}).items():\n                    int_changelog[int(version)] = log\n                api['changelog'] = int_changelog\n    finally:\n        stream.close()\n\n    return metadata"}
{"func_code_string": "def strip_punctuation_space(value):\n    \"Strip excess whitespace prior to punctuation.\"\n    def strip_punctuation(string):\n        replacement_list = (\n            (' .',  '.'),\n            (' :',  ':'),\n            ('( ',  '('),\n            (' )',  ')'),\n        )\n        for match, replacement in replacement_list:\n            string = string.replace(match, replacement)\n        return string\n    if value == None:\n        return None\n    if type(value) == list:\n        return [strip_punctuation(v) for v in value]\n    return strip_punctuation(value)"}
{"func_code_string": "def join_sentences(string1, string2, glue='.'):\n    \"concatenate two sentences together with punctuation glue\"\n    if not string1 or string1 == '':\n        return string2\n    if not string2 or string2 == '':\n        return string1\n    # both are strings, continue joining them together with the glue and whitespace\n    new_string = string1.rstrip()\n    if not new_string.endswith(glue):\n        new_string += glue\n    new_string += ' ' + string2.lstrip()\n    return new_string"}
{"func_code_string": "def coerce_to_int(val, default=0xDEADBEEF):\n    \"\"\"Attempts to cast given value to an integer, return the original value if failed or the default if one provided.\"\"\"\n    try:\n        return int(val)\n    except (TypeError, ValueError):\n        if default != 0xDEADBEEF:\n            return default\n        return val"}
{"func_code_string": "def nullify(function):\n    \"Decorator. If empty list, returns None, else list.\"\n    def wrapper(*args, **kwargs):\n        value = function(*args, **kwargs)\n        if(type(value) == list and len(value) == 0):\n            return None\n        return value\n    return wrapper"}
{"func_code_string": "def strippen(function):\n    \"Decorator. Strip excess whitespace from return value.\"\n    def wrapper(*args, **kwargs):\n        return strip_strings(function(*args, **kwargs))\n    return wrapper"}
{"func_code_string": "def inten(function):\n    \"Decorator. Attempts to convert return value to int\"\n    def wrapper(*args, **kwargs):\n        return coerce_to_int(function(*args, **kwargs))\n    return wrapper"}
{"func_code_string": "def date_struct(year, month, day, tz = \"UTC\"):\n    \"\"\"\n    Given year, month and day numeric values and a timezone\n    convert to structured date object\n    \"\"\"\n    ymdtz = (year, month, day, tz)\n    if None in ymdtz:\n        #logger.debug(\"a year, month, day or tz value was empty: %s\" % str(ymdtz))\n        return None # return early if we have a bad value\n    try:\n        return time.strptime(\"%s-%s-%s %s\" % ymdtz,  \"%Y-%m-%d %Z\")\n    except(TypeError, ValueError):\n        #logger.debug(\"date failed to convert: %s\" % str(ymdtz))\n        pass"}
{"func_code_string": "def date_struct_nn(year, month, day, tz=\"UTC\"):\n    \"\"\"\n    Assemble a date object but if day or month is none set them to 1\n    to make it easier to deal with partial dates\n    \"\"\"\n    if not day:\n        day = 1\n    if not month:\n        month = 1\n    return date_struct(year, month, day, tz)"}
{"func_code_string": "def doi_uri_to_doi(value):\n    \"Strip the uri schema from the start of DOI URL strings\"\n    if value is None:\n        return value\n    replace_values = ['http://dx.doi.org/', 'https://dx.doi.org/',\n                      'http://doi.org/', 'https://doi.org/']\n    for replace_value in replace_values:\n        value = value.replace(replace_value, '')\n    return value"}
{"func_code_string": "def remove_doi_paragraph(tags):\n    \"Given a list of tags, only return those whose text doesn't start with 'DOI:'\"\n    p_tags = list(filter(lambda tag: not starts_with_doi(tag), tags))\n    p_tags = list(filter(lambda tag: not paragraph_is_only_doi(tag), p_tags))\n    return p_tags"}
{"func_code_string": "def orcid_uri_to_orcid(value):\n    \"Strip the uri schema from the start of ORCID URL strings\"\n    if value is None:\n        return value\n    replace_values = ['http://orcid.org/', 'https://orcid.org/']\n    for replace_value in replace_values:\n        value = value.replace(replace_value, '')\n    return value"}
{"func_code_string": "def component_acting_parent_tag(parent_tag, tag):\n    \"\"\"\n    Only intended for use in getting components, look for tag name of fig-group\n    and if so, find the first fig tag inside it as the acting parent tag\n    \"\"\"\n    if parent_tag.name == \"fig-group\":\n        if (len(tag.find_previous_siblings(\"fig\")) > 0):\n            acting_parent_tag = first(extract_nodes(parent_tag, \"fig\"))\n        else:\n            # Do not return the first fig as parent of itself\n            return None\n    else:\n        acting_parent_tag = parent_tag\n    return acting_parent_tag"}
{"func_code_string": "def extract_nodes(soup, nodename, attr = None, value = None):\n    \"\"\"\n    Returns a list of tags (nodes) from the given soup matching the given nodename.\n    If an optional attribute and value are given, these are used to filter the results\n    further.\"\"\"\n    tags = soup.find_all(nodename)\n    if attr != None and value != None:\n        return list(filter(lambda tag: tag.get(attr) == value, tags))\n    return list(tags)"}
{"func_code_string": "def node_contents_str(tag):\n    \"\"\"\n    Return the contents of a tag, including it's children, as a string.\n    Does not include the root/parent of the tag.\n    \"\"\"\n    if not tag:\n        return None\n    tag_string = ''\n    for child_tag in tag.children:\n        if isinstance(child_tag, Comment):\n            # BeautifulSoup does not preserve comment tags, add them back\n            tag_string += '<!--%s-->' % unicode_value(child_tag)\n        else:\n            tag_string += unicode_value(child_tag)\n    return tag_string if tag_string != '' else None"}
{"func_code_string": "def first_parent(tag, nodename):\n    \"\"\"\n    Given a beautiful soup tag, look at its parents and return the first\n    tag name that matches nodename or the list nodename\n    \"\"\"\n    if nodename is not None and type(nodename) == str:\n        nodename = [nodename]\n    return first(list(filter(lambda tag: tag.name in nodename, tag.parents)))"}
{"func_code_string": "def tag_fig_ordinal(tag):\n    \"\"\"\n    Meant for finding the position of fig tags with respect to whether\n    they are for a main figure or a child figure\n    \"\"\"\n    tag_count = 0\n    if 'specific-use' not in tag.attrs:\n        # Look for tags with no \"specific-use\" attribute\n        return len(list(filter(lambda tag: 'specific-use' not in tag.attrs,\n                          tag.find_all_previous(tag.name)))) + 1"}
{"func_code_string": "def tag_limit_sibling_ordinal(tag, stop_tag_name):\n    \"\"\"\n    Count previous tags of the same name until it\n    reaches a tag name of type stop_tag, then stop counting\n    \"\"\"\n    tag_count = 1\n    for prev_tag in tag.previous_elements:\n        if prev_tag.name == tag.name:\n            tag_count += 1\n        if prev_tag.name == stop_tag_name:\n            break\n\n    return tag_count"}
{"func_code_string": "def tag_media_sibling_ordinal(tag):\n    \"\"\"\n    Count sibling ordinal differently depending on if the\n    mimetype is video or not\n    \"\"\"\n    if hasattr(tag, 'name') and tag.name != 'media':\n        return None\n\n    nodenames = ['fig','supplementary-material','sub-article']\n    first_parent_tag = first_parent(tag, nodenames)\n\n    sibling_ordinal = None\n\n    if first_parent_tag:\n        # Start counting at 0\n        sibling_ordinal = 0\n        for media_tag in first_parent_tag.find_all(tag.name):\n            if 'mimetype' in tag.attrs and tag['mimetype'] == 'video':\n                # Count all video type media tags\n                if 'mimetype' in media_tag.attrs and tag['mimetype'] == 'video':\n                    sibling_ordinal += 1\n                if media_tag == tag:\n                    break\n\n            else:\n                # Count all non-video type media tags\n                if (('mimetype' not in media_tag.attrs)\n                    or ('mimetype' in media_tag.attrs and tag['mimetype'] != 'video')):\n                    sibling_ordinal += 1\n                if media_tag == tag:\n                    break\n    else:\n        # Start counting at 1\n        sibling_ordinal = 1\n        for prev_tag in tag.find_all_previous(tag.name):\n            if not first_parent(prev_tag, nodenames):\n                if 'mimetype' in tag.attrs and tag['mimetype'] == 'video':\n                    # Count all video type media tags\n                    if supp_asset(prev_tag) == supp_asset(tag) and 'mimetype' in prev_tag.attrs:\n                        sibling_ordinal += 1\n                else:\n                    if supp_asset(prev_tag) == supp_asset(tag) and 'mimetype' not in prev_tag.attrs:\n                        sibling_ordinal += 1\n\n    return sibling_ordinal"}
{"func_code_string": "def tag_supplementary_material_sibling_ordinal(tag):\n    \"\"\"\n    Strategy is to count the previous supplementary-material tags\n    having the same asset value to get its sibling ordinal.\n    The result is its position inside any parent tag that\n    are the same asset type\n    \"\"\"\n    if hasattr(tag, 'name') and tag.name != 'supplementary-material':\n        return None\n\n    nodenames = ['fig','media','sub-article']\n    first_parent_tag = first_parent(tag, nodenames)\n\n    sibling_ordinal = 1\n\n    if first_parent_tag:\n        # Within the parent tag of interest, count the tags\n        #  having the same asset value\n        for supp_tag in first_parent_tag.find_all(tag.name):\n            if tag == supp_tag:\n                # Stop once we reach the same tag we are checking\n                break\n            if supp_asset(supp_tag) == supp_asset(tag):\n                sibling_ordinal += 1\n\n    else:\n        # Look in all previous elements that do not have a parent\n        #  and count the tags having the same asset value\n        for prev_tag in tag.find_all_previous(tag.name):\n            if not first_parent(prev_tag, nodenames):\n                if supp_asset(prev_tag) == supp_asset(tag):\n                    sibling_ordinal += 1\n\n    return sibling_ordinal"}
{"func_code_string": "def supp_asset(tag):\n    \"\"\"\n    Given a supplementary-material tag, the asset value depends on\n    its label text. This also informs in what order (its ordinal) it\n    has depending on how many of each type is present\n    \"\"\"\n    # Default\n    asset = 'supp'\n    if first(extract_nodes(tag, \"label\")):\n        label_text = node_text(first(extract_nodes(tag, \"label\"))).lower()\n        # Keyword match the label\n        if label_text.find('code') > 0:\n            asset = 'code'\n        elif label_text.find('data') > 0:\n            asset = 'data'\n    return asset"}
{"func_code_string": "def text_to_title(value):\n    \"\"\"when a title is required, generate one from the value\"\"\"\n    title = None\n    if not value:\n        return title\n    words = value.split(\" \")\n    keep_words = []\n    for word in words:\n        if word.endswith(\".\") or word.endswith(\":\"):\n            keep_words.append(word)\n            if len(word) > 1 and \"<italic>\" not in word and \"<i>\" not in word:\n                break\n        else:\n            keep_words.append(word)\n    if len(keep_words) > 0:\n        title = \" \".join(keep_words)\n        if title.split(\" \")[-1] != \"spp.\":\n            title = title.rstrip(\" .:\")\n    return title"}
{"func_code_string": "def escape_unmatched_angle_brackets(string, allowed_tag_fragments=()):\n    \"\"\"\n    In order to make an XML string less malformed, escape\n    unmatched less than tags that are not part of an allowed tag\n    Note: Very, very basic, and do not try regex \\1 style replacements\n      on unicode ever again! Instead this uses string replace\n    allowed_tag_fragments is a tuple of tag name matches for use with startswith()\n    \"\"\"\n    if not string:\n        return string\n\n    # Split string on tags\n    tags = re.split('(<.*?>)', string)\n    #print tags\n\n    for i, val in enumerate(tags):\n        # Use angle bracket character counts to find unmatched tags\n        #  as well as our allowed_tags list to ignore good tags\n\n        if val.count('<') == val.count('>') and not val.startswith(allowed_tag_fragments):\n            val = val.replace('<', '&lt;')\n            val = val.replace('>', '&gt;')\n        else:\n            # Count how many unmatched tags we have\n            while val.count('<') != val.count('>'):\n                if val.count('<') != val.count('>') and val.count('<') > 0:\n                    val = val.replace('<', '&lt;', 1)\n                elif val.count('<') != val.count('>') and val.count('>') > 0:\n                    val = val.replace('>', '&gt;', 1)\n            if val.count('<') == val.count('>') and not val.startswith(allowed_tag_fragments):\n                # Send it through again in case there are nested unmatched tags\n                val = escape_unmatched_angle_brackets(val, allowed_tag_fragments)\n\n        tags[i] = val\n\n    return ''.join(tags)"}
{"func_code_string": "def escape_ampersand(string):\n    \"\"\"\n    Quick convert unicode ampersand characters not associated with\n    a numbered entity or not starting with allowed characters to a plain &amp;\n    \"\"\"\n    if not string:\n        return string\n    start_with_match = r\"(\\#x(....);|lt;|gt;|amp;)\"\n    # The pattern below is match & that is not immediately followed by #\n    string = re.sub(r\"&(?!\" + start_with_match + \")\", '&amp;', string)\n    return string"}
{"func_code_string": "def parse(filename, return_doctype_dict=False):\n    \"\"\"\n    to extract the doctype details from the file when parsed and return the data\n    for later use, set return_doctype_dict to True\n    \"\"\"\n    doctype_dict = {}\n    # check for python version, doctype in ElementTree is deprecated 3.2 and above\n    if sys.version_info < (3,2):\n        parser = CustomXMLParser(html=0, target=None, encoding='utf-8')\n    else:\n        # Assume greater than Python 3.2, get the doctype from the TreeBuilder\n        tree_builder = CustomTreeBuilder()\n        parser = ElementTree.XMLParser(html=0, target=tree_builder, encoding='utf-8')\n\n    tree = ElementTree.parse(filename, parser)\n    root = tree.getroot()\n\n    if sys.version_info < (3,2):\n        doctype_dict = parser.doctype_dict\n    else:\n        doctype_dict = tree_builder.doctype_dict\n\n    if return_doctype_dict is True:\n        return root, doctype_dict\n    else:\n        return root"}
{"func_code_string": "def add_tag_before(tag_name, tag_text, parent_tag, before_tag_name):\n    \"\"\"\n    Helper function to refactor the adding of new tags\n    especially for when converting text to role tags\n    \"\"\"\n    new_tag = Element(tag_name)\n    new_tag.text = tag_text\n    if get_first_element_index(parent_tag, before_tag_name):\n        parent_tag.insert( get_first_element_index(parent_tag, before_tag_name) - 1, new_tag)\n    return parent_tag"}
{"func_code_string": "def get_first_element_index(root, tag_name):\n    \"\"\"\n    In order to use Element.insert() in a convenient way,\n    this function will find the first child tag with tag_name\n    and return its index position\n    The index can then be used to insert an element before or after the\n    found tag using Element.insert()\n    \"\"\"\n    tag_index = 1\n    for tag in root:\n        if tag.tag == tag_name:\n            # Return the first one found if there is a match\n            return tag_index\n        tag_index = tag_index + 1\n    # Default\n    return None"}
{"func_code_string": "def rewrite_subject_group(root, subjects, subject_group_type, overwrite=True):\n    \"add or rewrite subject tags inside subj-group tags\"\n    parent_tag_name = 'subj-group'\n    tag_name = 'subject'\n    wrap_tag_name = 'article-categories'\n    tag_attribute = 'subj-group-type'\n    # the parent tag where it should be found\n    xpath_parent = './/front/article-meta/article-categories'\n    # the wraping tag in case article-categories does not exist\n    xpath_article_meta = './/front/article-meta'\n    # the xpath to find the subject tags we are interested in\n    xpath = './/{parent_tag_name}[@{tag_attribute}=\"{group_type}\"]'.format(\n        parent_tag_name=parent_tag_name,\n        tag_attribute=tag_attribute,\n        group_type=subject_group_type)\n\n    count = 0\n    # get the parent tag\n    parent_tag = root.find(xpath_parent)\n    if parent_tag is None:\n        # parent tag not found, add one\n        wrap_tag = root.find(xpath_article_meta)\n        article_categories_tag = SubElement(wrap_tag, wrap_tag_name)\n        parent_tag = article_categories_tag\n    insert_index = 0\n    # iterate all tags to find the index of the first tag we are interested in\n    if parent_tag is not None:\n        for tag_index, tag in enumerate(parent_tag.findall('*')):\n            if tag.tag == parent_tag_name and tag.get(tag_attribute) == subject_group_type:\n                insert_index = tag_index\n                if overwrite is True:\n                    # if overwriting use the first one found\n                    break\n            # if not overwriting, use the last one found + 1\n            if overwrite is not True:\n                insert_index += 1\n    # remove the tag if overwriting the existing values\n    if overwrite is True:\n        # remove all the tags\n        for tag in root.findall(xpath):\n            parent_tag.remove(tag)\n    # add the subjects\n    for subject in subjects:\n        subj_group_tag = Element(parent_tag_name)\n        subj_group_tag.set(tag_attribute, subject_group_type)\n        subject_tag = SubElement(subj_group_tag, tag_name)\n        subject_tag.text = subject\n        parent_tag.insert(insert_index, subj_group_tag)\n        count += 1\n        insert_index += 1\n    return count"}
{"func_code_string": "def build_doctype(qualifiedName, publicId=None, systemId=None, internalSubset=None):\n    \"\"\"\n    Instantiate an ElifeDocumentType, a subclass of minidom.DocumentType, with\n    some properties so it is more testable\n    \"\"\"\n    doctype = ElifeDocumentType(qualifiedName)\n    doctype._identified_mixin_init(publicId, systemId)\n    if internalSubset:\n        doctype.internalSubset = internalSubset\n    return doctype"}
{"func_code_string": "def append_minidom_xml_to_elementtree_xml(parent, xml, recursive=False, attributes=None):\n    \"\"\"\n    Recursively,\n    Given an ElementTree.Element as parent, and a minidom instance as xml,\n    append the tags and content from xml to parent\n    Used primarily for adding a snippet of XML with <italic> tags\n    attributes: a list of attribute names to copy\n    \"\"\"\n\n    # Get the root tag name\n    if recursive is False:\n        tag_name = xml.documentElement.tagName\n        node = xml.getElementsByTagName(tag_name)[0]\n        new_elem = SubElement(parent, tag_name)\n        if attributes:\n            for attribute in attributes:\n                if xml.documentElement.getAttribute(attribute):\n                    new_elem.set(attribute, xml.documentElement.getAttribute(attribute))\n    else:\n        node = xml\n        tag_name = node.tagName\n        new_elem = parent\n\n    i = 0\n    for child_node in node.childNodes:\n        if child_node.nodeName == '#text':\n            if not new_elem.text and i <= 0:\n                new_elem.text = child_node.nodeValue\n            elif not new_elem.text and i > 0:\n                new_elem_sub.tail = child_node.nodeValue\n            else:\n                new_elem_sub.tail = child_node.nodeValue\n\n        elif child_node.childNodes is not None:\n            new_elem_sub = SubElement(new_elem, child_node.tagName)\n            new_elem_sub = append_minidom_xml_to_elementtree_xml(new_elem_sub, child_node,\n                                                                 True, attributes)\n\n        i = i + 1\n\n    # Debug\n    #encoding = 'utf-8'\n    #rough_string = ElementTree.tostring(parent, encoding)\n    #print rough_string\n\n    return parent"}
{"func_code_string": "def rewrite_json(rewrite_type, soup, json_content):\n    \"\"\"\n    Due to XML content that will not conform with the strict JSON schema validation rules,\n    for elife articles only, rewrite the JSON to make it valid\n    \"\"\"\n    if not soup:\n        return json_content\n    if not elifetools.rawJATS.doi(soup) or not elifetools.rawJATS.journal_id(soup):\n        return json_content\n\n    # Hook only onto elife articles for rewriting currently\n    journal_id_tag = elifetools.rawJATS.journal_id(soup)\n    doi_tag = elifetools.rawJATS.doi(soup)\n    journal_id = elifetools.utils.node_text(journal_id_tag)\n    doi = elifetools.utils.doi_uri_to_doi(elifetools.utils.node_text(doi_tag))\n    if journal_id.lower() == \"elife\":\n        function_name = rewrite_function_name(journal_id, rewrite_type)\n        if function_name:\n            try:\n                json_content = globals()[function_name](json_content, doi)\n            except KeyError:\n                pass\n    return json_content"}
{"func_code_string": "def rewrite_elife_references_json(json_content, doi):\n    \"\"\" this does the work of rewriting elife references json \"\"\"\n    references_rewrite_json = elife_references_rewrite_json()\n    if doi in references_rewrite_json:\n        json_content = rewrite_references_json(json_content, references_rewrite_json[doi])\n\n    # Edge case delete one reference\n    if doi == \"10.7554/eLife.12125\":\n        for i, ref in enumerate(json_content):\n            if ref.get(\"id\") and ref.get(\"id\") == \"bib11\":\n                del json_content[i]\n\n    return json_content"}
{"func_code_string": "def rewrite_references_json(json_content, rewrite_json):\n    \"\"\" general purpose references json rewriting by matching the id value \"\"\"\n    for ref in json_content:\n        if ref.get(\"id\") and ref.get(\"id\") in rewrite_json:\n            for key, value in iteritems(rewrite_json.get(ref.get(\"id\"))):\n                ref[key] = value\n    return json_content"}
{"func_code_string": "def elife_references_rewrite_json():\n    \"\"\" Here is the DOI and references json replacements data for elife \"\"\"\n    references_rewrite_json = {}\n\n    references_rewrite_json[\"10.7554/eLife.00051\"] = {\"bib25\": {\"date\": \"2012\"}}\n    references_rewrite_json[\"10.7554/eLife.00278\"] = {\"bib11\": {\"date\": \"2013\"}}\n    references_rewrite_json[\"10.7554/eLife.00444\"] = {\"bib2\": {\"date\": \"2013\"}}\n    references_rewrite_json[\"10.7554/eLife.00569\"] = {\"bib74\": {\"date\": \"1996\"}}\n    references_rewrite_json[\"10.7554/eLife.00592\"] = {\"bib8\": {\"date\": \"2013\"}}\n    references_rewrite_json[\"10.7554/eLife.00633\"] = {\"bib38\": {\"date\": \"2004\"}}\n    references_rewrite_json[\"10.7554/eLife.00646\"] = {\"bib1\": {\"date\": \"2012\"}}\n    references_rewrite_json[\"10.7554/eLife.00813\"] = {\"bib33\": {\"date\": \"2007\"}}\n    references_rewrite_json[\"10.7554/eLife.01355\"] = {\"bib9\": {\"date\": \"2014\"}}\n    references_rewrite_json[\"10.7554/eLife.01530\"] = {\"bib12\": {\"date\": \"2014\"}}\n    references_rewrite_json[\"10.7554/eLife.01681\"] = {\"bib5\": {\"date\": \"2000\"}}\n    references_rewrite_json[\"10.7554/eLife.01917\"] = {\"bib35\": {\"date\": \"2014\"}}\n    references_rewrite_json[\"10.7554/eLife.02030\"] = {\"bib53\": {\"date\": \"2013\"}, \"bib56\": {\"date\": \"2013\"}}\n    references_rewrite_json[\"10.7554/eLife.02076\"] = {\"bib93a\": {\"date\": \"1990\"}}\n    references_rewrite_json[\"10.7554/eLife.02217\"] = {\"bib27\": {\"date\": \"2009\"}}\n    references_rewrite_json[\"10.7554/eLife.02535\"] = {\"bib12\": {\"date\": \"2014\"}}\n    references_rewrite_json[\"10.7554/eLife.02862\"] = {\"bib8\": {\"date\": \"2010\"}}\n    references_rewrite_json[\"10.7554/eLife.03711\"] = {\"bib35\": {\"date\": \"2012\"}}\n    references_rewrite_json[\"10.7554/eLife.03819\"] = {\"bib37\": {\"date\": \"2008\"}}\n    references_rewrite_json[\"10.7554/eLife.04069\"] = {\"bib8\": {\"date\": \"2011\"}}\n    references_rewrite_json[\"10.7554/eLife.04247\"] = {\"bib19a\": {\"date\": \"2015\"}}\n    references_rewrite_json[\"10.7554/eLife.04333\"] = {\n        \"bib3\": {\"date\": \"1859\"},\n        \"bib37\": {\"date\": \"1959\"}}\n    references_rewrite_json[\"10.7554/eLife.04478\"] = {\"bib49\": {\"date\": \"2014\"}}\n    references_rewrite_json[\"10.7554/eLife.04580\"] = {\"bib139\": {\"date\": \"2014\"}}\n    references_rewrite_json[\"10.7554/eLife.05042\"] = {\"bib78\": {\"date\": \"2015\"}}\n    references_rewrite_json[\"10.7554/eLife.05323\"] = {\"bib102\": {\"date\": \"2014\"}}\n    references_rewrite_json[\"10.7554/eLife.05423\"] = {\"bib102\": {\"date\": \"2014\"}}\n    references_rewrite_json[\"10.7554/eLife.05503\"] = {\"bib94\": {\"date\": \"2016\"}}\n    references_rewrite_json[\"10.7554/eLife.05849\"] = {\"bib82\": {\"date\": \"2005\"}}\n    references_rewrite_json[\"10.7554/eLife.06072\"] = {\"bib17\": {\"date\": \"2003\"}}\n    references_rewrite_json[\"10.7554/eLife.06315\"] = {\"bib19\": {\"date\": \"2014\"}}\n    references_rewrite_json[\"10.7554/eLife.06426\"] = {\"bib39\": {\"date\": \"2015\"}}\n    references_rewrite_json[\"10.7554/eLife.07361\"] = {\"bib76\": {\"date\": \"2011\"}}\n    references_rewrite_json[\"10.7554/eLife.07460\"] = {\n        \"bib1\": {\"date\": \"2013\"},\n        \"bib2\": {\"date\": \"2014\"}}\n    references_rewrite_json[\"10.7554/eLife.08500\"] = {\"bib55\": {\"date\": \"2015\"}}\n    references_rewrite_json[\"10.7554/eLife.09066\"] = {\"bib46\": {\"date\": \"2015\"}}\n    references_rewrite_json[\"10.7554/eLife.09100\"] = {\"bib50\": {\"date\": \"2011\"}}\n    references_rewrite_json[\"10.7554/eLife.09148\"] = {\n        \"bib47\": {\"articleTitle\": \"97\u2013104\"},\n        \"bib59\": {\"articleTitle\": \"1913\u20131918\"}}\n    references_rewrite_json[\"10.7554/eLife.09186\"] = {\n        \"bib31\": {\"date\": \"2015\"},\n        \"bib54\": {\"date\": \"2014\"},\n        \"bib56\": {\"date\": \"2014\"},\n        \"bib65\": {\"date\": \"2015\"}}\n    references_rewrite_json[\"10.7554/eLife.09215\"] = {\"bib5\": {\"date\": \"2012\"}}\n\n    references_rewrite_json[\"10.7554/eLife.09520\"] = {\n        \"bib35\": OrderedDict([\n            (\"conference\", OrderedDict([\n                (\"name\", [\"WHO Expert Committee on Malaria\"])\n            ])),\n            (\"articleTitle\", \"WHO Expert Committee on Malaria [meeting held in Geneva from 19 to 30 October 1970]: fifteenth report\"),\n            (\"publisher\", OrderedDict([\n                (\"name\", [\"World Health Organization\"]),\n                (\"address\", OrderedDict([\n                    (\"formatted\", [\"Geneva\"]),\n                    (\"components\", OrderedDict([\n                        (\"locality\", [\"Geneva\"])\n                    ])),\n                ])),\n            ])),\n        ])\n    }\n\n    references_rewrite_json[\"10.7554/eLife.09579\"] = {\n        \"bib19\": {\"date\": \"2007\"},\n        \"bib49\": {\"date\": \"2002\"}}\n    references_rewrite_json[\"10.7554/eLife.09600\"] = {\"bib13\": {\"date\": \"2009\"}}\n    references_rewrite_json[\"10.7554/eLife.09672\"] = {\n        \"bib25\": {\"conference\": {\"name\": [\"Seventeenth Meeting of the RBM Partnership Monitoring and Evaluation Reference Group (MERG)\"]}}}\n    references_rewrite_json[\"10.7554/eLife.09771\"] = {\"bib22\": {\"date\": \"2012\"}}\n    references_rewrite_json[\"10.7554/eLife.09972\"] = {\"bib61\": {\"date\": \"2007\", \"discriminator\": \"a\"}}\n    references_rewrite_json[\"10.7554/eLife.09977\"] = {\"bib41\": {\"date\": \"2014\"}}\n    references_rewrite_json[\"10.7554/eLife.10032\"] = {\"bib45\": {\"date\": \"2016\"}}\n    references_rewrite_json[\"10.7554/eLife.10042\"] = {\"bib14\": {\"date\": \"2015\"}}\n    references_rewrite_json[\"10.7554/eLife.10070\"] = {\"bib15\": {\"date\": \"2015\"}, \"bib38\": {\"date\": \"2014\"}}\n    references_rewrite_json[\"10.7554/eLife.10222\"] = {\"bib30\": {\"date\": \"2015\"}}\n    references_rewrite_json[\"10.7554/eLife.10670\"] = {\"bib7\": {\"date\": \"2015\"}, \"bib8\": {\"date\": \"2015\"}}\n    references_rewrite_json[\"10.7554/eLife.10781\"] = {\"bib32\": {\"date\": \"2003\"}}\n    references_rewrite_json[\"10.7554/eLife.11273\"] = {\"bib43\": {\"date\": \"2004\"}}\n    references_rewrite_json[\"10.7554/eLife.11305\"] = {\"bib68\": {\"date\": \"2000\"}}\n    references_rewrite_json[\"10.7554/eLife.11416\"] = {\"bib22\": {\"date\": \"1997\"}}\n    references_rewrite_json[\"10.7554/eLife.11860\"] = {\"bib48\": {\"title\": \"Light-switchable gene expression system\"}}\n    references_rewrite_json[\"10.7554/eLife.12401\"] = {\"bib25\": {\"date\": \"2011\"}}\n    references_rewrite_json[\"10.7554/eLife.12366\"] = {\"bib10\": {\"date\": \"2008\"}}\n    references_rewrite_json[\"10.7554/eLife.12703\"] = {\"bib27\": {\"date\": \"2013\"}}\n    references_rewrite_json[\"10.7554/eLife.12735\"] = {\"bib35\": {\"date\": \"2014\"}}\n    references_rewrite_json[\"10.7554/eLife.12830\"] = {\"bib118\": {\"date\": \"1982\"}}\n    references_rewrite_json[\"10.7554/eLife.13133\"] = {\"bib11\": {\"date\": \"2011\"}}\n    references_rewrite_json[\"10.7554/eLife.13152\"] = {\"bib25\": {\"date\": \"2000\"}}\n    references_rewrite_json[\"10.7554/eLife.13195\"] = {\"bib6\": {\"date\": \"2013\"}, \"bib12\": {\"date\": \"2003\"}}\n    references_rewrite_json[\"10.7554/eLife.13479\"] = {\"bib5\": {\"date\": \"2016\"}}\n    references_rewrite_json[\"10.7554/eLife.13463\"] = {\"bib15\": {\"date\": \"2016\"}}\n    references_rewrite_json[\"10.7554/eLife.14119\"] = {\"bib40\": {\"date\": \"2007\"}}\n    references_rewrite_json[\"10.7554/eLife.14169\"] = {\"bib6\": {\"date\": \"2015\"}}\n    references_rewrite_json[\"10.7554/eLife.14523\"] = {\"bib7\": {\"date\": \"2013\"}}\n    references_rewrite_json[\"10.7554/eLife.15272\"] = {\"bib78\": {\"date\": \"2014\"}}\n    references_rewrite_json[\"10.7554/eLife.15504\"] = {\"bib67\": {\"isbn\": \"9780198524304\"}}\n    references_rewrite_json[\"10.7554/eLife.16105\"] = {\"bib2\": {\"date\": \"2013\"}}\n    references_rewrite_json[\"10.7554/eLife.16349\"] = {\"bib68\": {\"date\": \"2005\"}}\n    references_rewrite_json[\"10.7554/eLife.16394\"] = {\n        \"bib6\": {\"type\": \"thesis\",\n        \"author\": {\"type\": \"person\", \"name\": {\"preferred\": \"B Berret\",\"index\": \"Berret, B\" }},\n        \"publisher\": {\"name\": [\"Universit\u00e9 de Bourgogne\"]}}}\n    references_rewrite_json[\"10.7554/eLife.16443\"] = {\"bib58\": {\"date\": \"1987\"}}\n    references_rewrite_json[\"10.7554/eLife.16764\"] = {\"bib4\": {\"date\": \"2013\"}}\n    references_rewrite_json[\"10.7554/eLife.17092\"] = {\"bib102\": {\"date\": \"1980\"}}\n    references_rewrite_json[\"10.7554/eLife.18044\"] = {\"bib25\": {\"date\": \"2005\"}}\n    references_rewrite_json[\"10.7554/eLife.18370\"] = {\"bib1\": {\"date\": \"2006\"}}\n    references_rewrite_json[\"10.7554/eLife.18425\"] = {\"bib54\": {\"date\": \"2014\"}}\n    references_rewrite_json[\"10.7554/eLife.18683\"] = {\"bib47\": {\"date\": \"2015\"}}\n    references_rewrite_json[\"10.7554/eLife.19532\"] = {\"bib27\": {\"date\": \"2015\"}}\n    references_rewrite_json[\"10.7554/eLife.19545\"] = {\"bib51\": {\"date\": \"1996\"}}\n    references_rewrite_json[\"10.7554/eLife.19571\"] = {\"bib56\": {\"date\": \"2016\"}}\n    references_rewrite_json[\"10.7554/eLife.20352\"] = {\"bib53\": {\"country\": \"United States\"}}\n    references_rewrite_json[\"10.7554/eLife.21864\"] = {\"bib2\": {\"date\": \"2016-10-24\"}}\n    references_rewrite_json[\"10.7554/eLife.20522\"] = {\n        \"bib42\": {\"date\": \"2016\"},\n        \"bib110\": {\"date\": \"1996\"}}\n    references_rewrite_json[\"10.7554/eLife.22053\"] = {\"bib123\": {\"date\": \"2016\"}}\n\n    # Reference authors data to replace, processed further below into json\n    references_authors = []\n    references_authors.append((\"10.7554/eLife.00036\", \"bib8\", \"authors\", [\n        {\"surname\": \"Butler\", \"given-names\": \"H\"},\n        {\"surname\": \"Juurlink\", \"given-names\": \"BHJ\"}\n        ]))\n    references_authors.append((\"10.7554/eLife.00036\", \"bib30\", \"authors\", [\n        {\"surname\": \"Joyner\", \"given-names\": \"AL\"}\n        ]))\n\n    references_authors.append((\"10.7554/eLife.00048\", \"bib15\", \"authors\", [\n        {\"surname\": \"Guthrie\", \"given-names\": \"C\"},\n        {\"surname\": \"Fink\", \"given-names\": \"GR\"}\n        ]))\n\n    references_authors.append((\"10.7554/eLife.00051\", \"bib21\", \"authors\", [\n        {\"surname\": \"Jamison\", \"given-names\": \"DT\"},\n        {\"surname\": \"Breman\", \"given-names\": \"JG\"},\n        {\"surname\": \"Measham\", \"given-names\": \"AR\"},\n        {\"surname\": \"Alleyne\", \"given-names\": \"G\"},\n        {\"surname\": \"Claeson\", \"given-names\": \"M\"},\n        {\"surname\": \"Evans\", \"given-names\": \"DB\"},\n        {\"surname\": \"Jha\", \"given-names\": \"P\"},\n        {\"surname\": \"Mills\", \"given-names\": \"A\"},\n        {\"surname\": \"Musgrove\", \"given-names\": \"P\"}\n        ]))\n    references_authors.append((\"10.7554/eLife.00051\", \"bib36\", \"authors\", [\n        {\"surname\": \"Rogers\", \"given-names\": \"RG\"},\n        {\"surname\": \"Crimmins\", \"given-names\": \"EM\"}\n        ]))\n\n    references_authors.append((\"10.7554/eLife.00668\", \"bib39\", \"authors\", [\n        {\"surname\": \"Rice\", \"given-names\": \"SA\"}\n        ]))\n\n    references_authors.append((\"10.7554/eLife.01730\", \"bib75\", \"authors\", [\n        {\"collab\": \"Look AHEAD Research Group\"}\n        ]))\n\n    references_authors.append((\"10.7554/eLife.03714\", \"bib64\", \"authors\", [\n        {\"surname\": \"Otwinowski\", \"given-names\": \"Z\"},\n        {\"surname\": \"Minor\", \"given-names\": \"W\"}\n        ]))\n\n    references_authors.append((\"10.7554/eLife.04220\", \"bib31\", \"authors\", [\n        {\"surname\": \"Tishby\", \"given-names\": \"N\"},\n        {\"surname\": \"Polani\", \"given-names\": \"D\"}\n        ]))\n\n    references_authors.append((\"10.7554/eLife.04395\", \"bib67\", \"authors\", [\n        {\"surname\": \"King\", \"given-names\": \"AMQ\"},\n        {\"surname\": \"Adams\", \"given-names\": \"MJ\"},\n        {\"surname\": \"Carstens\", \"given-names\": \"EB\"},\n        {\"surname\": \"Lefkowitz\", \"given-names\": \"E\"}\n        ]))\n\n    references_authors.append((\"10.7554/eLife.04449\", \"bib62\", \"authors\", [\n        {\"surname\": \"Shaham\", \"given-names\": \"S\"}\n        ]))\n\n    references_authors.append((\"10.7554/eLife.04659\", \"bib57\", \"authors\", [\n        {\"surname\": \"Sambrook\", \"given-names\": \"J\"},\n        {\"surname\": \"Russell\", \"given-names\": \"TW\"}\n        ]))\n\n    references_authors.append((\"10.7554/eLife.05423\", \"bib4\", \"authors\", [\n        {\"surname\": \"Birkhead\", \"given-names\": \"TR\"},\n        {\"surname\": \"M\u00f8ller\", \"given-names\": \"AP\"}\n        ]))\n    references_authors.append((\"10.7554/eLife.05423\", \"bib5\", \"authors\", [\n        {\"surname\": \"Birkhead\", \"given-names\": \"TR\"},\n        {\"surname\": \"M\u00f8ller\", \"given-names\": \"AP\"}\n        ]))\n    references_authors.append((\"10.7554/eLife.05423\", \"bib90\", \"authors\", [\n        {\"surname\": \"Smith\", \"given-names\": \"RL\"}\n        ]))\n\n    references_authors.append((\"10.7554/eLife.05564\", \"bib39\", \"authors\", [\n        {\"surname\": \"Pattyn\", \"given-names\": \"S\"}\n        ]))\n\n    references_authors.append((\"10.7554/eLife.05959\", \"bib76\", \"authors\", [\n        {\"surname\": \"Machol\u00e1n\", \"given-names\": \"M\"},\n        {\"surname\": \"Baird\", \"given-names\": \"SJE\"},\n        {\"surname\": \"Munclinger\", \"given-names\": \"P\"},\n        {\"surname\": \"Pi\u00e1lek\", \"given-names\": \"J\"}\n        ]))\n\n    references_authors.append((\"10.7554/eLife.06565\", \"bib1\", \"authors\", [\n        {\"surname\": \"Ahringer\", \"given-names\": \"J\"}\n        ]))\n\n    references_authors.append((\"10.7554/eLife.06576\", \"bib57\", \"authors\", [\n        {\"surname\": \"Moller\", \"given-names\": \"AR\"}\n        ]))\n\n    references_authors.append((\"10.7554/eLife.06813\", \"bib54\", \"authors\", [\n        {\"surname\": \"King\", \"given-names\": \"JA\"}\n        ]))\n\n    references_authors.append((\"10.7554/eLife.06813\", \"bib55\", \"authors\", [\n        {\"surname\": \"Kirkland\", \"given-names\": \"Gl\"},\n        {\"surname\": \"Layne\", \"given-names\": \"JN\"}\n        ]))\n\n    references_authors.append((\"10.7554/eLife.07460\", \"bib1\", \"authors\", [\n        {\"surname\": \"Rallapalli\", \"given-names\": \"Ghanasyam\"}\n        ]))\n    references_authors.append((\"10.7554/eLife.07460\", \"bib2\", \"authors\", [\n        {\"surname\": \"Bazyl\", \"given-names\": \"Steven\"}\n        ]))\n\n    references_authors.append((\"10.7554/eLife.07847\", \"bib40\", \"authors\", [\n        {\"collab\": \"Nature Immunology\"}\n        ]))\n\n    references_authors.append((\"10.7554/eLife.09666\", \"bib9\", \"authors\", [\n        {\"surname\": \"Sch\u00fcler\", \"given-names\": \"D\"}\n        ]))\n\n    references_authors.append((\"10.7554/eLife.09868\", \"bib5\", \"authors\", [\n        {\"surname\": \"Barlow\", \"given-names\": \"HB\"}\n        ]))\n\n    references_authors.append((\"10.7554/eLife.10222\", \"bib30\", \"authors\", [\n        {\"collab\": \"PharmaMar\"}\n        ]))\n\n    references_authors.append((\"10.7554/eLife.11860\", \"bib48\", \"authors\", [\n        {\"surname\": \"Yang\", \"given-names\": \"Y\"},\n        {\"surname\": \"Wang\", \"given-names\": \"X\"},\n        {\"surname\": \"Chen\", \"given-names\": \"X\"},\n        ]))\n\n    references_authors.append((\"10.7554/eLife.11945\", \"bib23\", \"authors\", [\n        {\"surname\": \"Glimcher\", \"given-names\": \"P\"},\n        {\"surname\": \"Fehr\", \"given-names\": \"E\"}\n        ]))\n\n    references_authors.append((\"10.7554/eLife.13135\", \"bib26\", \"authors\", [\n        {\"surname\": \"Ivanova\", \"given-names\": \"S\"},\n        {\"surname\": \"Herbreteau\", \"given-names\": \"B\"},\n        {\"surname\": \"Blasdell\", \"given-names\": \"K\"},\n        {\"surname\": \"Chaval\", \"given-names\": \"Y\"},\n        {\"surname\": \"Buchy\", \"given-names\": \"P\"},\n        {\"surname\": \"Guillard\", \"given-names\": \"B\"},\n        {\"surname\": \"Morand\", \"given-names\": \"S\"},\n        ]))\n\n    references_authors.append((\"10.7554/eLife.13135\", \"bib27\", \"authors\", [\n        {\"surname\": \"King\", \"given-names\": \"AMQ\"},\n        {\"surname\": \"Adams\", \"given-names\": \"J\"},\n        {\"surname\": \"Carstens\", \"given-names\": \"EB\"},\n        {\"surname\": \"Lefkowitz\", \"given-names\": \"EJ\"}\n        ]))\n\n    references_authors.append((\"10.7554/eLife.14188\", \"bib1\", \"authors\", [\n        {\"collab\": \"Avisoft Bioacoustics\"}\n        ]))\n\n    references_authors.append((\"10.7554/eLife.17716\", \"bib7\", \"authors\", [\n        {\"collab\": \"World Health Organization\"}\n        ]))\n\n    references_authors.append((\"10.7554/eLife.17956\", \"bib4\", \"authors\", [\n        {\"surname\": \"Barrett\", \"given-names\": \"SCH\"}\n        ]))\n\n    references_authors.append((\"10.7554/eLife.18109\", \"bib39\", \"authors\", [\n        {\"surname\": \"Weber\", \"given-names\": \"EH\"}\n        ]))\n\n\n    # Now turn the authors data into the json\n    for author_row in references_authors:\n        ref_json = OrderedDict()\n        doi, id, author_type, authors = author_row\n        #if id not in ref_json:\n        ref_json[id] = OrderedDict()\n        ref_json[id][author_type] = []\n        for ref_author in authors:\n            if  \"collab\" in ref_author:\n                author_json = elifetools.utils_html.references_author_collab(ref_author)\n            else:\n                author_json = elifetools.utils.references_author_person(ref_author)\n            if author_json:\n                ref_json[id][author_type].append(author_json)\n        # Add to json array, and do not verwrite existing rule of a specific bib id (if present)\n        if doi not in references_rewrite_json:\n            references_rewrite_json[doi] = ref_json\n        else:\n            for key, value in iteritems(ref_json):\n                if key not in references_rewrite_json[doi]:\n                    references_rewrite_json[doi][key] = value\n                else:\n                    # Append dict items\n                    for k, v in iteritems(value):\n                        references_rewrite_json[doi][key][k] = v\n\n    return references_rewrite_json"}
{"func_code_string": "def rewrite_elife_body_json(json_content, doi):\n    \"\"\" rewrite elife body json \"\"\"\n\n    # Edge case add an id to a section\n    if doi == \"10.7554/eLife.00013\":\n        if (json_content and len(json_content) > 0):\n            if (json_content[0].get(\"type\") and json_content[0].get(\"type\") == \"section\"\n                and json_content[0].get(\"title\") and json_content[0].get(\"title\") ==\"Introduction\"\n                and not json_content[0].get(\"id\")):\n                json_content[0][\"id\"] = \"s1\"\n\n    # Edge case remove an extra section\n    if doi == \"10.7554/eLife.04232\":\n        if (json_content and len(json_content) > 0):\n            for outer_block in json_content:\n                if outer_block.get(\"id\") and outer_block.get(\"id\") == \"s4\":\n                    for mid_block in outer_block.get(\"content\"):\n                        if mid_block.get(\"id\") and mid_block.get(\"id\") == \"s4-6\":\n                            for inner_block in mid_block.get(\"content\"):\n                                if inner_block.get(\"content\") and not inner_block.get(\"title\"):\n                                    mid_block[\"content\"] = inner_block.get(\"content\")\n\n    # Edge case remove unwanted sections\n    if doi == \"10.7554/eLife.04871\":\n        if (json_content and len(json_content) > 0):\n            for i, outer_block in enumerate(json_content):\n                if (outer_block.get(\"id\") and outer_block.get(\"id\") in [\"s7\", \"s8\"]\n                    and not outer_block.get(\"title\")):\n                    if outer_block.get(\"content\"):\n                        json_content[i] = outer_block.get(\"content\")[0]\n\n    # Edge case remove an extra section\n    if doi == \"10.7554/eLife.05519\":\n        if (json_content and len(json_content) > 0):\n            for outer_block in json_content:\n                if outer_block.get(\"id\") and outer_block.get(\"id\") == \"s4\":\n                    for mid_block in outer_block.get(\"content\"):\n                        if mid_block.get(\"content\") and not mid_block.get(\"id\"):\n                            new_blocks = []\n                            for inner_block in mid_block.get(\"content\"):\n                                 new_blocks.append(inner_block)\n                            outer_block[\"content\"] = new_blocks\n\n    # Edge case add a title to a section\n    if doi == \"10.7554/eLife.07157\":\n        if (json_content and len(json_content) > 0):\n            if (json_content[0].get(\"type\") and json_content[0].get(\"type\") == \"section\"\n                and json_content[0].get(\"id\") and json_content[0].get(\"id\") == \"s1\"):\n                json_content[0][\"title\"] = \"Main text\"\n\n    # Edge case remove a section with no content\n    if doi == \"10.7554/eLife.09977\":\n        if (json_content and len(json_content) > 0):\n            i_index = j_index = None\n            for i, outer_block in enumerate(json_content):\n                if (outer_block.get(\"id\") and outer_block.get(\"id\") == \"s4\"\n                    and outer_block.get(\"content\")):\n                    # We have i\n                    i_index = i\n                    break\n            if i_index is not None:\n                for j, inner_block in enumerate(json_content[i_index].get(\"content\")):\n                    if (inner_block.get(\"id\") and inner_block.get(\"id\") == \"s4-11\"\n                        and inner_block.get(\"content\") is None):\n                        # Now we have i and j for deletion outside of the loop\n                        j_index = j\n                        break\n            # Do the deletion on the original json\n            if i_index is not None and j_index is not None:\n                del json_content[i_index][\"content\"][j_index]\n\n    # Edge case wrap sections differently\n    if doi == \"10.7554/eLife.12844\":\n        if (json_content and len(json_content) > 0 and json_content[0].get(\"type\")\n            and json_content[0][\"type\"] == \"section\"):\n            new_body = OrderedDict()\n            for i, tag_block in enumerate(json_content):\n                if i == 0:\n                    tag_block[\"title\"] = \"Main text\"\n                    new_body = tag_block\n                elif i > 0:\n                    new_body[\"content\"].append(tag_block)\n            json_content = [new_body]\n\n    return json_content"}
{"func_code_string": "def rewrite_elife_funding_awards(json_content, doi):\n    \"\"\" rewrite elife funding awards \"\"\"\n\n    # remove a funding award\n    if doi == \"10.7554/eLife.00801\":\n        for i, award in enumerate(json_content):\n            if \"id\" in award and award[\"id\"] == \"par-2\":\n                del json_content[i]\n\n    # add funding award recipient\n    if doi == \"10.7554/eLife.04250\":\n        recipients_for_04250 = [{\"type\": \"person\", \"name\": {\"preferred\": \"Eric Jonas\", \"index\": \"Jonas, Eric\"}}]\n        for i, award in enumerate(json_content):\n            if \"id\" in award and award[\"id\"] in [\"par-2\", \"par-3\", \"par-4\"]:\n                if \"recipients\" not in award:\n                    json_content[i][\"recipients\"] = recipients_for_04250\n\n    # add funding award recipient\n    if doi == \"10.7554/eLife.06412\":\n        recipients_for_06412 = [{\"type\": \"person\", \"name\": {\"preferred\": \"Adam J Granger\", \"index\": \"Granger, Adam J\"}}]\n        for i, award in enumerate(json_content):\n            if \"id\" in award and award[\"id\"] == \"par-1\":\n                if \"recipients\" not in award:\n                    json_content[i][\"recipients\"] = recipients_for_06412\n\n    return json_content"}
{"func_code_string": "def rewrite_elife_authors_json(json_content, doi):\n    \"\"\" this does the work of rewriting elife authors json \"\"\"\n\n    # Convert doi from testing doi if applicable\n    article_doi = elifetools.utils.convert_testing_doi(doi)\n\n    # Edge case fix an affiliation name\n    if article_doi == \"10.7554/eLife.06956\":\n        for i, ref in enumerate(json_content):\n            if ref.get(\"orcid\") and ref.get(\"orcid\") == \"0000-0001-6798-0064\":\n                json_content[i][\"affiliations\"][0][\"name\"] = [\"Cambridge\"]\n\n    # Edge case fix an ORCID\n    if article_doi == \"10.7554/eLife.09376\":\n        for i, ref in enumerate(json_content):\n            if ref.get(\"orcid\") and ref.get(\"orcid\") == \"000-0001-7224-925X\":\n                json_content[i][\"orcid\"] = \"0000-0001-7224-925X\"\n\n    # Edge case competing interests\n    if article_doi == \"10.7554/eLife.00102\":\n        for i, ref in enumerate(json_content):\n            if not ref.get(\"competingInterests\"):\n                if ref[\"name\"][\"index\"].startswith(\"Chen,\"):\n                    json_content[i][\"competingInterests\"] = \"ZJC: Reviewing Editor, <i>eLife</i>\"\n                elif ref[\"name\"][\"index\"].startswith(\"Li,\"):\n                    json_content[i][\"competingInterests\"] = \"The remaining authors have no competing interests to declare.\"\n    if article_doi == \"10.7554/eLife.00270\":\n        for i, ref in enumerate(json_content):\n            if not ref.get(\"competingInterests\"):\n                if ref[\"name\"][\"index\"].startswith(\"Patterson,\"):\n                    json_content[i][\"competingInterests\"] = \"MP: Managing Executive Editor, <i>eLife</i>\"\n\n    # Remainder of competing interests rewrites\n    elife_author_competing_interests = {}\n    elife_author_competing_interests[\"10.7554/eLife.00133\"] = \"The authors declare that no competing interests exist.\"\n    elife_author_competing_interests[\"10.7554/eLife.00190\"] = \"The authors declare that no competing interests exist.\"\n    elife_author_competing_interests[\"10.7554/eLife.00230\"] = \"The authors have declared that no competing interests exist\"\n    elife_author_competing_interests[\"10.7554/eLife.00288\"] = \"The authors declare that no competing interests exist.\"\n    elife_author_competing_interests[\"10.7554/eLife.00352\"] = \"The author declares that no competing interest exist\"\n    elife_author_competing_interests[\"10.7554/eLife.00362\"] = \"The authors declare that no competing interests exist.\"\n    elife_author_competing_interests[\"10.7554/eLife.00475\"] = \"The remaining authors have no competing interests to declare.\"\n    elife_author_competing_interests[\"10.7554/eLife.00592\"] = \"The other authors declare that no competing interests exist.\"\n    elife_author_competing_interests[\"10.7554/eLife.00633\"] = \"The authors declare that no competing interests exist.\"\n    elife_author_competing_interests[\"10.7554/eLife.02725\"] = \"The other authors declare that no competing interests exist.\"\n    elife_author_competing_interests[\"10.7554/eLife.02935\"] = \"The authors declare that no competing interests exist.\"\n    elife_author_competing_interests[\"10.7554/eLife.04126\"] = \"The authors declare that no competing interests exist.\"\n    elife_author_competing_interests[\"10.7554/eLife.04878\"] = \"The authors declare that no competing interests exist.\"\n    elife_author_competing_interests[\"10.7554/eLife.05322\"] = \"The other authors declare that no competing interests exist.\"\n    elife_author_competing_interests[\"10.7554/eLife.06011\"] = \"The authors declare that no competing interests exist.\"\n    elife_author_competing_interests[\"10.7554/eLife.06416\"] = \"The authors declare that no competing interests exist.\"\n    elife_author_competing_interests[\"10.7554/eLife.07383\"] = \"The other authors declare that no competing interests exist.\"\n    elife_author_competing_interests[\"10.7554/eLife.08421\"] = \"The authors declare that no competing interests exist.\"\n    elife_author_competing_interests[\"10.7554/eLife.08494\"] = \"The authors declare that no competing interests exist.\"\n    elife_author_competing_interests[\"10.7554/eLife.08648\"] = \"The other authors declare that no competing interests exist.\"\n    elife_author_competing_interests[\"10.7554/eLife.08924\"] = \"The authors declare that no competing interests exist.\"\n    elife_author_competing_interests[\"10.7554/eLife.09083\"] = \"The other authors declare that no competing interests exists.\"\n    elife_author_competing_interests[\"10.7554/eLife.09102\"] = \"The authors declare that no competing interests exist.\"\n    elife_author_competing_interests[\"10.7554/eLife.09460\"] = \"The authors declare that no competing interests exist.\"\n    elife_author_competing_interests[\"10.7554/eLife.09591\"] = \"The authors declare that no competing interests exist.\"\n    elife_author_competing_interests[\"10.7554/eLife.09600\"] = \"The authors declare that no competing interests exist.\"\n    elife_author_competing_interests[\"10.7554/eLife.10113\"] = \"The authors declare that no competing interests exist.\"\n    elife_author_competing_interests[\"10.7554/eLife.10230\"] = \"The authors declare that no competing interests exist.\"\n    elife_author_competing_interests[\"10.7554/eLife.10453\"] = \"The authors declare that no competing interests exist.\"\n    elife_author_competing_interests[\"10.7554/eLife.10635\"] = \"The authors declare that no competing interests exist.\"\n    elife_author_competing_interests[\"10.7554/eLife.11407\"] = \"The authors declare that no competing interests exist.\"\n    elife_author_competing_interests[\"10.7554/eLife.11473\"] = \"The authors declare that no competing interests exist.\"\n    elife_author_competing_interests[\"10.7554/eLife.11750\"] = \"The authors declare that no competing interests exist.\"\n    elife_author_competing_interests[\"10.7554/eLife.12217\"] = \"The authors declare that no competing interests exist.\"\n    elife_author_competing_interests[\"10.7554/eLife.12620\"] = \"The authors declare that no competing interests exist.\"\n    elife_author_competing_interests[\"10.7554/eLife.12724\"] = \"The authors declare that no competing interests exist.\"\n    elife_author_competing_interests[\"10.7554/eLife.13023\"] = \"The authors declare that no competing interests exist.\"\n    elife_author_competing_interests[\"10.7554/eLife.13732\"] = \"The authors declare that no competing interests exist.\"\n    elife_author_competing_interests[\"10.7554/eLife.14116\"] = \"The authors declare that no competing interests exist.\"\n    elife_author_competing_interests[\"10.7554/eLife.14258\"] = \"The authors declare that no competing interests exist.\"\n    elife_author_competing_interests[\"10.7554/eLife.14694\"] = \"The authors declare that no competing interests exist.\"\n    elife_author_competing_interests[\"10.7554/eLife.15085\"] = \"The other authors declare that no competing interests exist.\"\n    elife_author_competing_interests[\"10.7554/eLife.15312\"] = \"The other authors declare that no competing interests exist.\"\n    elife_author_competing_interests[\"10.7554/eLife.16011\"] = \"The authors declare that no competing interests exist.\"\n    elife_author_competing_interests[\"10.7554/eLife.16940\"] = \"The authors declare that no competing interests exist.\"\n    elife_author_competing_interests[\"10.7554/eLife.17023\"] = \"The authors declare that no competing interests exist.\"\n    elife_author_competing_interests[\"10.7554/eLife.17092\"] = \"The authors declare that no competing interests exist.\"\n    elife_author_competing_interests[\"10.7554/eLife.17218\"] = \"The authors declare that no competing interests exist.\"\n    elife_author_competing_interests[\"10.7554/eLife.17267\"] = \"The authors declare that no competing interests exist.\"\n    elife_author_competing_interests[\"10.7554/eLife.17523\"] = \"The authors declare that no competing interests exist.\"\n    elife_author_competing_interests[\"10.7554/eLife.17556\"] = \"The authors declare that no competing interests exist.\"\n    elife_author_competing_interests[\"10.7554/eLife.17769\"] = \"The authors declare that no competing interests exist.\"\n    elife_author_competing_interests[\"10.7554/eLife.17834\"] = \"The authors declare that no competing interests exist.\"\n    elife_author_competing_interests[\"10.7554/eLife.18101\"] = \"The authors declare that no competing interests exist.\"\n    elife_author_competing_interests[\"10.7554/eLife.18515\"] = \"The authors declare that no competing interests exist.\"\n    elife_author_competing_interests[\"10.7554/eLife.18544\"] = \"The authors declare that no competing interests exist.\"\n    elife_author_competing_interests[\"10.7554/eLife.18648\"] = \"The authors declare that no competing interests exist.\"\n    elife_author_competing_interests[\"10.7554/eLife.19071\"] = \"The authors declare that no competing interests exist.\"\n    elife_author_competing_interests[\"10.7554/eLife.19334\"] = \"The authors declare that no competing interests exist.\"\n    elife_author_competing_interests[\"10.7554/eLife.19510\"] = \"The authors declare that no competing interests exist.\"\n    elife_author_competing_interests[\"10.7554/eLife.20183\"] = \"The authors declare that no competing interests exist.\"\n    elife_author_competing_interests[\"10.7554/eLife.20242\"] = \"The authors declare that no competing interests exist.\"\n    elife_author_competing_interests[\"10.7554/eLife.20375\"] = \"The authors declare that no competing interests exist.\"\n    elife_author_competing_interests[\"10.7554/eLife.20797\"] = \"The other authors declare that no competing interests exist.\"\n    elife_author_competing_interests[\"10.7554/eLife.21454\"] = \"The authors declare that no competing interests exist.\"\n    elife_author_competing_interests[\"10.7554/eLife.21491\"] = \"The other authors declare that no competing interests exist.\"\n    elife_author_competing_interests[\"10.7554/eLife.22187\"] = \"The authors declare that no competing interests exist.\"\n\n    if article_doi in elife_author_competing_interests:\n        for i, ref in enumerate(json_content):\n            if not ref.get(\"competingInterests\"):\n                json_content[i][\"competingInterests\"] = elife_author_competing_interests[article_doi]\n\n    # Rewrite \"other authors declare\" ... competing interests statements using a string match\n    for i, ref in enumerate(json_content):\n        if (ref.get(\"competingInterests\") and (\n            ref.get(\"competingInterests\").startswith(\"The other author\") or\n            ref.get(\"competingInterests\").startswith(\"The others author\") or\n            ref.get(\"competingInterests\").startswith(\"The remaining authors\") or\n            ref.get(\"competingInterests\").startswith(\"The remaining have declared\")\n            )):\n            json_content[i][\"competingInterests\"] = \"No competing interests declared.\"\n\n    return json_content"}
{"func_code_string": "def rewrite_elife_datasets_json(json_content, doi):\n    \"\"\" this does the work of rewriting elife datasets json \"\"\"\n\n    # Add dates in bulk\n    elife_dataset_dates = []\n    elife_dataset_dates.append((\"10.7554/eLife.00348\", \"used\", \"dataro17\", u\"2010\"))\n    elife_dataset_dates.append((\"10.7554/eLife.01179\", \"used\", \"dataro4\", u\"2016\"))\n    elife_dataset_dates.append((\"10.7554/eLife.01603\", \"used\", \"dataro2\", u\"2012\"))\n    elife_dataset_dates.append((\"10.7554/eLife.02304\", \"used\", \"dataro15\", u\"2005\"))\n    elife_dataset_dates.append((\"10.7554/eLife.02935\", \"used\", \"dataro2\", u\"2014\"))\n    elife_dataset_dates.append((\"10.7554/eLife.03583\", \"used\", \"dataro5\", u\"2013\"))\n    if doi in map(lambda dataset: dataset[0], elife_dataset_dates):\n        for (match_doi, used_or_generated, id, dataset_date) in elife_dataset_dates:\n            if doi == match_doi:\n                if json_content.get(used_or_generated):\n                    for dataset in json_content[used_or_generated]:\n                        if dataset.get(\"id\") and dataset[\"id\"] == id:\n                            if not dataset.get(\"date\"):\n                                dataset[\"date\"] = dataset_date\n\n    # Continue with individual article JSON rewriting\n    if doi == \"10.7554/eLife.01311\":\n        if json_content.get(\"used\"):\n            for dataset in json_content[\"used\"]:\n                if dataset.get(\"id\") and dataset[\"id\"] in [\"dataro3\", \"dataro4\", \"dataro5\"]:\n                    if not dataset.get(\"date\"):\n                        dataset[\"date\"] = u\"2012\"\n                    if not dataset.get(\"authors\"):\n                        dataset[\"authors\"] = [{\"type\": \"group\", \"name\": \"Duke\"}]\n                if dataset.get(\"id\") and dataset[\"id\"] == \"dataro6\":\n                    if not dataset.get(\"date\"):\n                        dataset[\"date\"] = u\"2011\"\n                    if not dataset.get(\"authors\"):\n                        dataset[\"authors\"] = [{\"type\": \"group\", \"name\": \"FlyBase\"}]\n                if dataset.get(\"id\") and dataset[\"id\"] == \"dataro7\":\n                    if not dataset.get(\"date\"):\n                        dataset[\"date\"] = u\"2011\"\n                    if not dataset.get(\"authors\"):\n                        dataset[\"authors\"] = [{\"type\": \"group\", \"name\": \"Baylor College of Medicine (BCM)\"}]\n                if dataset.get(\"id\") and dataset[\"id\"] in [\"dataro8\", \"dataro9\"]:\n                    if not dataset.get(\"date\"):\n                        dataset[\"date\"] = u\"2012\"\n                    if not dataset.get(\"authors\"):\n                        dataset[\"authors\"] = [{\"type\": \"group\", \"name\": \"University of California, Berkeley\"}]\n\n    if doi == \"10.7554/eLife.01440\":\n        if json_content.get(\"used\"):\n            for dataset in json_content[\"used\"]:\n                if dataset.get(\"id\") and dataset[\"id\"] == \"dataro1\":\n                    if not dataset.get(\"authors\"):\n                        dataset[\"authors\"] = [{\"type\": \"group\", \"name\": \"EnsemblMetazoa\"}]\n\n    if doi == \"10.7554/eLife.01535\":\n        if json_content.get(\"used\"):\n            for dataset in json_content[\"used\"]:\n                if dataset.get(\"id\") and dataset[\"id\"] == \"dataro1\":\n                    if dataset.get(\"date\") and dataset.get(\"date\") == \"2000, 2005\":\n                        dataset[\"date\"] = u\"2000\"\n\n    if doi == \"10.7554/eLife.02304\":\n        if json_content.get(\"used\"):\n            for dataset in json_content[\"used\"]:\n                if dataset.get(\"id\") and dataset[\"id\"] == \"dataro11\":\n                    if not dataset.get(\"title\"):\n                        dataset[\"title\"] = u\"T.gondii LDH1 ternary complex with APAD+ and oxalate\"\n\n    if doi == \"10.7554/eLife.03574\":\n        if json_content.get(\"used\"):\n            for dataset in json_content[\"used\"]:\n                if dataset.get(\"id\") and dataset[\"id\"] == \"dataro2\":\n                    if not dataset.get(\"date\"):\n                        dataset[\"date\"] = u\"2006\"\n                    if not dataset.get(\"authors\"):\n                        dataset[\"authors\"] = [{\"type\": \"group\", \"name\": \"Riley,M.\"}, {\"type\": \"group\", \"name\": \"Abe,T.\"}, {\"type\": \"group\", \"name\": \"Arnaud,M.B.\"}, {\"type\": \"group\", \"name\": \"Berlyn,M.K.\"}, {\"type\": \"group\", \"name\": \"Blattner,F.R.\"}, {\"type\": \"group\", \"name\": \"Chaudhuri,R.R.\"}, {\"type\": \"group\", \"name\": \"Glasner,J.D.\"}, {\"type\": \"group\", \"name\": \"Horiuchi,T.\"}, {\"type\": \"group\", \"name\": \"Keseler,I.M.\"}, {\"type\": \"group\", \"name\": \"Kosuge,T.\"}, {\"type\": \"group\", \"name\": \"Mori,H.\"}, {\"type\": \"group\", \"name\": \"Perna,N.T.\"}, {\"type\": \"group\", \"name\": \"Plunkett,G. III\"}, {\"type\": \"group\", \"name\": \"Rudd,K.E.\"}, {\"type\": \"group\", \"name\": \"Serres,M.H.\"}, {\"type\": \"group\", \"name\": \"Thomas,G.H.\"}, {\"type\": \"group\", \"name\": \"Thomson,N.R.\"}, {\"type\": \"group\", \"name\": \"Wishart,D.\"}, {\"type\": \"group\", \"name\": \"Wanner,B.L.\"}]\n\n    if doi == \"10.7554/eLife.03676\":\n        if json_content.get(\"used\"):\n            for dataset in json_content[\"used\"]:\n                if dataset.get(\"id\") and dataset[\"id\"] == \"dataro4\":\n                    if not dataset.get(\"date\"):\n                        dataset[\"date\"] = u\"2013\"\n                    if not dataset.get(\"authors\"):\n                        dataset[\"authors\"] = [{\"type\": \"group\", \"name\": \"Human Gene Sequencing Center\"}]\n\n    if doi == \"10.7554/eLife.03971\":\n        if json_content.get(\"used\"):\n            for dataset in json_content[\"used\"]:\n                if dataset.get(\"id\") and dataset[\"id\"] == \"dataro2\":\n                    if not dataset.get(\"authors\"):\n                        dataset[\"authors\"] = [{\"type\": \"group\", \"name\": \"Vanderperre B.\"}]\n\n    if doi == \"10.7554/eLife.04660\":\n        if json_content.get(\"generated\"):\n            for dataset in json_content[\"generated\"]:\n                if dataset.get(\"id\") and dataset[\"id\"] == \"dataro1\":\n                    if dataset.get(\"date\") and dataset.get(\"date\") == \"2014-2015\":\n                        dataset[\"date\"] = u\"2014\"\n\n    if doi == \"10.7554/eLife.06421\":\n        if json_content.get(\"used\"):\n            for dataset in json_content[\"used\"]:\n                if dataset.get(\"id\") and dataset[\"id\"] == \"dataro2\":\n                    if dataset.get(\"date\") and dataset.get(\"date\") == \"NA\":\n                        dataset[\"date\"] = u\"2006\"\n\n    if doi == \"10.7554/eLife.08445\":\n        if json_content.get(\"used\"):\n            for dataset in json_content[\"used\"]:\n                if dataset.get(\"id\") and dataset[\"id\"] == \"data-ro1\":\n                    if not dataset.get(\"date\"):\n                        dataset[\"date\"] = u\"2006\"\n                    if not dataset.get(\"authors\"):\n                        dataset[\"authors\"] = [{\"type\": \"group\", \"name\": \"BDTNP SELEX\"}]\n\n    if doi == \"10.7554/eLife.08916\":\n        if json_content.get(\"used\"):\n            for dataset in json_content[\"used\"]:\n                if dataset.get(\"id\") and dataset[\"id\"] == \"dataro2\":\n                    if dataset.get(\"date\") and dataset.get(\"date\") == \"2008, updated 2014\":\n                        dataset[\"date\"] = u\"2008\"\n                if dataset.get(\"id\") and dataset[\"id\"] == \"dataro3\":\n                    if dataset.get(\"date\") and dataset.get(\"date\") == \"2013, updated 2014\":\n                        dataset[\"date\"] = u\"2013\"\n\n    if doi == \"10.7554/eLife.08955\":\n        if json_content.get(\"generated\"):\n            for dataset in json_content[\"generated\"]:\n                if dataset.get(\"id\") and dataset[\"id\"] == \"dataro2\":\n                    if not dataset.get(\"authors\"):\n                        dataset[\"authors\"] = [{\"type\": \"group\", \"name\": \"Kurdistani S\"}, {\"type\": \"group\", \"name\": \"Marrban C\"}, {\"type\": \"group\", \"name\": \"Su T\"}]\n\n    if doi == \"10.7554/eLife.09207\":\n        if json_content.get(\"used\"):\n            for dataset in json_content[\"used\"]:\n                if dataset.get(\"id\") and dataset[\"id\"] == \"dataro1\":\n                    if not dataset.get(\"authors\"):\n                        dataset[\"authors\"] = [{\"type\": \"group\", \"name\": \"Prostate Cancer Genome Sequencing Project\"}]\n\n    if doi == \"10.7554/eLife.10607\":\n        if json_content.get(\"generated\"):\n            for dataset in json_content[\"generated\"]:\n                if dataset.get(\"id\") and dataset[\"id\"] == \"data-ro4\":\n                    if not dataset.get(\"authors\"):\n                        dataset[\"authors\"] = [{\"type\": \"group\", \"name\": \"Authors\"}]\n\n    if doi == \"10.7554/eLife.10670\":\n        if json_content.get(\"used\"):\n            for dataset in json_content[\"used\"]:\n                if dataset.get(\"id\") and dataset[\"id\"] == \"data-ro1\":\n                    if not dataset.get(\"authors\"):\n                        dataset[\"authors\"] = [{\"type\": \"group\", \"name\": \"HIVdb\"}]\n\n    # Add dates, authors, other details\n    if doi == \"10.7554/eLife.10856\":\n        if json_content.get(\"generated\"):\n            datasets_authors_for_10856 = [{\"type\": \"group\", \"name\": \"Dagdas YF\"}, {\"type\": \"group\", \"name\": \"Belhaj K\"}, {\"type\": \"group\", \"name\": \"Maqbool A\"}, {\"type\": \"group\", \"name\": \"Chaparro-Garcia A\"}, {\"type\": \"group\", \"name\": \"Pandey P\"}, {\"type\": \"group\", \"name\": \"Petre B\"}, {\"type\": \"group\", \"name\": \"Tabassum N\"}, {\"type\": \"group\", \"name\": \"Cruz-Mireles N\"}, {\"type\": \"group\", \"name\": \"Hughes RK\"}, {\"type\": \"group\", \"name\": \"Sklenar J\"}, {\"type\": \"group\", \"name\": \"Win J\"}, {\"type\": \"group\", \"name\": \"Menke F\"}, {\"type\": \"group\", \"name\": \"Findlay K\"}, {\"type\": \"group\", \"name\": \"Banfield MJ\"}, {\"type\": \"group\", \"name\": \"Kamoun S\"}, {\"type\": \"group\", \"name\": \"Bozkurt TO\"}]\n            for dataset in json_content[\"generated\"]:\n                if dataset.get(\"id\") and dataset[\"id\"] == \"dataro7\":\n                    if not dataset.get(\"date\"):\n                        dataset[\"date\"] = u\"2016\"\n                    if not dataset.get(\"title\"):\n                        dataset[\"title\"] = u\"An effector of the Irish potato famine pathogen antagonizes a host autophagy cargo receptor\"\n                    if not dataset.get(\"authors\"):\n                        dataset[\"authors\"] = datasets_authors_for_10856\n                    if dataset.get(\"uri\") and dataset[\"uri\"] == \"http://www.ncbi.nlm.nih.\":\n                         dataset[\"uri\"] = \"https://www.ncbi.nlm.nih.gov/nuccore/976151098/\"\n                if dataset.get(\"id\") and dataset[\"id\"] == \"dataro8\":\n                    if not dataset.get(\"date\"):\n                        dataset[\"date\"] = u\"2015\"\n                    if not dataset.get(\"title\"):\n                        dataset[\"title\"] = u\"An effector of the Irish potato famine pathogen antagonizes a host autophagy cargo receptor\"\n                    if not dataset.get(\"authors\"):\n                        dataset[\"authors\"] = datasets_authors_for_10856\n                    if dataset.get(\"uri\") and dataset[\"uri\"] == \"http://www.ncbi.nlm.nih.\":\n                         dataset[\"uri\"] = \"https://www.ncbi.nlm.nih.gov/nuccore/976151096/\"\n                if dataset.get(\"id\") and dataset[\"id\"] == \"dataro9\":\n                    if not dataset.get(\"authors\"):\n                        dataset[\"authors\"] = datasets_authors_for_10856\n\n    if doi == \"10.7554/eLife.10877\":\n        if json_content.get(\"generated\"):\n            for dataset in json_content[\"generated\"]:\n                if dataset.get(\"id\") and dataset[\"id\"] == \"dataro1\":\n                    if not dataset.get(\"title\"):\n                        dataset[\"title\"] = u\"Oct4 ChIP-Seq at G1 and G2/M phase of cell cycle in mouse embryonic stem cells\"\n\n    if doi == \"10.7554/eLife.10921\":\n        if json_content.get(\"generated\"):\n            for dataset in json_content[\"generated\"]:\n                if dataset.get(\"id\") and dataset[\"id\"] == \"dataro1\":\n                    if not dataset.get(\"authors\"):\n                        dataset[\"authors\"] = [{\"type\": \"group\", \"name\": \"Floor SN\"}, {\"type\": \"group\", \"name\": \"Doudna JA\"}]\n        if json_content.get(\"used\"):\n            for dataset in json_content[\"used\"]:\n                if dataset.get(\"id\") and dataset[\"id\"] == \"dataro2\":\n                    if not dataset.get(\"authors\"):\n                        dataset[\"authors\"] = [{\"type\": \"group\", \"name\": \"Sidrauski C\"}, {\"type\": \"group\", \"name\": \"McGeachy A\"}, {\"type\": \"group\", \"name\": \"Ingolia N\"}, {\"type\": \"group\", \"name\": \"Walter P\"}]\n\n    if doi == \"10.7554/eLife.11117\":\n        if json_content.get(\"used\"):\n            for dataset in json_content[\"used\"]:\n                if dataset.get(\"id\") and dataset[\"id\"] == \"dataro14\":\n                    if not dataset.get(\"authors\"):\n                        dataset[\"authors\"] = [{\"type\": \"group\", \"name\": \"Authors\"}]\n\n    if doi == \"10.7554/eLife.12204\":\n        if json_content.get(\"used\"):\n            for dataset in json_content[\"used\"]:\n                if dataset.get(\"id\") and dataset[\"id\"] == \"dataro1\":\n                    if not dataset.get(\"authors\"):\n                        dataset[\"authors\"] = [{\"type\": \"group\", \"name\": \"Rhodes DR\"}, {\"type\": \"group\", \"name\": \"Kalyana-Sundaram S\"}, {\"type\": \"group\", \"name\": \"Mahavisno V\"}, {\"type\": \"group\", \"name\": \"Varambally R\"}, {\"type\": \"group\", \"name\": \"Yu J\"}, {\"type\": \"group\", \"name\": \"Briggs BB\"}, {\"type\": \"group\", \"name\": \"Barrette TR\"}, {\"type\": \"group\", \"name\": \"Anstet MJ\"}, {\"type\": \"group\", \"name\": \"Kincead-Beal C\"}, {\"type\": \"group\", \"name\": \"Kulkarni P\"}, {\"type\": \"group\", \"name\": \"Varambally S\"}, {\"type\": \"group\", \"name\": \"Ghosh D\"}, {\"type\": \"group\", \"name\": \"Chinnaiyan AM.\"}]\n                if dataset.get(\"id\") and dataset[\"id\"] == \"dataro2\":\n                    if not dataset.get(\"authors\"):\n                        dataset[\"authors\"] = [{\"type\": \"group\", \"name\": \"Gaspar C\"}, {\"type\": \"group\", \"name\": \"Cardoso J\"}, {\"type\": \"group\", \"name\": \"Franken P\"}, {\"type\": \"group\", \"name\": \"Molenaar L\"}, {\"type\": \"group\", \"name\": \"Morreau H\"}, {\"type\": \"group\", \"name\": \"M\u00f6slein G\"}, {\"type\": \"group\", \"name\": \"Sampson J\"}, {\"type\": \"group\", \"name\": \"Boer JM\"}, {\"type\": \"group\", \"name\": \"de Menezes RX\"}, {\"type\": \"group\", \"name\": \"Fodde R.\"}]\n                if dataset.get(\"id\") and dataset[\"id\"] == \"dataro3\":\n                    if not dataset.get(\"authors\"):\n                        dataset[\"authors\"] = [{\"type\": \"group\", \"name\": \"Graudens E\"}, {\"type\": \"group\", \"name\": \"Boulanger V\"}, {\"type\": \"group\", \"name\": \"Mollard C\"}, {\"type\": \"group\", \"name\": \"Mariage-Samson R\"}, {\"type\": \"group\", \"name\": \"Barlet X\"}, {\"type\": \"group\", \"name\": \"Gr\u00e9my G\"}, {\"type\": \"group\", \"name\": \"Couillault C\"}, {\"type\": \"group\", \"name\": \"Laj\u00e9mi M\"}, {\"type\": \"group\", \"name\": \"Piatier-Tonneau D\"}, {\"type\": \"group\", \"name\": \"Zaborski P\"}, {\"type\": \"group\", \"name\": \"Eveno E\"}, {\"type\": \"group\", \"name\": \"Auffray C\"}, {\"type\": \"group\", \"name\": \"Imbeaud S.\"}]\n                if dataset.get(\"id\") and dataset[\"id\"] == \"dataro4\":\n                    if not dataset.get(\"authors\"):\n                        dataset[\"authors\"] = [{\"type\": \"group\", \"name\": \"Hong Y\"}, {\"type\": \"group\", \"name\": \"Downey T\"}, {\"type\": \"group\", \"name\": \"Eu KW\"}, {\"type\": \"group\", \"name\": \"Koh PK\"},{\"type\": \"group\", \"name\": \"Cheah PY\"}]\n                if dataset.get(\"id\") and dataset[\"id\"] == \"dataro5\":\n                    if not dataset.get(\"authors\"):\n                        dataset[\"authors\"] = [{\"type\": \"group\", \"name\": \"Kaiser S\"}, {\"type\": \"group\", \"name\": \"Park YK\"}, {\"type\": \"group\", \"name\": \"Franklin JL\"}, {\"type\": \"group\", \"name\": \"Halberg RB\"}, {\"type\": \"group\", \"name\": \"Yu M\"}, {\"type\": \"group\", \"name\": \"Jessen WJ\"}, {\"type\": \"group\", \"name\": \"Freudenberg J\"}, {\"type\": \"group\", \"name\": \"Chen X\"}, {\"type\": \"group\", \"name\": \"Haigis K\"}, {\"type\": \"group\", \"name\": \"Jegga AG\"}, {\"type\": \"group\", \"name\": \"Kong S\"}, {\"type\": \"group\", \"name\": \"Sakthivel B\"}, {\"type\": \"group\", \"name\": \"Xu H\"}, {\"type\": \"group\", \"name\": \"Reichling T\"}, {\"type\": \"group\", \"name\": \"Azhar M\"}, {\"type\": \"group\", \"name\": \"Boivin GP\"}, {\"type\": \"group\", \"name\": \"Roberts RB\"}, {\"type\": \"group\", \"name\": \"Bissahoyo AC\"}, {\"type\": \"group\", \"name\": \"Gonzales F\"}, {\"type\": \"group\", \"name\": \"Bloom GC\"}, {\"type\": \"group\", \"name\": \"Eschrich S\"}, {\"type\": \"group\", \"name\": \"Carter SL\"}, {\"type\": \"group\", \"name\": \"Aronow JE\"}, {\"type\": \"group\", \"name\": \"Kleimeyer J\"}, {\"type\": \"group\", \"name\": \"Kleimeyer M\"}, {\"type\": \"group\", \"name\": \"Ramaswamy V\"}, {\"type\": \"group\", \"name\": \"Settle SH\"}, {\"type\": \"group\", \"name\": \"Boone B\"}, {\"type\": \"group\", \"name\": \"Levy S\"}, {\"type\": \"group\", \"name\": \"Graff JM\"}, {\"type\": \"group\", \"name\": \"Doetschman T\"}, {\"type\": \"group\", \"name\": \"Groden J\"}, {\"type\": \"group\", \"name\": \"Dove WF\"}, {\"type\": \"group\", \"name\": \"Threadgill DW\"}, {\"type\": \"group\", \"name\": \"Yeatman TJ\"}, {\"type\": \"group\", \"name\": \"Coffey RJ Jr\"}, {\"type\": \"group\", \"name\": \"Aronow BJ.\"}]\n                if dataset.get(\"id\") and dataset[\"id\"] == \"dataro6\":\n                    if not dataset.get(\"authors\"):\n                        dataset[\"authors\"] = [{\"type\": \"group\", \"name\": \"Muzny DM et al\"}]\n                if dataset.get(\"id\") and dataset[\"id\"] == \"dataro7\":\n                    if not dataset.get(\"authors\"):\n                        dataset[\"authors\"] = [{\"type\": \"group\", \"name\": \"Skrzypczak M\"}, {\"type\": \"group\", \"name\": \"Goryca K\"}, {\"type\": \"group\", \"name\": \"Rubel T\"}, {\"type\": \"group\", \"name\": \"Paziewska A\"}, {\"type\": \"group\", \"name\": \"Mikula M\"}, {\"type\": \"group\", \"name\": \"Jarosz D\"}, {\"type\": \"group\", \"name\": \"Pachlewski J\"}, {\"type\": \"group\", \"name\": \"Oledzki J\"}, {\"type\": \"group\", \"name\": \"Ostrowski J.\"}]\n                if dataset.get(\"id\") and dataset[\"id\"] == \"dataro8\":\n                    if not dataset.get(\"authors\"):\n                        dataset[\"authors\"] = [{\"type\": \"group\", \"name\": \"Cancer Genome Atlas Network\"}]\n\n    if doi == \"10.7554/eLife.12876\":\n        if json_content.get(\"used\"):\n            for dataset in json_content[\"used\"]:\n                if dataset.get(\"id\") and dataset[\"id\"] == \"dataro1\":\n                    if not dataset.get(\"authors\"):\n                        dataset[\"authors\"] = [{\"type\": \"group\", \"name\": \"Department of Human Genetics, University of Utah\"}]\n\n    if doi == \"10.7554/eLife.13195\":\n        if json_content.get(\"generated\"):\n            for dataset in json_content[\"generated\"]:\n                if dataset.get(\"id\") and dataset[\"id\"] == \"dataro1\":\n                    if not dataset.get(\"authors\"):\n                        dataset[\"authors\"] = [{\"type\": \"group\", \"name\": \"Microbial Ecology Group, Colorado State University\"}]\n\n    if doi == \"10.7554/eLife.14158\":\n        if json_content.get(\"generated\"):\n            for dataset in json_content[\"generated\"]:\n                if dataset.get(\"id\") and dataset[\"id\"] == \"data-ro1\":\n                    if not dataset.get(\"title\"):\n                        dataset[\"title\"] = u\"Bacterial initiation protein\"\n                if dataset.get(\"id\") and dataset[\"id\"] == \"data-ro2\":\n                    if not dataset.get(\"title\"):\n                        dataset[\"title\"] = u\"Bacterial initiation protein in complex with Phage inhibitor protein\"\n        if json_content.get(\"used\"):\n            for dataset in json_content[\"used\"]:\n                if dataset.get(\"id\") and dataset[\"id\"] == \"dataro3\":\n                    if not dataset.get(\"date\"):\n                        dataset[\"date\"] = u\"2007\"\n\n    if doi == \"10.7554/eLife.14243\":\n        if json_content.get(\"generated\"):\n            for dataset in json_content[\"generated\"]:\n                if dataset.get(\"id\") and dataset[\"id\"] == \"dataro2\":\n                    if not dataset.get(\"authors\"):\n                        dataset[\"authors\"] = [{\"type\": \"group\", \"name\": \"Tramantano M\"}, {\"type\": \"group\", \"name\": \"Sun L\"}, {\"type\": \"group\", \"name\": \"Au C\"}, {\"type\": \"group\", \"name\": \"Labuz D\"}, {\"type\": \"group\", \"name\": \"Liu Z\"}, {\"type\": \"group\", \"name\": \"Chou M\"}, {\"type\": \"group\", \"name\": \"Shen C\"}, {\"type\": \"group\", \"name\": \"Luk E\"}]\n\n\n    if doi == \"10.7554/eLife.16078\":\n        if json_content.get(\"generated\"):\n            for dataset in json_content[\"generated\"]:\n                if dataset.get(\"id\") and dataset[\"id\"] == \"dataro1\":\n                    if dataset.get(\"date\") and dataset.get(\"date\") == \"current manuscript\":\n                        dataset[\"date\"] = u\"2016\"\n\n    if doi == \"10.7554/eLife.17082\":\n        if json_content.get(\"used\"):\n            for dataset in json_content[\"used\"]:\n                if dataset.get(\"id\") and dataset[\"id\"] == \"data-ro4\":\n                    if not dataset.get(\"date\"):\n                        dataset[\"date\"] = u\"2012\"\n                if dataset.get(\"id\") and dataset[\"id\"] == \"data-ro5\":\n                    if not dataset.get(\"date\"):\n                        dataset[\"date\"] = u\"2014\"\n                if dataset.get(\"id\") and dataset[\"id\"] == \"data-ro6\":\n                    if not dataset.get(\"date\"):\n                        dataset[\"date\"] = u\"2014\"\n                    if not dataset.get(\"authors\"):\n                        dataset[\"authors\"] = [{\"type\": \"group\", \"name\": \"The Cancer Genome Atlas (TCGA)\"}]\n\n    if doi == \"10.7554/eLife.17473\":\n        if json_content.get(\"generated\"):\n            for dataset in json_content[\"generated\"]:\n                if dataset.get(\"id\") and dataset[\"id\"] == \"dataro1\":\n                    if dataset.get(\"date\") and dataset.get(\"date\").startswith(\"Release date\"):\n                        dataset[\"date\"] = u\"2016\"\n\n    return json_content"}
{"func_code_string": "def rewrite_elife_editors_json(json_content, doi):\n    \"\"\" this does the work of rewriting elife editors json \"\"\"\n\n    # Remove affiliations with no name value\n    for i, ref in enumerate(json_content):\n        if ref.get(\"affiliations\"):\n            for aff in ref.get(\"affiliations\"):\n                if \"name\" not in aff:\n                    del(json_content[i][\"affiliations\"])\n\n    # Add editor role\n    editor_roles = {}\n    editor_roles[\"10.7554/eLife.00534\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.09376\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.10056\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.11031\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.12081\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.12241\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.12509\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.13023\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.13053\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.13426\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.13620\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.13810\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.13828\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.13887\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.13905\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.14000\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.14155\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.14170\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.14226\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.14277\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.14315\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.14316\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.14530\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.14601\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.14618\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.14749\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.14814\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.15266\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.15275\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.15292\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.15316\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.15470\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.15545\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.15716\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.15747\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.15828\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.15833\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.15915\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.15986\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.16088\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.16093\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.16127\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.16159\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.16178\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.16309\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.16777\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.16793\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.16950\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.17101\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.17180\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.17240\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.17262\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.17282\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.17463\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.17551\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.17667\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.17681\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.17978\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.17985\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.18103\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.18207\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.18246\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.18249\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.18432\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.18447\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.18458\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.18491\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.18541\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.18542\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.18579\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.18605\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.18633\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.18657\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.18919\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.18970\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.19027\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.19088\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.19089\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.19295\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.19377\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.19406\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.19466\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.19484\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.19505\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.19535\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.19568\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.19573\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.19662\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.19671\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.19686\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.19695\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.19720\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.19749\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.19766\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.19804\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.19809\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.19887\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.19976\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.19991\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.20010\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.20054\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.20070\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.20183\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.20185\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.20214\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.20236\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.20309\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.20343\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.20357\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.20362\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.20365\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.20390\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.20417\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.20515\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.20533\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.20607\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.20640\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.20667\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.20718\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.20722\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.20777\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.20782\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.20787\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.20797\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.20799\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.20813\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.20954\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.20958\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.20985\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.21032\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.21049\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.21052\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.21170\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.21172\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.21290\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.21330\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.21394\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.21397\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.21455\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.21481\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.21491\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.21589\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.21598\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.21616\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.21635\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.21728\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.21771\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.21776\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.21855\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.21886\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.21920\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.21989\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.22028\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.22053\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.22170\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.22177\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.22280\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.22409\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.22429\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.22431\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.22467\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.22472\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.22502\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.22771\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.22784\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.22866\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.23156\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.23352\"] = \"Reviewing Editor\"\n    editor_roles[\"10.7554/eLife.23804\"] = \"Reviewing Editor\"\n\n    # Edge case fix an affiliation name\n    if doi in editor_roles:\n        for i, ref in enumerate(json_content):\n            if not ref.get(\"role\"):\n                json_content[i][\"role\"] = editor_roles[doi]\n            elif ref.get(\"role\"):\n                json_content[i][\"role\"] = \"Reviewing Editor\"\n    else:\n        # Fix capitalisation on exiting role values\n        for i, ref in enumerate(json_content):\n            if ref.get(\"role\") == \"Reviewing editor\":\n                json_content[i][\"role\"] = \"Reviewing Editor\"\n\n    # Remove duplicates\n    editors_kept = []\n    for i, ref in enumerate(json_content):\n        editor_values = OrderedDict()\n        editor_values[\"role\"] = ref.get(\"role\")\n        if ref.get(\"name\"):\n            editor_values[\"name\"] = ref.get(\"name\").get(\"index\")\n        if editor_values in editors_kept:\n            # remove if one is already kept\n            del json_content[i]\n        else:\n            editors_kept.append(editor_values)\n\n    # Merge two role values\n    role_replacements = [\n        {\n            \"role_from\": [\"Senior Editor\", \"Reviewing Editor\"],\n            \"role_to\": \"Senior and Reviewing Editor\"}\n    ]\n    for replace_rule in role_replacements:\n        same_name_map = person_same_name_map(json_content, replace_rule.get('role_from'))\n        role_is_set = None\n        for same_id_list in same_name_map.values():\n            if not same_id_list or len(same_id_list) <= 1:\n                # no more than one name match, nothing to replace\n                continue\n            deleted_count = 0\n            for same_id in same_id_list:\n                if not role_is_set:\n                    # reset the role for the first person record\n                    json_content[same_id][\"role\"] = replace_rule.get(\"role_to\")\n                    role_is_set = True\n                else:\n                    # first one is already set, remove the duplicates\n                    del json_content[same_id-deleted_count]\n                    deleted_count += 1\n\n    return json_content"}
{"func_code_string": "def person_same_name_map(json_content, role_from):\n    \"to merge multiple editors into one record, filter by role values and group by name\"\n    matched_editors = [(i, person) for i, person in enumerate(json_content)\n                       if person.get('role') in role_from]\n    same_name_map = {}\n    for i, editor in matched_editors:\n        if not editor.get(\"name\"):\n            continue\n        # compare name of each\n        name = editor.get(\"name\").get(\"index\")\n        if name not in same_name_map:\n            same_name_map[name] = []\n        same_name_map[name].append(i)\n    return same_name_map"}
{"func_code_string": "def rewrite_elife_title_prefix_json(json_content, doi):\n    \"\"\" this does the work of rewriting elife title prefix json values\"\"\"\n    if not json_content:\n        return json_content\n\n    # title prefix rewrites by article DOI\n    title_prefix_values = {}\n    title_prefix_values[\"10.7554/eLife.00452\"] = \"Point of View\"\n    title_prefix_values[\"10.7554/eLife.00615\"] = \"Point of View\"\n    title_prefix_values[\"10.7554/eLife.00639\"] = \"Point of View\"\n    title_prefix_values[\"10.7554/eLife.00642\"] = \"Point of View\"\n    title_prefix_values[\"10.7554/eLife.00856\"] = \"Point of View\"\n    title_prefix_values[\"10.7554/eLife.01061\"] = \"Point of View\"\n    title_prefix_values[\"10.7554/eLife.01138\"] = \"Point of View\"\n    title_prefix_values[\"10.7554/eLife.01139\"] = \"Point of View\"\n    title_prefix_values[\"10.7554/eLife.01820\"] = \"Animal Models of Disease\"\n    title_prefix_values[\"10.7554/eLife.02576\"] = \"Point of View\"\n    title_prefix_values[\"10.7554/eLife.04902\"] = \"Point of View\"\n    title_prefix_values[\"10.7554/eLife.05614\"] = \"Point of View\"\n    title_prefix_values[\"10.7554/eLife.05635\"] = \"The Natural History of Model Organisms\"\n    title_prefix_values[\"10.7554/eLife.05826\"] = \"The Natural History of Model Organisms\"\n    title_prefix_values[\"10.7554/eLife.05835\"] = \"The Natural History of Model Organisms\"\n    title_prefix_values[\"10.7554/eLife.05849\"] = \"The Natural History of Model Organisms\"\n    title_prefix_values[\"10.7554/eLife.05861\"] = \"The Natural History of Model Organisms\"\n    title_prefix_values[\"10.7554/eLife.05959\"] = \"The Natural History of Model Organisms\"\n    title_prefix_values[\"10.7554/eLife.06024\"] = \"The Natural History of Model Organisms\"\n    title_prefix_values[\"10.7554/eLife.06100\"] = \"The Natural History of Model Organisms\"\n    title_prefix_values[\"10.7554/eLife.06793\"] = \"The Natural History of Model Organisms\"\n    title_prefix_values[\"10.7554/eLife.06813\"] = \"The Natural History of Model Organisms\"\n    title_prefix_values[\"10.7554/eLife.06956\"] = \"The Natural History of Model Organisms\"\n    title_prefix_values[\"10.7554/eLife.09305\"] = \"Point of View\"\n    title_prefix_values[\"10.7554/eLife.10825\"] = \"Point of View\"\n    title_prefix_values[\"10.7554/eLife.11628\"] = \"Living Science\"\n    title_prefix_values[\"10.7554/eLife.12708\"] = \"Point of View\"\n    title_prefix_values[\"10.7554/eLife.12844\"] = \"Point of View\"\n    title_prefix_values[\"10.7554/eLife.13035\"] = \"Point of View\"\n    title_prefix_values[\"10.7554/eLife.14258\"] = \"Cutting Edge\"\n    title_prefix_values[\"10.7554/eLife.14424\"] = \"Point of View\"\n    title_prefix_values[\"10.7554/eLife.14511\"] = \"Cell Proliferation\"\n    title_prefix_values[\"10.7554/eLife.14721\"] = \"Intracellular Bacteria\"\n    title_prefix_values[\"10.7554/eLife.14790\"] = \"Decision Making\"\n    title_prefix_values[\"10.7554/eLife.14830\"] = \"Progenitor Cells\"\n    title_prefix_values[\"10.7554/eLife.14953\"] = \"Gene Expression\"\n    title_prefix_values[\"10.7554/eLife.14973\"] = \"Breast Cancer\"\n    title_prefix_values[\"10.7554/eLife.15352\"] = \"Autoimmune Disorders\"\n    title_prefix_values[\"10.7554/eLife.15438\"] = \"Motor Circuits\"\n    title_prefix_values[\"10.7554/eLife.15591\"] = \"Protein Tagging\"\n    title_prefix_values[\"10.7554/eLife.15928\"] = \"Point of View\"\n    title_prefix_values[\"10.7554/eLife.15938\"] = \"Cancer Metabolism\"\n    title_prefix_values[\"10.7554/eLife.15957\"] = \"Stem Cells\"\n    title_prefix_values[\"10.7554/eLife.15963\"] = \"Prediction Error\"\n    title_prefix_values[\"10.7554/eLife.16019\"] = \"Social Networks\"\n    title_prefix_values[\"10.7554/eLife.16076\"] = \"mRNA Decay\"\n    title_prefix_values[\"10.7554/eLife.16207\"] = \"Cardiac Development\"\n    title_prefix_values[\"10.7554/eLife.16209\"] = \"Neural Coding\"\n    title_prefix_values[\"10.7554/eLife.16393\"] = \"Neural Circuits\"\n    title_prefix_values[\"10.7554/eLife.16598\"] = \"RNA Localization\"\n    title_prefix_values[\"10.7554/eLife.16758\"] = \"Adaptive Evolution\"\n    title_prefix_values[\"10.7554/eLife.16800\"] = \"Point of View\"\n    title_prefix_values[\"10.7554/eLife.16846\"] = \"Living Science\"\n    title_prefix_values[\"10.7554/eLife.16931\"] = \"Point of View\"\n    title_prefix_values[\"10.7554/eLife.16964\"] = \"Ion Channels\"\n    title_prefix_values[\"10.7554/eLife.17224\"] = \"Host-virus Interactions\"\n    title_prefix_values[\"10.7554/eLife.17293\"] = \"Ion Channels\"\n    title_prefix_values[\"10.7554/eLife.17393\"] = \"Point of View\"\n    title_prefix_values[\"10.7554/eLife.17394\"] = \"p53 Family Proteins\"\n    title_prefix_values[\"10.7554/eLife.18203\"] = \"Antibody Engineering\"\n    title_prefix_values[\"10.7554/eLife.18243\"] = \"Host-virus Interactions\"\n    title_prefix_values[\"10.7554/eLife.18365\"] = \"DNA Repair\"\n    title_prefix_values[\"10.7554/eLife.18431\"] = \"Unfolded Protein Response\"\n    title_prefix_values[\"10.7554/eLife.18435\"] = \"Long Distance Transport\"\n    title_prefix_values[\"10.7554/eLife.18721\"] = \"Decision Making\"\n    title_prefix_values[\"10.7554/eLife.18753\"] = \"Resource Competition\"\n    title_prefix_values[\"10.7554/eLife.18871\"] = \"Mathematical Modeling\"\n    title_prefix_values[\"10.7554/eLife.18887\"] = \"Sensorimotor Transformation\"\n    title_prefix_values[\"10.7554/eLife.19285\"] = \"Genetic Screen\"\n    title_prefix_values[\"10.7554/eLife.19351\"] = \"Motor Control\"\n    title_prefix_values[\"10.7554/eLife.19405\"] = \"Membrane Structures\"\n    title_prefix_values[\"10.7554/eLife.19733\"] = \"Focal Adhesions\"\n    title_prefix_values[\"10.7554/eLife.20043\"] = \"Amyloid-beta Peptides\"\n    title_prefix_values[\"10.7554/eLife.20314\"] = \"Plant Reproduction\"\n    title_prefix_values[\"10.7554/eLife.20468\"] = \"Endoplasmic Reticulum\"\n    title_prefix_values[\"10.7554/eLife.20516\"] = \"Innate Like Lymphocytes\"\n    title_prefix_values[\"10.7554/eLife.21070\"] = \"Scientific Publishing\"\n    title_prefix_values[\"10.7554/eLife.21236\"] = \"Developmental Neuroscience\"\n    title_prefix_values[\"10.7554/eLife.21522\"] = \"Developmental Neuroscience\"\n    title_prefix_values[\"10.7554/eLife.21723\"] = \"Living Science\"\n    title_prefix_values[\"10.7554/eLife.21863\"] = \"Genetic Screening\"\n    title_prefix_values[\"10.7554/eLife.21864\"] = \"Evolutionary Biology\"\n    title_prefix_values[\"10.7554/eLife.22073\"] = \"Unfolded Protein Response\"\n    title_prefix_values[\"10.7554/eLife.22186\"] = \"Point of View\"\n    title_prefix_values[\"10.7554/eLife.22215\"] = \"Neural Wiring\"\n    title_prefix_values[\"10.7554/eLife.22256\"] = \"Molecular Communication\"\n    title_prefix_values[\"10.7554/eLife.22471\"] = \"Point of View\"\n    title_prefix_values[\"10.7554/eLife.22661\"] = \"Reproducibility in Cancer Biology\"\n    title_prefix_values[\"10.7554/eLife.22662\"] = \"Reproducibility in Cancer Biology\"\n    title_prefix_values[\"10.7554/eLife.22735\"] = \"Motor Networks\"\n    title_prefix_values[\"10.7554/eLife.22850\"] = \"Heat Shock Response\"\n    title_prefix_values[\"10.7554/eLife.22915\"] = \"Reproducibility in Cancer Biology\"\n    title_prefix_values[\"10.7554/eLife.22926\"] = \"Skeletal Stem Cells\"\n    title_prefix_values[\"10.7554/eLife.23375\"] = \"Social Evolution\"\n    title_prefix_values[\"10.7554/eLife.23383\"] = \"Reproducibility in Cancer Biology\"\n    title_prefix_values[\"10.7554/eLife.23447\"] = \"Genetic Rearrangement\"\n    title_prefix_values[\"10.7554/eLife.23693\"] = \"Reproducibility in Cancer Biology\"\n    title_prefix_values[\"10.7554/eLife.23804\"] = \"Point of View\"\n    title_prefix_values[\"10.7554/eLife.24038\"] = \"Cell Division\"\n    title_prefix_values[\"10.7554/eLife.24052\"] = \"DNA Replication\"\n    title_prefix_values[\"10.7554/eLife.24106\"] = \"Germ Granules\"\n    title_prefix_values[\"10.7554/eLife.24238\"] = \"Tumor Angiogenesis\"\n    title_prefix_values[\"10.7554/eLife.24276\"] = \"Stem Cells\"\n    title_prefix_values[\"10.7554/eLife.24611\"] = \"Point of View\"\n    title_prefix_values[\"10.7554/eLife.24896\"] = \"Visual Behavior\"\n    title_prefix_values[\"10.7554/eLife.25000\"] = \"Chromatin Mapping\"\n    title_prefix_values[\"10.7554/eLife.25001\"] = \"Cell Cycle\"\n    title_prefix_values[\"10.7554/eLife.25159\"] = \"Ion Channels\"\n    title_prefix_values[\"10.7554/eLife.25358\"] = \"Cell Division\"\n    title_prefix_values[\"10.7554/eLife.25375\"] = \"Membrane Phase Separation\"\n    title_prefix_values[\"10.7554/eLife.25408\"] = \"Plain-language Summaries of Research\"\n    title_prefix_values[\"10.7554/eLife.25410\"] = \"Plain-language Summaries of Research\"\n    title_prefix_values[\"10.7554/eLife.25411\"] = \"Plain-language Summaries of Research\"\n    title_prefix_values[\"10.7554/eLife.25412\"] = \"Plain-language Summaries of Research\"\n    title_prefix_values[\"10.7554/eLife.25431\"] = \"Genetic Diversity\"\n    title_prefix_values[\"10.7554/eLife.25654\"] = \"Systems Biology\"\n    title_prefix_values[\"10.7554/eLife.25669\"] = \"Paternal Effects\"\n    title_prefix_values[\"10.7554/eLife.25700\"] = \"TOR Signaling\"\n    title_prefix_values[\"10.7554/eLife.25835\"] = \"Cutting Edge\"\n    title_prefix_values[\"10.7554/eLife.25858\"] = \"Developmental Biology\"\n    title_prefix_values[\"10.7554/eLife.25956\"] = \"Point of View\"\n    title_prefix_values[\"10.7554/eLife.25996\"] = \"Cancer Therapeutics\"\n    title_prefix_values[\"10.7554/eLife.26295\"] = \"Point of View\"\n    title_prefix_values[\"10.7554/eLife.26401\"] = \"Object Recognition\"\n    title_prefix_values[\"10.7554/eLife.26775\"] = \"Human Evolution\"\n    title_prefix_values[\"10.7554/eLife.26787\"] = \"Cutting Edge\"\n    title_prefix_values[\"10.7554/eLife.26942\"] = \"Alzheimer\u2019s Disease\"\n    title_prefix_values[\"10.7554/eLife.27085\"] = \"Translational Control\"\n    title_prefix_values[\"10.7554/eLife.27198\"] = \"Cell Signaling\"\n    title_prefix_values[\"10.7554/eLife.27438\"] = \"Point of View\"\n    title_prefix_values[\"10.7554/eLife.27467\"] = \"Evolutionary Developmental Biology\"\n    title_prefix_values[\"10.7554/eLife.27605\"] = \"Population Genetics\"\n    title_prefix_values[\"10.7554/eLife.27933\"] = \"Ion Channels\"\n    title_prefix_values[\"10.7554/eLife.27982\"] = \"Living Science\"\n    title_prefix_values[\"10.7554/eLife.28339\"] = \"Oncogene Regulation\"\n    title_prefix_values[\"10.7554/eLife.28514\"] = \"Maternal Behavior\"\n    title_prefix_values[\"10.7554/eLife.28699\"] = \"Point of View\"\n    title_prefix_values[\"10.7554/eLife.28757\"] = \"Mitochondrial Homeostasis\"\n    title_prefix_values[\"10.7554/eLife.29056\"] = \"Gene Variation\"\n    title_prefix_values[\"10.7554/eLife.29104\"] = \"Cardiac Hypertrophy\"\n    title_prefix_values[\"10.7554/eLife.29502\"] = \"Meiotic Recombination\"\n    title_prefix_values[\"10.7554/eLife.29586\"] = \"Virus Evolution\"\n    title_prefix_values[\"10.7554/eLife.29942\"] = \"Post-translational Modifications\"\n    title_prefix_values[\"10.7554/eLife.30076\"] = \"Scientific Publishing\"\n    title_prefix_values[\"10.7554/eLife.30183\"] = \"Point of View\"\n    title_prefix_values[\"10.7554/eLife.30194\"] = \"Organ Development\"\n    title_prefix_values[\"10.7554/eLife.30249\"] = \"Tissue Regeneration\"\n    title_prefix_values[\"10.7554/eLife.30280\"] = \"Adverse Drug Reactions\"\n    title_prefix_values[\"10.7554/eLife.30599\"] = \"Living Science\"\n    title_prefix_values[\"10.7554/eLife.30865\"] = \"Stone Tool Use\"\n    title_prefix_values[\"10.7554/eLife.31106\"] = \"Sensory Neurons\"\n    title_prefix_values[\"10.7554/eLife.31328\"] = \"Drought Stress\"\n    title_prefix_values[\"10.7554/eLife.31697\"] = \"Scientific Publishing\"\n    title_prefix_values[\"10.7554/eLife.31808\"] = \"Tissue Engineering\"\n    title_prefix_values[\"10.7554/eLife.31816\"] = \"Sound Processing\"\n    title_prefix_values[\"10.7554/eLife.32011\"] = \"Peer Review\"\n    title_prefix_values[\"10.7554/eLife.32012\"] = \"Peer Review\"\n    title_prefix_values[\"10.7554/eLife.32014\"] = \"Peer Review\"\n    title_prefix_values[\"10.7554/eLife.32015\"] = \"Peer Review\"\n    title_prefix_values[\"10.7554/eLife.32016\"] = \"Peer Review\"\n    title_prefix_values[\"10.7554/eLife.32715\"] = \"Point of View\"\n\n    # Edge case fix title prefix values\n    if doi in title_prefix_values:\n        # Do a quick sanity check, only replace if the lowercase comparison is equal\n        #  just in case the value has been changed to something else we will not replace it\n        if json_content.lower() == title_prefix_values[doi].lower():\n            json_content = title_prefix_values[doi]\n\n    return json_content"}
{"func_code_string": "def metadata_lint(old, new, locations):\n    \"\"\"Run the linter over the new metadata, comparing to the old.\"\"\"\n    # ensure we don't modify the metadata\n    old = old.copy()\n    new = new.copy()\n    # remove version info\n    old.pop('$version', None)\n    new.pop('$version', None)\n\n    for old_group_name in old:\n        if old_group_name not in new:\n            yield LintError('', 'api group removed', api_name=old_group_name)\n\n    for group_name, new_group in new.items():\n        old_group = old.get(group_name, {'apis': {}})\n\n        for name, api in new_group['apis'].items():\n            old_api = old_group['apis'].get(name, {})\n            api_locations = locations[name]\n            for message in lint_api(name, old_api, api, api_locations):\n                message.api_name = name\n                if message.location is None:\n                    message.location = api_locations['api']\n                yield message"}
{"func_code_string": "def lint_api(api_name, old, new, locations):\n    \"\"\"Lint an acceptable api metadata.\"\"\"\n    is_new_api = not old\n    api_location = locations['api']\n    changelog = new.get('changelog', {})\n    changelog_location = api_location\n\n    if locations['changelog']:\n        changelog_location = list(locations['changelog'].values())[0]\n\n    # apis must have documentation if they are new\n    if not new.get('doc'):\n        msg_type = LintError if is_new_api else LintWarning\n        yield msg_type(\n            'doc',\n            'missing docstring documentation',\n            api_name=api_name,\n            location=locations.get('view', api_location)\n        )\n\n    introduced_at = new.get('introduced_at')\n    if introduced_at is None:\n        yield LintError(\n            'introduced_at',\n            'missing introduced_at field',\n            location=api_location,\n        )\n\n    if not is_new_api:\n        # cannot change introduced_at if we already have it\n        old_introduced_at = old.get('introduced_at')\n        if old_introduced_at is not None:\n            if old_introduced_at != introduced_at:\n                yield LintError(\n                    'introduced_at',\n                    'introduced_at changed from {} to {}',\n                    old_introduced_at,\n                    introduced_at,\n                    api_name=api_name,\n                    location=api_location,\n                )\n\n    # cannot change url\n    if new['url'] != old.get('url', new['url']):\n        yield LintError(\n            'url',\n            'url changed from {} to {}',\n            old['url'],\n            new['url'],\n            api_name=api_name,\n            location=api_location,\n        )\n\n    # cannot add required fields\n    for removed in set(old.get('methods', [])) - set(new['methods']):\n        yield LintError(\n            'methods',\n            'HTTP method {} removed',\n            removed,\n            api_name=api_name,\n            location=api_location,\n        )\n\n    for schema in ['request_schema', 'response_schema']:\n        new_schema = new.get(schema)\n        if new_schema is None:\n            continue\n\n        schema_location = locations[schema]\n        old_schema = old.get(schema, {})\n\n        for message in walk_schema(\n                schema, old_schema, new_schema, root=True, new_api=is_new_api):\n            if isinstance(message, CheckChangelog):\n                if message.revision not in changelog:\n                    yield LintFixit(\n                        message.name,\n                        'No changelog entry for revision {}',\n                        message.revision,\n                        location=changelog_location,\n                    )\n            else:\n                # add in here, saves passing it down the recursive call\n                message.location = schema_location\n                yield message"}
{"func_code_string": "def bind(self, flask_app, service, group=None):\n        \"\"\"Bind the service API urls to a flask app.\"\"\"\n        if group not in self.services[service]:\n            raise RuntimeError(\n                'API group {} does not exist in service {}'.format(\n                    group, service)\n            )\n        for name, api in self.services[service][group].items():\n            # only bind APIs that have views associated with them\n            if api.view_fn is None:\n                continue\n            if name not in flask_app.view_functions:\n                flask_app.add_url_rule(\n                    api.url, name, view_func=api.view_fn, **api.options)"}
{"func_code_string": "def serialize(self):\n        \"\"\"Serialize into JSONable dict, and associated locations data.\"\"\"\n        api_metadata = OrderedDict()\n        # $ char makes this come first in sort ordering\n        api_metadata['$version'] = self.current_version\n        locations = {}\n\n        for svc_name, group in self.groups():\n            group_apis = OrderedDict()\n            group_metadata = OrderedDict()\n            group_metadata['apis'] = group_apis\n            group_metadata['title'] = group.title\n            api_metadata[group.name] = group_metadata\n\n            if group.docs is not None:\n                group_metadata['docs'] = group.docs\n\n            for name, api in group.items():\n                group_apis[name] = OrderedDict()\n                group_apis[name]['service'] = svc_name\n                group_apis[name]['api_group'] = group.name\n                group_apis[name]['api_name'] = api.name\n                group_apis[name]['introduced_at'] = api.introduced_at\n                group_apis[name]['methods'] = api.methods\n                group_apis[name]['request_schema'] = api.request_schema\n                group_apis[name]['response_schema'] = api.response_schema\n                group_apis[name]['doc'] = api.docs\n                group_apis[name]['changelog'] = api._changelog\n                if api.title:\n                    group_apis[name]['title'] = api.title\n                else:\n                    title = name.replace('-', ' ').replace('_', ' ').title()\n                    group_apis[name]['title'] = title\n\n                group_apis[name]['url'] = api.resolve_url()\n\n                if api.undocumented:\n                    group_apis[name]['undocumented'] = True\n                if api.deprecated_at is not None:\n                    group_apis[name]['deprecated_at'] = api.deprecated_at\n\n                locations[name] = {\n                    'api': api.location,\n                    'request_schema': api._request_schema_location,\n                    'response_schema': api._response_schema_location,\n                    'changelog': api._changelog_locations,\n                    'view': api.view_fn_location,\n                }\n\n        return api_metadata, locations"}
{"func_code_string": "def api(self,\n            url,\n            name,\n            introduced_at=None,\n            undocumented=False,\n            deprecated_at=None,\n            title=None,\n            **options):\n        \"\"\"Add an API to the service.\n\n        :param url: This is the url that the API should be registered at.\n        :param name: This is the name of the api, and will be registered with\n            flask apps under.\n\n        Other keyword arguments may be used, and they will be passed to the\n        flask application when initialised. Of particular interest is the\n        'methods' keyword argument, which can be used to specify the HTTP\n        method the URL will be added for.\n        \"\"\"\n        location = get_callsite_location()\n        api = AcceptableAPI(\n            self,\n            name,\n            url,\n            introduced_at,\n            options,\n            undocumented=undocumented,\n            deprecated_at=deprecated_at,\n            title=title,\n            location=location,\n        )\n        self.metadata.register_api(self.name, self.group, api)\n        return api"}
{"func_code_string": "def django_api(\n            self,\n            name,\n            introduced_at,\n            undocumented=False,\n            deprecated_at=None,\n            title=None,\n            **options):\n        \"\"\"Add a django API handler to the service.\n\n        :param name: This is the name of the django url to use.\n\n        The 'methods' paramater can be supplied as normal, you can also user\n        the @api.handler decorator to link this API to its handler.\n\n        \"\"\"\n        from acceptable.djangoutil import DjangoAPI\n        location = get_callsite_location()\n        api = DjangoAPI(\n            self,\n            name,\n            introduced_at,\n            options,\n            location=location,\n            undocumented=undocumented,\n            deprecated_at=deprecated_at,\n            title=title,\n        )\n        self.metadata.register_api(self.name, self.group, api)\n        return api"}
{"func_code_string": "def bind(self, flask_app):\n        \"\"\"Bind the service API urls to a flask app.\"\"\"\n        self.metadata.bind(flask_app, self.name, self.group)"}
{"func_code_string": "def changelog(self, api_version, doc):\n        \"\"\"Add a changelog entry for this api.\"\"\"\n        doc = textwrap.dedent(doc).strip()\n        self._changelog[api_version] = doc\n        self._changelog_locations[api_version] = get_callsite_location()"}
{"func_code_string": "def title_prefix(soup):\n    \"titlePrefix for article JSON is only articles with certain display_channel values\"\n    prefix = None\n    display_channel_match_list = ['feature article', 'insight', 'editorial']\n    for d_channel in display_channel(soup):\n        if d_channel.lower() in display_channel_match_list:\n            if raw_parser.sub_display_channel(soup):\n                prefix = node_text(first(raw_parser.sub_display_channel(soup)))\n    return prefix"}
{"func_code_string": "def title_prefix_json(soup):\n    \"titlePrefix with capitalisation changed\"\n    prefix = title_prefix(soup)\n    prefix_rewritten = elifetools.json_rewrite.rewrite_json(\"title_prefix_json\", soup, prefix)\n    return prefix_rewritten"}
{"func_code_string": "def research_organism(soup):\n    \"Find the research-organism from the set of kwd-group tags\"\n    if not raw_parser.research_organism_keywords(soup):\n        return []\n    return list(map(node_text, raw_parser.research_organism_keywords(soup)))"}
{"func_code_string": "def full_research_organism(soup):\n    \"research-organism list including inline tags, such as italic\"\n    if not raw_parser.research_organism_keywords(soup):\n        return []\n    return list(map(node_contents_str, raw_parser.research_organism_keywords(soup)))"}
{"func_code_string": "def keywords(soup):\n    \"\"\"\n    Find the keywords from the set of kwd-group tags\n    which are typically labelled as the author keywords\n    \"\"\"\n    if not raw_parser.author_keywords(soup):\n        return []\n    return list(map(node_text, raw_parser.author_keywords(soup)))"}
{"func_code_string": "def full_keywords(soup):\n    \"author keywords list including inline tags, such as italic\"\n    if not raw_parser.author_keywords(soup):\n        return []\n    return list(map(node_contents_str, raw_parser.author_keywords(soup)))"}
{"func_code_string": "def version_history(soup, html_flag=True):\n    \"extract the article version history details\"\n    convert = lambda xml_string: xml_to_html(html_flag, xml_string)\n    version_history = []\n    related_object_tags = raw_parser.related_object(raw_parser.article_meta(soup))\n    for tag in related_object_tags:\n        article_version = OrderedDict()\n        date_tag = first(raw_parser.date(tag))\n        if date_tag:\n            copy_attribute(date_tag.attrs, 'date-type', article_version, 'version')\n            (day, month, year) = ymd(date_tag)\n            article_version['day'] = day\n            article_version['month'] = month\n            article_version['year'] = year\n            article_version['date'] = date_struct_nn(year, month, day)\n        copy_attribute(tag.attrs, 'xlink:href', article_version, 'xlink_href')\n        set_if_value(article_version, \"comment\",\n                     convert(node_contents_str(first(raw_parser.comment(tag)))))\n        version_history.append(article_version)\n    return version_history"}
{"func_code_string": "def article_id_list(soup):\n    \"\"\"return a list of article-id data\"\"\"\n    id_list = []\n    for article_id_tag in raw_parser.article_id(soup):\n        id_details = OrderedDict()\n        set_if_value(id_details, \"type\", article_id_tag.get(\"pub-id-type\"))\n        set_if_value(id_details, \"value\", article_id_tag.text)\n        set_if_value(id_details, \"assigning-authority\", article_id_tag.get(\"assigning-authority\"))\n        id_list.append(id_details)\n    return id_list"}
{"func_code_string": "def copyright_holder_json(soup):\n    \"for json output add a full stop if ends in et al\"\n    holder = None\n    permissions_tag = raw_parser.article_permissions(soup)\n    if permissions_tag:\n        holder = node_text(raw_parser.copyright_holder(permissions_tag))\n    if holder is not None and holder.endswith('et al'):\n        holder = holder + '.'\n    return holder"}
{"func_code_string": "def subject_area(soup):\n    \"\"\"\n    Find the subject areas from article-categories subject tags\n    \"\"\"\n    subject_area = []\n\n    tags = raw_parser.subject_area(soup)\n    for tag in tags:\n        subject_area.append(node_text(tag))\n\n    return subject_area"}
{"func_code_string": "def display_channel(soup):\n    \"\"\"\n    Find the subject areas of type display-channel\n    \"\"\"\n    display_channel = []\n\n    tags = raw_parser.display_channel(soup)\n    for tag in tags:\n        display_channel.append(node_text(tag))\n\n    return display_channel"}
{"func_code_string": "def category(soup):\n    \"\"\"\n    Find the category from subject areas\n    \"\"\"\n    category = []\n\n    tags = raw_parser.category(soup)\n    for tag in tags:\n        category.append(node_text(tag))\n\n    return category"}
{"func_code_string": "def ymd(soup):\n    \"\"\"\n    Get the year, month and day from child tags\n    \"\"\"\n    day = node_text(raw_parser.day(soup))\n    month = node_text(raw_parser.month(soup))\n    year = node_text(raw_parser.year(soup))\n    return (day, month, year)"}
{"func_code_string": "def pub_date(soup):\n    \"\"\"\n    Return the publishing date in struct format\n    pub_date_date, pub_date_day, pub_date_month, pub_date_year, pub_date_timestamp\n    Default date_type is pub\n    \"\"\"\n    pub_date = first(raw_parser.pub_date(soup, date_type=\"pub\"))\n    if pub_date is None:\n        pub_date = first(raw_parser.pub_date(soup, date_type=\"publication\"))\n    if pub_date is None:\n        return None\n    (day, month, year) = ymd(pub_date)\n    return date_struct(year, month, day)"}
{"func_code_string": "def pub_dates(soup):\n    \"\"\"\n    return a list of all the pub dates\n    \"\"\"\n    pub_dates = []\n    tags = raw_parser.pub_date(soup)\n    for tag in tags:\n        pub_date = OrderedDict()\n        copy_attribute(tag.attrs, 'publication-format', pub_date)\n        copy_attribute(tag.attrs, 'date-type', pub_date)\n        copy_attribute(tag.attrs, 'pub-type', pub_date)\n        for tag_attr in [\"date-type\", \"pub-type\"]:\n            if tag_attr in tag.attrs:\n                (day, month, year) = ymd(tag)\n                pub_date['day'] = day\n                pub_date['month'] = month\n                pub_date['year'] = year\n                pub_date['date'] = date_struct_nn(year, month, day)\n        pub_dates.append(pub_date)\n    return pub_dates"}
{"func_code_string": "def history_date(soup, date_type = None):\n    \"\"\"\n    Find a date in the history tag for the specific date_type\n    typical date_type values: received, accepted\n    \"\"\"\n    if(date_type == None):\n        return None\n\n    history_date = raw_parser.history_date(soup, date_type)\n    if history_date is None:\n        return None\n    (day, month, year) = ymd(history_date)\n    return date_struct(year, month, day)"}
{"func_code_string": "def collection_year(soup):\n    \"\"\"\n    Pub date of type collection will hold a year element for VOR articles\n    \"\"\"\n    pub_date = first(raw_parser.pub_date(soup, pub_type=\"collection\"))\n    if not pub_date:\n        pub_date = first(raw_parser.pub_date(soup, date_type=\"collection\"))\n    if not pub_date:\n        return None\n\n    year = None\n    year_tag = raw_parser.year(pub_date)\n    if year_tag:\n        year = int(node_text(year_tag))\n\n    return year"}
{"func_code_string": "def abstracts(soup):\n    \"\"\"\n    Find the article abstract and format it\n    \"\"\"\n\n    abstracts = []\n\n    abstract_tags = raw_parser.abstract(soup)\n\n    for tag in abstract_tags:\n        abstract = {}\n\n        abstract[\"abstract_type\"] = tag.get(\"abstract-type\")\n        title_tag = raw_parser.title(tag)\n        if title_tag:\n            abstract[\"title\"] = node_text(title_tag)\n\n        abstract[\"content\"] = None\n        if raw_parser.paragraph(tag):\n            abstract[\"content\"] = \"\"\n            abstract[\"full_content\"] = \"\"\n\n            good_paragraphs = remove_doi_paragraph(raw_parser.paragraph(tag))\n\n            # Plain text content\n            glue = \"\"\n            for p_tag in good_paragraphs:\n                abstract[\"content\"] += glue + node_text(p_tag)\n                glue = \" \"\n\n            # Content including markup tags\n            # When more than one paragraph, wrap each in a <p> tag\n            for p_tag in good_paragraphs:\n                abstract[\"full_content\"] += '<p>' + node_contents_str(p_tag) + '</p>'\n\n        abstracts.append(abstract)\n\n    return abstracts"}
{"func_code_string": "def component_doi(soup):\n    \"\"\"\n    Look for all object-id of pub-type-id = doi, these are the component DOI tags\n    \"\"\"\n    component_doi = []\n\n    object_id_tags = raw_parser.object_id(soup, pub_id_type = \"doi\")\n\n    # Get components too for later\n    component_list = components(soup)\n\n    position = 1\n\n    for tag in object_id_tags:\n        component_object = {}\n        component_object[\"doi\"] = doi_uri_to_doi(tag.text)\n        component_object[\"position\"] = position\n\n        # Try to find the type of component\n        for component in component_list:\n            if \"doi\" in component and component[\"doi\"] == component_object[\"doi\"]:\n                component_object[\"type\"] = component[\"type\"]\n\n        component_doi.append(component_object)\n\n        position = position + 1\n\n    return component_doi"}
{"func_code_string": "def tag_details(tag, nodenames):\n    \"\"\"\n    Used in media and graphics to extract data from their parent tags\n    \"\"\"\n    details = {}\n\n    details['type'] = tag.name\n    details['ordinal'] = tag_ordinal(tag)\n\n    # Ordinal value\n    if tag_details_sibling_ordinal(tag):\n        details['sibling_ordinal'] = tag_details_sibling_ordinal(tag)\n\n    # Asset name\n    if tag_details_asset(tag):\n        details['asset'] = tag_details_asset(tag)\n\n    object_id_tag = first(raw_parser.object_id(tag, pub_id_type= \"doi\"))\n    if object_id_tag:\n        details['component_doi'] = extract_component_doi(tag, nodenames)\n\n    return details"}
{"func_code_string": "def media(soup):\n    \"\"\"\n    All media tags and some associated data about the related component doi\n    and the parent of that doi (not always present)\n    \"\"\"\n    media = []\n\n    media_tags = raw_parser.media(soup)\n\n    position = 1\n\n    for tag in media_tags:\n        media_item = {}\n\n        copy_attribute(tag.attrs, 'mime-subtype', media_item)\n        copy_attribute(tag.attrs, 'mimetype', media_item)\n        copy_attribute(tag.attrs, 'xlink:href', media_item, 'xlink_href')\n        copy_attribute(tag.attrs, 'content-type', media_item)\n\n        nodenames = [\"sub-article\", \"media\", \"fig-group\", \"fig\", \"supplementary-material\"]\n\n        details = tag_details(tag, nodenames)\n        copy_attribute(details, 'component_doi', media_item)\n        copy_attribute(details, 'type', media_item)\n        copy_attribute(details, 'sibling_ordinal', media_item)\n\n        # Try to get the component DOI of the parent tag\n        parent_tag = first_parent(tag, nodenames)\n        if parent_tag:\n            acting_parent_tag = component_acting_parent_tag(parent_tag, tag)\n            if acting_parent_tag:\n                details = tag_details(acting_parent_tag, nodenames)\n                copy_attribute(details, 'type', media_item, 'parent_type')\n                copy_attribute(details, 'ordinal', media_item, 'parent_ordinal')\n                copy_attribute(details, 'asset', media_item, 'parent_asset')\n                copy_attribute(details, 'sibling_ordinal', media_item, 'parent_sibling_ordinal')\n                copy_attribute(details, 'component_doi', media_item, 'parent_component_doi')\n\n            # Try to get the parent parent\n            p_parent_tag = first_parent(parent_tag, nodenames)\n            if p_parent_tag:\n                acting_p_parent_tag = component_acting_parent_tag(p_parent_tag, parent_tag)\n                if acting_p_parent_tag:\n                    details = tag_details(acting_p_parent_tag, nodenames)\n                    copy_attribute(details, 'type', media_item, 'p_parent_type')\n                    copy_attribute(details, 'ordinal', media_item, 'p_parent_ordinal')\n                    copy_attribute(details, 'asset', media_item, 'p_parent_asset')\n                    copy_attribute(details, 'sibling_ordinal', media_item, 'p_parent_sibling_ordinal')\n                    copy_attribute(details, 'component_doi', media_item, 'p_parent_component_doi')\n\n                # Try to get the parent parent parent\n                p_p_parent_tag = first_parent(p_parent_tag, nodenames)\n                if p_p_parent_tag:\n                    acting_p_p_parent_tag = component_acting_parent_tag(p_p_parent_tag, p_parent_tag)\n                    if acting_p_p_parent_tag:\n                        details = tag_details(acting_p_p_parent_tag, nodenames)\n                        copy_attribute(details, 'type', media_item, 'p_p_parent_type')\n                        copy_attribute(details, 'ordinal', media_item, 'p_p_parent_ordinal')\n                        copy_attribute(details, 'asset', media_item, 'p_p_parent_asset')\n                        copy_attribute(details, 'sibling_ordinal', media_item, 'p_p_parent_sibling_ordinal')\n                        copy_attribute(details, 'component_doi', media_item, 'p_p_parent_component_doi')\n\n        # Increment the position\n        media_item['position'] = position\n        # Ordinal should be the same as position in this case but set it anyway\n        media_item['ordinal'] = tag_ordinal(tag)\n\n        media.append(media_item)\n\n        position += 1\n\n    return media"}
{"func_code_string": "def graphics(soup):\n    \"\"\"\n    All graphic tags and some associated data about the related component doi\n    and the parent of that doi (not always present), and whether it is\n    part of a figure supplement\n    \"\"\"\n    graphics = []\n\n    graphic_tags = raw_parser.graphic(soup)\n\n    position = 1\n\n    for tag in graphic_tags:\n        graphic_item = {}\n\n        copy_attribute(tag.attrs, 'xlink:href', graphic_item, 'xlink_href')\n\n        # Get the tag type\n        nodenames = [\"sub-article\", \"fig-group\", \"fig\", \"app\"]\n        details = tag_details(tag, nodenames)\n        copy_attribute(details, 'type', graphic_item)\n\n        parent_tag = first_parent(tag, nodenames)\n        if parent_tag:\n            details = tag_details(parent_tag, nodenames)\n            copy_attribute(details, 'type', graphic_item, 'parent_type')\n            copy_attribute(details, 'ordinal', graphic_item, 'parent_ordinal')\n            copy_attribute(details, 'asset', graphic_item, 'parent_asset')\n            copy_attribute(details, 'sibling_ordinal', graphic_item, 'parent_sibling_ordinal')\n            copy_attribute(details, 'component_doi', graphic_item, 'parent_component_doi')\n\n            # Try to get the parent parent - special for looking at fig tags\n            #  use component_acting_parent_tag\n            p_parent_tag = first_parent(parent_tag, nodenames)\n            if p_parent_tag:\n                acting_p_parent_tag = component_acting_parent_tag(p_parent_tag, parent_tag)\n                if acting_p_parent_tag:\n                    details = tag_details(acting_p_parent_tag, nodenames)\n                    copy_attribute(details, 'type', graphic_item, 'p_parent_type')\n                    copy_attribute(details, 'ordinal', graphic_item, 'p_parent_ordinal')\n                    copy_attribute(details, 'asset', graphic_item, 'p_parent_asset')\n                    copy_attribute(details, 'sibling_ordinal', graphic_item, 'p_parent_sibling_ordinal')\n                    copy_attribute(details, 'component_doi', graphic_item, 'p_parent_component_doi')\n\n        # Increment the position\n        graphic_item['position'] = position\n        # Ordinal should be the same as position in this case but set it anyway\n        graphic_item['ordinal'] = tag_ordinal(tag)\n\n        graphics.append(graphic_item)\n\n        position += 1\n\n    return graphics"}
{"func_code_string": "def inline_graphics(soup):\n    \"\"\"\n    inline-graphic tags\n    \"\"\"\n    inline_graphics = []\n\n    inline_graphic_tags = raw_parser.inline_graphic(soup)\n\n    position = 1\n\n    for tag in inline_graphic_tags:\n        item = {}\n\n        copy_attribute(tag.attrs, 'xlink:href', item, 'xlink_href')\n\n        # Get the tag type\n        nodenames = [\"sub-article\"]\n        details = tag_details(tag, nodenames)\n        copy_attribute(details, 'type', item)\n\n        # Increment the position\n        item['position'] = position\n        # Ordinal should be the same as position in this case but set it anyway\n        item['ordinal'] = tag_ordinal(tag)\n\n        inline_graphics.append(item)\n\n    return inline_graphics"}
{"func_code_string": "def self_uri(soup):\n    \"\"\"\n    self-uri tags\n    \"\"\"\n\n    self_uri = []\n    self_uri_tags = raw_parser.self_uri(soup)\n    position = 1\n    for tag in self_uri_tags:\n        item = {}\n\n        copy_attribute(tag.attrs, 'xlink:href', item, 'xlink_href')\n        copy_attribute(tag.attrs, 'content-type', item)\n\n        # Get the tag type\n        nodenames = [\"sub-article\"]\n        details = tag_details(tag, nodenames)\n        copy_attribute(details, 'type', item)\n\n        # Increment the position\n        item['position'] = position\n        # Ordinal should be the same as position in this case but set it anyway\n        item['ordinal'] = tag_ordinal(tag)\n\n        self_uri.append(item)\n\n    return self_uri"}
{"func_code_string": "def supplementary_material(soup):\n    \"\"\"\n    supplementary-material tags\n    \"\"\"\n    supplementary_material = []\n\n    supplementary_material_tags = raw_parser.supplementary_material(soup)\n\n    position = 1\n\n    for tag in supplementary_material_tags:\n        item = {}\n\n        copy_attribute(tag.attrs, 'id', item)\n\n        # Get the tag type\n        nodenames = [\"supplementary-material\"]\n        details = tag_details(tag, nodenames)\n        copy_attribute(details, 'type', item)\n        copy_attribute(details, 'asset', item)\n        copy_attribute(details, 'component_doi', item)\n        copy_attribute(details, 'sibling_ordinal', item)\n\n        if raw_parser.label(tag):\n            item['label'] = node_text(raw_parser.label(tag))\n            item['full_label'] = node_contents_str(raw_parser.label(tag))\n\n        # Increment the position\n        item['position'] = position\n        # Ordinal should be the same as position in this case but set it anyway\n        item['ordinal'] = tag_ordinal(tag)\n\n        supplementary_material.append(item)\n\n    return supplementary_material"}
{"func_code_string": "def contrib_email(contrib_tag):\n    \"\"\"\n    Given a contrib tag, look for an email tag, and\n    only return the value if it is not inside an aff tag\n    \"\"\"\n    email = []\n    for email_tag in extract_nodes(contrib_tag, \"email\"):\n        if email_tag.parent.name != \"aff\":\n            email.append(email_tag.text)\n    return email if len(email) > 0 else None"}
{"func_code_string": "def contrib_phone(contrib_tag):\n    \"\"\"\n    Given a contrib tag, look for an phone tag\n    \"\"\"\n    phone = None\n    if raw_parser.phone(contrib_tag):\n        phone = first(raw_parser.phone(contrib_tag)).text\n    return phone"}
{"func_code_string": "def contrib_inline_aff(contrib_tag):\n    \"\"\"\n    Given a contrib tag, look for an aff tag directly inside it\n    \"\"\"\n    aff_tags = []\n    for child_tag in contrib_tag:\n        if child_tag and child_tag.name and child_tag.name == \"aff\":\n            aff_tags.append(child_tag)\n    return aff_tags"}
{"func_code_string": "def contrib_xref(contrib_tag, ref_type):\n    \"\"\"\n    Given a contrib tag, look for an xref tag of type ref_type directly inside the contrib tag\n    \"\"\"\n    aff_tags = []\n    for child_tag in contrib_tag:\n        if (child_tag and child_tag.name and child_tag.name == \"xref\"\n            and child_tag.get('ref-type') and child_tag.get('ref-type') == ref_type):\n            aff_tags.append(child_tag)\n    return aff_tags"}
{"func_code_string": "def all_contributors(soup, detail=\"brief\"):\n    \"find all contributors not contrained to only the ones in article meta\"\n    contrib_tags = raw_parser.contributors(soup)\n    contributors = format_authors(soup, contrib_tags, detail)\n    return contributors"}
{"func_code_string": "def authors_non_byline(soup, detail=\"full\"):\n    \"\"\"Non-byline authors for group author members\"\"\"\n    # Get a filtered list of contributors, in order to get their group-author-id\n    contrib_type = \"author non-byline\"\n    contributors_ = contributors(soup, detail)\n    non_byline_authors = [author for author in contributors_ if author.get('type', None) == contrib_type]\n\n    # Then renumber their position attribute\n    position = 1\n    for author in non_byline_authors:\n        author[\"position\"] = position\n        position = position + 1\n    return non_byline_authors"}
{"func_code_string": "def refs(soup):\n    \"\"\"Find and return all the references\"\"\"\n    tags = raw_parser.ref_list(soup)\n    refs = []\n    position = 1\n\n    article_doi = doi(soup)\n\n    for tag in tags:\n        ref = {}\n\n        ref['ref'] = ref_text(tag)\n\n        # ref_id\n        copy_attribute(tag.attrs, \"id\", ref)\n\n        # article_title\n        if raw_parser.article_title(tag):\n            ref['article_title'] = node_text(raw_parser.article_title(tag))\n            ref['full_article_title'] = node_contents_str(raw_parser.article_title(tag))\n\n        if raw_parser.pub_id(tag, \"pmid\"):\n            ref['pmid'] = node_contents_str(first(raw_parser.pub_id(tag, \"pmid\")))\n\n        if raw_parser.pub_id(tag, \"isbn\"):\n            ref['isbn'] = node_contents_str(first(raw_parser.pub_id(tag, \"isbn\")))\n\n        if raw_parser.pub_id(tag, \"doi\"):\n            ref['reference_id'] = node_contents_str(first(raw_parser.pub_id(tag, \"doi\")))\n            ref['doi'] = doi_uri_to_doi(node_contents_str(first(raw_parser.pub_id(tag, \"doi\"))))\n\n        uri_tag = None\n        if raw_parser.ext_link(tag, \"uri\"):\n            uri_tag = first(raw_parser.ext_link(tag, \"uri\"))\n        elif raw_parser.uri(tag):\n            uri_tag = first(raw_parser.uri(tag))\n        if uri_tag:\n            set_if_value(ref, \"uri\", uri_tag.get('xlink:href'))\n            set_if_value(ref, \"uri_text\", node_contents_str(uri_tag))\n        # look for a pub-id tag if no uri yet\n        if not ref.get('uri') and raw_parser.pub_id(tag, \"archive\"):\n            pub_id_tag = first(raw_parser.pub_id(tag, pub_id_type=\"archive\"))\n            set_if_value(ref, \"uri\", pub_id_tag.get('xlink:href'))\n\n        # accession, could be in either of two tags\n        set_if_value(ref, \"accession\", node_contents_str(first(raw_parser.object_id(tag, \"art-access-id\"))))\n        if not ref.get('accession'):\n            set_if_value(ref, \"accession\", node_contents_str(first(raw_parser.pub_id(tag, pub_id_type=\"accession\"))))\n        if not ref.get('accession'):\n            set_if_value(ref, \"accession\", node_contents_str(first(raw_parser.pub_id(tag, pub_id_type=\"archive\"))))\n\n        if(raw_parser.year(tag)):\n            set_if_value(ref, \"year\", node_text(raw_parser.year(tag)))\n            set_if_value(ref, \"year-iso-8601-date\", raw_parser.year(tag).get('iso-8601-date'))\n\n        if(raw_parser.date_in_citation(tag)):\n            set_if_value(ref, \"date-in-citation\", node_text(first(raw_parser.date_in_citation(tag))))\n            set_if_value(ref, \"iso-8601-date\", first(raw_parser.date_in_citation(tag)).get('iso-8601-date'))\n\n        if(raw_parser.patent(tag)):\n            set_if_value(ref, \"patent\", node_text(first(raw_parser.patent(tag))))\n            set_if_value(ref, \"country\", first(raw_parser.patent(tag)).get('country'))\n\n        set_if_value(ref, \"source\", node_text(first(raw_parser.source(tag))))\n        set_if_value(ref, \"elocation-id\", node_text(first(raw_parser.elocation_id(tag))))\n        if raw_parser.element_citation(tag):\n            copy_attribute(first(raw_parser.element_citation(tag)).attrs, \"publication-type\", ref)\n        if \"publication-type\" not in ref and raw_parser.mixed_citations(tag):\n            copy_attribute(first(raw_parser.mixed_citations(tag)).attrs, \"publication-type\", ref)\n\n        # authors\n        person_group = raw_parser.person_group(tag)\n        authors = []\n\n        for group in person_group:\n\n            author_type = None\n            if \"person-group-type\" in group.attrs:\n                author_type = group[\"person-group-type\"]\n\n            # Read name or collab tag in the order they are listed\n            for name_or_collab_tag in extract_nodes(group, [\"name\", \"string-name\", \"collab\"]):\n                author = {}\n\n                # Shared tag attribute\n                set_if_value(author, \"group-type\", author_type)\n\n                # name tag attributes\n                if name_or_collab_tag.name in [\"name\", \"string-name\"]:\n                    set_if_value(author, \"surname\", node_text(first(raw_parser.surname(name_or_collab_tag))))\n                    set_if_value(author, \"given-names\", node_text(first(raw_parser.given_names(name_or_collab_tag))))\n                    set_if_value(author, \"suffix\", node_text(first(raw_parser.suffix(name_or_collab_tag))))\n\n                # collab tag attribute\n                if name_or_collab_tag.name == \"collab\":\n                    set_if_value(author, \"collab\", node_contents_str(name_or_collab_tag))\n\n                if len(author) > 0:\n                    authors.append(author)\n\n            # etal for the person group\n            if first(raw_parser.etal(group)):\n                author = {}\n                author['etal'] = True\n                set_if_value(author, \"group-type\", author_type)\n                authors.append(author)\n\n        # Check for collab tag not wrapped in a person-group for backwards compatibility\n        if len(person_group) == 0:\n            collab_tags = raw_parser.collab(tag)\n            for collab_tag in collab_tags:\n                author = {}\n                set_if_value(author, \"group-type\", \"author\")\n                set_if_value(author, \"collab\", node_contents_str(collab_tag))\n\n                if len(author) > 0:\n                    authors.append(author)\n\n        if len(authors) > 0:\n            ref['authors'] = authors\n\n        set_if_value(ref, \"volume\", node_text(first(raw_parser.volume(tag))))\n        set_if_value(ref, \"issue\", node_text(first(raw_parser.issue(tag))))\n        set_if_value(ref, \"fpage\", node_text(first(raw_parser.fpage(tag))))\n        set_if_value(ref, \"lpage\", node_text(first(raw_parser.lpage(tag))))\n        set_if_value(ref, \"collab\", node_text(first(raw_parser.collab(tag))))\n        set_if_value(ref, \"publisher_loc\", node_text(first(raw_parser.publisher_loc(tag))))\n        set_if_value(ref, \"publisher_name\", node_text(first(raw_parser.publisher_name(tag))))\n        set_if_value(ref, \"edition\", node_contents_str(first(raw_parser.edition(tag))))\n        set_if_value(ref, \"version\", node_contents_str(first(raw_parser.version(tag))))\n        set_if_value(ref, \"chapter-title\", node_contents_str(first(raw_parser.chapter_title(tag))))\n        set_if_value(ref, \"comment\", node_text(first(raw_parser.comment(tag))))\n        set_if_value(ref, \"data-title\", node_contents_str(first(raw_parser.data_title(tag))))\n        set_if_value(ref, \"conf-name\", node_text(first(raw_parser.conf_name(tag))))\n\n        # If not empty, add position value, append, then increment the position counter\n        if(len(ref) > 0):\n            ref['article_doi'] = article_doi\n\n            ref['position'] = position\n\n            refs.append(ref)\n            position += 1\n\n    return refs"}
{"func_code_string": "def extract_component_doi(tag, nodenames):\n    \"\"\"\n    Used to get component DOI from a tag and confirm it is actually for that tag\n    and it is not for one of its children in the list of nodenames\n    \"\"\"\n    component_doi = None\n\n    if(tag.name == \"sub-article\"):\n        component_doi = doi_uri_to_doi(node_text(first(raw_parser.article_id(tag, pub_id_type= \"doi\"))))\n    else:\n        object_id_tag = first(raw_parser.object_id(tag, pub_id_type= \"doi\"))\n        # Tweak: if it is media and has no object_id_tag then it is not a \"component\"\n        if tag.name == \"media\" and not object_id_tag:\n            component_doi = None\n        else:\n            # Check the object id is for this tag and not one of its children\n            #   This happens for example when boxed text has a child figure,\n            #   the boxed text does not have a DOI, the figure does have one\n            if object_id_tag and first_parent(object_id_tag, nodenames).name == tag.name:\n                component_doi = doi_uri_to_doi(node_text(object_id_tag))\n\n    return component_doi"}
{"func_code_string": "def components(soup):\n    \"\"\"\n    Find the components, i.e. those parts that would be assigned\n    a unique component DOI, such as figures, tables, etc.\n    - position is in what order the tag appears in the entire set of nodes\n    - ordinal is in what order it is for all the tags of its own type\n    \"\"\"\n    components = []\n\n    nodenames = [\"abstract\", \"fig\", \"table-wrap\", \"media\",\n                 \"chem-struct-wrap\", \"sub-article\", \"supplementary-material\",\n                 \"boxed-text\", \"app\"]\n\n    # Count node order overall\n    position = 1\n\n    position_by_type = {}\n    for nodename in nodenames:\n        position_by_type[nodename] = 1\n\n    article_doi = doi(soup)\n\n    # Find all tags for all component_types, allows the order\n    #  in which they are found to be preserved\n    component_tags = extract_nodes(soup, nodenames)\n\n    for tag in component_tags:\n\n        component = OrderedDict()\n\n        # Component type is the tag's name\n        ctype = tag.name\n\n        # First find the doi if present\n        component_doi = extract_component_doi(tag, nodenames)\n        if component_doi is None:\n            continue\n        else:\n            component['doi'] = doi_uri_to_doi(component_doi)\n            component['doi_url'] = doi_to_doi_uri(component['doi'])\n\n        copy_attribute(tag.attrs, 'id', component)\n\n        if(ctype == \"sub-article\"):\n            title_tag = raw_parser.article_title(tag)\n        elif(ctype == \"boxed-text\"):\n            title_tag = title_tag_inspected(tag, tag.name, direct_sibling_only=True)\n            if not title_tag:\n                title_tag = title_tag_inspected(tag, \"caption\", \"boxed-text\")\n            # New kitchen sink has boxed-text inside app tags, tag the sec tag title if so\n            #  but do not take it if there is a caption\n            if (not title_tag and tag.parent and tag.parent.name in [\"sec\", \"app\"]\n                and not caption_tag_inspected(tag, tag.name)):\n                title_tag = title_tag_inspected(tag.parent, tag.parent.name, direct_sibling_only=True)\n        else:\n            title_tag = raw_parser.title(tag)\n\n        if title_tag:\n            component['title'] = node_text(title_tag)\n            component['full_title'] = node_contents_str(title_tag)\n\n        if ctype == \"boxed-text\":\n            label_tag = label_tag_inspected(tag, \"boxed-text\")\n        else:\n            label_tag = raw_parser.label(tag)\n\n        if label_tag:\n            component['label'] = node_text(label_tag)\n            component['full_label'] = node_contents_str(label_tag)\n\n        if raw_parser.caption(tag):\n            first_paragraph = first(paragraphs(raw_parser.caption(tag)))\n            # fix a problem with the new kitchen sink of caption within caption tag\n            if first_paragraph:\n                nested_caption = raw_parser.caption(first_paragraph)\n                if nested_caption:\n                    nested_paragraphs = paragraphs(nested_caption) \n                    first_paragraph = first(nested_paragraphs) or first_paragraph\n            if first_paragraph and not starts_with_doi(first_paragraph):\n                # Remove the supplementary tag from the paragraph if present\n                if raw_parser.supplementary_material(first_paragraph):\n                    first_paragraph = remove_tag_from_tag(first_paragraph, 'supplementary-material')\n                if node_text(first_paragraph).strip():\n                    component['caption'] = node_text(first_paragraph)\n                    component['full_caption'] = node_contents_str(first_paragraph)\n\n        if raw_parser.permissions(tag):\n\n            component['permissions'] = []\n            for permissions_tag in raw_parser.permissions(tag):\n                permissions_item = {}\n                if raw_parser.copyright_statement(permissions_tag):\n                    permissions_item['copyright_statement'] = \\\n                        node_text(raw_parser.copyright_statement(permissions_tag))\n\n                if raw_parser.copyright_year(permissions_tag):\n                    permissions_item['copyright_year'] = \\\n                        node_text(raw_parser.copyright_year(permissions_tag))\n\n                if raw_parser.copyright_holder(permissions_tag):\n                    permissions_item['copyright_holder'] = \\\n                        node_text(raw_parser.copyright_holder(permissions_tag))\n\n                if raw_parser.licence_p(permissions_tag):\n                    permissions_item['license'] = \\\n                        node_text(first(raw_parser.licence_p(permissions_tag)))\n                    permissions_item['full_license'] = \\\n                        node_contents_str(first(raw_parser.licence_p(permissions_tag)))\n\n                component['permissions'].append(permissions_item)\n\n        if raw_parser.contributors(tag):\n            component['contributors'] = []\n            for contributor_tag in raw_parser.contributors(tag):\n                component['contributors'].append(format_contributor(contributor_tag, soup))\n\n        # There are only some parent tags we care about for components\n        #  and only check two levels of parentage\n        parent_nodenames = [\"sub-article\", \"fig-group\", \"fig\", \"boxed-text\", \"table-wrap\", \"app\", \"media\"]\n        parent_tag = first_parent(tag, parent_nodenames)\n\n        if parent_tag:\n\n            # For fig-group we actually want the first fig of the fig-group as the parent\n            acting_parent_tag = component_acting_parent_tag(parent_tag, tag)\n\n            # Only counts if the acting parent tag has a DOI\n            if (acting_parent_tag and \\\n               extract_component_doi(acting_parent_tag, parent_nodenames) is not None):\n\n                component['parent_type'] = acting_parent_tag.name\n                component['parent_ordinal'] = tag_ordinal(acting_parent_tag)\n                component['parent_sibling_ordinal'] = tag_details_sibling_ordinal(acting_parent_tag)\n                component['parent_asset'] = tag_details_asset(acting_parent_tag)\n\n            # Look for parent parent, if available\n            parent_parent_tag = first_parent(parent_tag, parent_nodenames)\n\n            if parent_parent_tag:\n\n                acting_parent_tag = component_acting_parent_tag(parent_parent_tag, parent_tag)\n\n                if (acting_parent_tag and \\\n                   extract_component_doi(acting_parent_tag, parent_nodenames) is not None):\n                    component['parent_parent_type'] = acting_parent_tag.name\n                    component['parent_parent_ordinal'] = tag_ordinal(acting_parent_tag)\n                    component['parent_parent_sibling_ordinal'] = tag_details_sibling_ordinal(acting_parent_tag)\n                    component['parent_parent_asset'] = tag_details_asset(acting_parent_tag)\n\n        content = \"\"\n        for p_tag in extract_nodes(tag, \"p\"):\n            if content != \"\":\n                # Add a space before each new paragraph for now\n                content = content + \" \"\n            content = content + node_text(p_tag)\n\n        if(content != \"\"):\n            component['content'] = content\n\n        # mime type\n        media_tag = None\n        if(ctype == \"media\"):\n            media_tag = tag\n        elif(ctype == \"supplementary-material\"):\n            media_tag = first(raw_parser.media(tag))\n        if media_tag:\n            component['mimetype'] = media_tag.get(\"mimetype\")\n            component['mime-subtype'] = media_tag.get(\"mime-subtype\")\n\n        if(len(component) > 0):\n            component['article_doi'] = article_doi\n            component['type'] = ctype\n            component['position'] = position\n\n            # Ordinal is based on all tags of the same type even if they have no DOI\n            component['ordinal'] = tag_ordinal(tag)\n            component['sibling_ordinal'] = tag_details_sibling_ordinal(tag)\n            component['asset'] = tag_details_asset(tag)\n            #component['ordinal'] = position_by_type[ctype]\n\n            components.append(component)\n\n            position += 1\n            position_by_type[ctype] += 1\n\n\n    return components"}
{"func_code_string": "def correspondence(soup):\n    \"\"\"\n    Find the corresp tags included in author-notes\n    for primary correspondence\n    \"\"\"\n    correspondence = []\n\n    author_notes_nodes = raw_parser.author_notes(soup)\n\n    if author_notes_nodes:\n        corresp_nodes = raw_parser.corresp(author_notes_nodes)\n        for tag in corresp_nodes:\n            correspondence.append(tag.text)\n\n    return correspondence"}
{"func_code_string": "def author_notes(soup):\n    \"\"\"\n    Find the fn tags included in author-notes\n    \"\"\"\n    author_notes = []\n\n    author_notes_section = raw_parser.author_notes(soup)\n    if author_notes_section:\n        fn_nodes = raw_parser.fn(author_notes_section)\n        for tag in fn_nodes:\n            if 'fn-type' in tag.attrs:\n                if(tag['fn-type'] != 'present-address'):\n                    author_notes.append(node_text(tag))\n\n    return author_notes"}
{"func_code_string": "def full_author_notes(soup, fntype_filter=None):\n    \"\"\"\n    Find the fn tags included in author-notes\n    \"\"\"\n    notes = []\n\n    author_notes_section = raw_parser.author_notes(soup)\n    if author_notes_section:\n        fn_nodes = raw_parser.fn(author_notes_section)\n        notes = footnotes(fn_nodes, fntype_filter)\n\n    return notes"}
{"func_code_string": "def competing_interests(soup, fntype_filter):\n    \"\"\"\n    Find the fn tags included in the competing interest\n    \"\"\"\n\n    competing_interests_section = extract_nodes(soup, \"fn-group\", attr=\"content-type\", value=\"competing-interest\")\n    if not competing_interests_section:\n        return None\n    fn = extract_nodes(first(competing_interests_section), \"fn\")\n    interests = footnotes(fn, fntype_filter)\n\n    return interests"}
{"func_code_string": "def author_contributions(soup, fntype_filter):\n    \"\"\"\n    Find the fn tags included in the competing interest\n    \"\"\"\n\n    author_contributions_section = extract_nodes(soup, \"fn-group\", attr=\"content-type\", value=\"author-contribution\")\n    if not author_contributions_section:\n        return None\n    fn = extract_nodes(first(author_contributions_section), \"fn\")\n    cons = footnotes(fn, fntype_filter)\n\n    return cons"}
{"func_code_string": "def full_award_groups(soup):\n    \"\"\"\n    Find the award-group items and return a list of details\n    \"\"\"\n    award_groups = []\n\n    funding_group_section = extract_nodes(soup, \"funding-group\")\n    # counter for auto generated id values, if required\n    generated_id_counter = 1\n    for fg in funding_group_section:\n\n        award_group_tags = extract_nodes(fg, \"award-group\")\n\n        for ag in award_group_tags:\n            if 'id' in ag.attrs:\n                ref = ag['id']\n            else:\n                # hack: generate and increment an id value none is available\n                ref = \"award-group-{id}\".format(id=generated_id_counter)\n                generated_id_counter += 1\n\n            award_group = {}\n            award_group_id = award_group_award_id(ag)\n            if award_group_id is not None:\n                award_group['award-id'] = first(award_group_id)\n            funding_sources = full_award_group_funding_source(ag)\n            source = first(funding_sources)\n            if source is not None:\n                copy_attribute(source, 'institution', award_group)\n                copy_attribute(source, 'institution-id', award_group, 'id')\n                copy_attribute(source, 'institution-id-type', award_group, destination_key='id-type')\n            award_group_by_ref = {}\n            award_group_by_ref[ref] = award_group\n            award_groups.append(award_group_by_ref)\n\n    return award_groups"}
{"func_code_string": "def award_groups(soup):\n    \"\"\"\n    Find the award-group items and return a list of details\n    \"\"\"\n    award_groups = []\n\n    funding_group_section = extract_nodes(soup, \"funding-group\")\n    for fg in funding_group_section:\n\n        award_group_tags = extract_nodes(fg, \"award-group\")\n\n        for ag in award_group_tags:\n\n            award_group = {}\n\n            award_group['funding_source'] = award_group_funding_source(ag)\n            award_group['recipient'] = award_group_principal_award_recipient(ag)\n            award_group['award_id'] = award_group_award_id(ag)\n\n            award_groups.append(award_group)\n\n    return award_groups"}
{"func_code_string": "def award_group_funding_source(tag):\n    \"\"\"\n    Given a funding group element\n    Find the award group funding sources, one for each\n    item found in the get_funding_group section\n    \"\"\"\n    award_group_funding_source = []\n    funding_source_tags = extract_nodes(tag, \"funding-source\")\n    for t in funding_source_tags:\n        award_group_funding_source.append(t.text)\n    return award_group_funding_source"}
{"func_code_string": "def full_award_group_funding_source(tag):\n    \"\"\"\n    Given a funding group element\n    Find the award group funding sources, one for each\n    item found in the get_funding_group section\n    \"\"\"\n    award_group_funding_sources = []\n    funding_source_nodes = extract_nodes(tag, \"funding-source\")\n    for funding_source_node in funding_source_nodes:\n\n        award_group_funding_source = {}\n\n        institution_nodes = extract_nodes(funding_source_node, 'institution')\n\n        institution_node = first(institution_nodes)\n        if institution_node:\n            award_group_funding_source['institution'] = node_text(institution_node)\n            if 'content-type' in institution_node.attrs:\n                award_group_funding_source['institution-type'] = institution_node['content-type']\n\n        institution_id_nodes = extract_nodes(funding_source_node, 'institution-id')\n        institution_id_node = first(institution_id_nodes)\n        if institution_id_node:\n            award_group_funding_source['institution-id'] = node_text(institution_id_node)\n            if 'institution-id-type' in institution_id_node.attrs:\n                award_group_funding_source['institution-id-type'] = institution_id_node['institution-id-type']\n\n        award_group_funding_sources.append(award_group_funding_source)\n\n    return award_group_funding_sources"}
{"func_code_string": "def award_group_award_id(tag):\n    \"\"\"\n    Find the award group award id, one for each\n    item found in the get_funding_group section\n    \"\"\"\n    award_group_award_id = []\n    award_id_tags = extract_nodes(tag, \"award-id\")\n    for t in award_id_tags:\n        award_group_award_id.append(t.text)\n    return award_group_award_id"}
{"func_code_string": "def award_group_principal_award_recipient(tag):\n    \"\"\"\n    Find the award group principal award recipient, one for each\n    item found in the get_funding_group section\n    \"\"\"\n    award_group_principal_award_recipient = []\n    principal_award_recipients = extract_nodes(tag, \"principal-award-recipient\")\n\n    for t in principal_award_recipients:\n        principal_award_recipient_text = \"\"\n\n        institution = node_text(first(extract_nodes(t, \"institution\")))\n        surname = node_text(first(extract_nodes(t, \"surname\")))\n        given_names = node_text(first(extract_nodes(t, \"given-names\")))\n        string_name = node_text(first(raw_parser.string_name(t)))\n        # Concatenate name and institution values if found\n        #  while filtering out excess whitespace\n        if(given_names):\n            principal_award_recipient_text += given_names\n        if(principal_award_recipient_text != \"\"):\n            principal_award_recipient_text += \" \"\n        if(surname):\n            principal_award_recipient_text += surname\n        if(institution):\n            principal_award_recipient_text += institution\n        if(string_name):\n            principal_award_recipient_text += string_name\n\n        award_group_principal_award_recipient.append(principal_award_recipient_text)\n    return award_group_principal_award_recipient"}
{"func_code_string": "def object_id_doi(tag, parent_tag_name=None):\n    \"\"\"DOI in an object-id tag found inside the tag\"\"\"\n    doi = None\n    object_id = None\n    object_ids = raw_parser.object_id(tag, \"doi\")\n    if object_ids:\n        object_id = first([id_ for id_ in object_ids])\n    if parent_tag_name and object_id and object_id.parent.name != parent_tag_name:\n        object_id = None\n    if object_id:\n        doi = node_contents_str(object_id)\n    return doi"}
{"func_code_string": "def title_tag_inspected(tag, parent_tag_name=None, p_parent_tag_name=None, direct_sibling_only=False):\n    \"\"\"Extract the title tag and sometimes inspect its parents\"\"\"\n\n    title_tag = None\n    if direct_sibling_only is True:\n        for sibling_tag in tag:\n            if sibling_tag.name and sibling_tag.name == \"title\":\n                title_tag = sibling_tag\n    else:\n        title_tag = raw_parser.title(tag)\n\n    if parent_tag_name and p_parent_tag_name:\n        if (title_tag and title_tag.parent.name and title_tag.parent.parent.name\n            and title_tag.parent.name == parent_tag_name\n            and title_tag.parent.parent.name == p_parent_tag_name):\n            pass\n        else:\n            title_tag = None\n\n    return title_tag"}
{"func_code_string": "def title_text(tag, parent_tag_name=None, p_parent_tag_name=None, direct_sibling_only=False):\n    \"\"\"Extract the text of a title tag and sometimes inspect its parents\"\"\"\n    title = None\n\n    title_tag = title_tag_inspected(tag, parent_tag_name, p_parent_tag_name, direct_sibling_only)\n\n    if title_tag:\n        title = node_contents_str(title_tag)\n    return title"}
{"func_code_string": "def boxed_text_to_image_block(tag):\n    \"covert boxed-text to an image block containing an inline-graphic\"\n    tag_block = OrderedDict()\n    image_content = body_block_image_content(first(raw_parser.inline_graphic(tag)))\n    tag_block[\"type\"] = \"image\"\n    set_if_value(tag_block, \"doi\", doi_uri_to_doi(object_id_doi(tag, tag.name)))\n    set_if_value(tag_block, \"id\", tag.get(\"id\"))\n    set_if_value(tag_block, \"image\", image_content)\n    # render paragraphs into a caption\n    p_tags = raw_parser.paragraph(tag)\n    caption_content = []\n    for p_tag in p_tags:\n        if not raw_parser.inline_graphic(p_tag):\n            caption_content.append(body_block_content(p_tag))\n    set_if_value(tag_block, \"caption\", caption_content)\n    return tag_block"}
{"func_code_string": "def body_json(soup, base_url=None):\n    \"\"\" Get body json and then alter it with section wrapping and removing boxed-text \"\"\"\n    body_content = body(soup, remove_key_info_box=True, base_url=base_url)\n    # Wrap in a section if the first block is not a section\n    if (body_content and len(body_content) > 0 and \"type\" in body_content[0]\n        and body_content[0][\"type\"] != \"section\"):\n        # Wrap this one\n        new_body_section = OrderedDict()\n        new_body_section[\"type\"] = \"section\"\n        new_body_section[\"id\"] = \"s0\"\n        new_body_section[\"title\"] = \"Main text\"\n        new_body_section[\"content\"] = []\n        for body_block in body_content:\n            new_body_section[\"content\"].append(body_block)\n        new_body = []\n        new_body.append(new_body_section)\n        body_content = new_body\n    body_content_rewritten = elifetools.json_rewrite.rewrite_json(\"body_json\", soup, body_content)\n    return body_content_rewritten"}
{"func_code_string": "def body_block_content_render(tag, recursive=False, base_url=None):\n    \"\"\"\n    Render the tag as body content and call recursively if\n    the tag has child tags\n    \"\"\"\n    block_content_list = []\n    tag_content = OrderedDict()\n\n    if tag.name == \"p\":\n        for block_content in body_block_paragraph_render(tag, base_url=base_url):\n            if block_content != {}:\n                block_content_list.append(block_content)\n    else:\n        tag_content = body_block_content(tag, base_url=base_url)\n\n    nodenames = body_block_nodenames()\n\n    tag_content_content = []\n\n    # Collect the content of the tag but only for some tags\n    if tag.name not in [\"p\", \"fig\", \"table-wrap\", \"list\", \"media\", \"disp-quote\", \"code\"]:\n        for child_tag in tag:\n            if not(hasattr(child_tag, 'name')):\n                continue\n\n            if child_tag.name == \"p\":\n                # Ignore paragraphs that start with DOI:\n                if node_text(child_tag) and len(remove_doi_paragraph([child_tag])) <= 0:\n                    continue\n                for block_content in body_block_paragraph_render(child_tag, base_url=base_url):\n                    if block_content != {}:\n                        tag_content_content.append(block_content)\n\n            elif child_tag.name == \"fig\" and tag.name == \"fig-group\":\n                # Do not fig inside fig-group a second time\n                pass\n            elif child_tag.name == \"media\" and tag.name == \"fig-group\":\n                # Do not include a media video inside fig-group a second time\n                if child_tag.get(\"mimetype\") == \"video\":\n                    pass\n            else:\n                for block_content in body_block_content_render(child_tag, recursive=True, base_url=base_url):\n                    if block_content != {}:\n                        tag_content_content.append(block_content)\n\n    if len(tag_content_content) > 0:\n        if tag.name in nodenames or recursive is False:\n            tag_content[\"content\"] = []\n            for block_content in tag_content_content:\n                tag_content[\"content\"].append(block_content)\n            block_content_list.append(tag_content)\n        else:\n            # Not a block tag, e.g. a caption tag, let the content pass through\n            block_content_list = tag_content_content\n    else:\n        block_content_list.append(tag_content)\n\n    return block_content_list"}
{"func_code_string": "def body_block_paragraph_render(p_tag, html_flag=True, base_url=None):\n    \"\"\"\n    paragraphs may wrap some other body block content\n    this is separated out so it can be called from more than one place\n    \"\"\"\n    # Configure the XML to HTML conversion preference for shorthand use below\n    convert = lambda xml_string: xml_to_html(html_flag, xml_string, base_url)\n\n    block_content_list = []\n\n    tag_content_content = []\n    nodenames = body_block_nodenames()\n\n    paragraph_content = u''\n    for child_tag in p_tag:\n\n        if child_tag.name is None or body_block_content(child_tag) == {}:\n            paragraph_content = paragraph_content + unicode_value(child_tag)\n\n        else:\n            # Add previous paragraph content first\n            if paragraph_content.strip() != '':\n                tag_content_content.append(body_block_paragraph_content(convert(paragraph_content)))\n                paragraph_content = u''\n\n        if child_tag.name is not None and body_block_content(child_tag) != {}:\n            for block_content in body_block_content_render(child_tag, base_url=base_url):\n                if block_content != {}:\n                    tag_content_content.append(block_content)\n    # finish up\n    if paragraph_content.strip() != '':\n        tag_content_content.append(body_block_paragraph_content(convert(paragraph_content)))\n\n    if len(tag_content_content) > 0:\n        for block_content in tag_content_content:\n            block_content_list.append(block_content)\n\n    return block_content_list"}
{"func_code_string": "def body_block_caption_render(caption_tags, base_url=None):\n    \"\"\"fig and media tag captions are similar so use this common function\"\"\"\n    caption_content = []\n    supplementary_material_tags = []\n\n    for block_tag in remove_doi_paragraph(caption_tags):\n        # Note then skip p tags with supplementary-material inside\n        if raw_parser.supplementary_material(block_tag):\n            for supp_tag in raw_parser.supplementary_material(block_tag):\n                supplementary_material_tags.append(supp_tag)\n            continue\n\n        for block_content in body_block_content_render(block_tag, base_url=base_url):\n\n            if block_content != {}:\n                caption_content.append(block_content)\n\n    return caption_content, supplementary_material_tags"}
{"func_code_string": "def body_block_supplementary_material_render(supp_tags, base_url=None):\n    \"\"\"fig and media tag caption may have supplementary material\"\"\"\n    source_data = []\n    for supp_tag in supp_tags:\n        for block_content in body_block_content_render(supp_tag, base_url=base_url):\n            if block_content != {}:\n                if \"content\" in block_content:\n                    del block_content[\"content\"]\n                source_data.append(block_content)\n    return source_data"}
{"func_code_string": "def body_block_paragraph_content(text):\n    \"for formatting of simple paragraphs of text only, and check if it is all whitespace\"\n    tag_content = OrderedDict()\n    if text and text != '':\n        tag_content[\"type\"] = \"paragraph\"\n        tag_content[\"text\"] = clean_whitespace(text)\n    return tag_content"}
{"func_code_string": "def body_block_image_content(tag):\n    \"format a graphic or inline-graphic into a body block json format\"\n    image_content = OrderedDict()\n    if tag:\n        copy_attribute(tag.attrs, 'xlink:href', image_content, 'uri')\n        if \"uri\" in image_content:\n            # todo!! alt\n            set_if_value(image_content, \"alt\", \"\")\n    return image_content"}
{"func_code_string": "def body_block_title_label_caption(tag_content, title_value, label_value,\n                                   caption_content, set_caption=True, prefer_title=False, prefer_label=False):\n    \"\"\"set the title, label and caption values in a consistent way\n\n    set_caption: insert a \"caption\" field\n    prefer_title: when only one value is available, set title rather than label. If False, set label rather than title\"\"\"\n    set_if_value(tag_content, \"label\", rstrip_punctuation(label_value))\n    set_if_value(tag_content, \"title\", title_value)\n    if set_caption is True and caption_content and len(caption_content) > 0:\n        tag_content[\"caption\"] = caption_content\n    if prefer_title:\n        if \"title\" not in tag_content and label_value:\n            set_if_value(tag_content, \"title\", label_value)\n            del(tag_content[\"label\"])\n    if prefer_label:\n        if \"label\" not in tag_content and title_value:\n            set_if_value(tag_content, \"label\", rstrip_punctuation(title_value))\n            del(tag_content[\"title\"])"}
{"func_code_string": "def body_block_attribution(tag):\n    \"extract the attribution content for figures, tables, videos\"\n    attributions = []\n    if raw_parser.attrib(tag):\n        for attrib_tag in raw_parser.attrib(tag):\n            attributions.append(node_contents_str(attrib_tag))\n    if raw_parser.permissions(tag):\n        # concatenate content from from the permissions tag\n        for permissions_tag in raw_parser.permissions(tag):\n            attrib_string = ''\n            # add the copyright statement if found\n            attrib_string = join_sentences(attrib_string,\n                node_contents_str(raw_parser.copyright_statement(permissions_tag)), '.')\n            # add the license paragraphs\n            if raw_parser.licence_p(permissions_tag):\n                for licence_p_tag in raw_parser.licence_p(permissions_tag):\n                    attrib_string = join_sentences(attrib_string,\n                                                   node_contents_str(licence_p_tag), '.')\n            if attrib_string != '':\n                attributions.append(attrib_string)\n    return attributions"}
{"func_code_string": "def body_blocks(soup):\n    \"\"\"\n    Note: for some reason this works and few other attempted methods work\n    Search for certain node types, find the first nodes siblings of the same type\n    Add the first sibling and the other siblings to a list and return them\n    \"\"\"\n    nodenames = body_block_nodenames()\n\n    body_block_tags = []\n\n    if not soup:\n        return body_block_tags\n\n    first_sibling_node = firstnn(soup.find_all())\n\n    if first_sibling_node is None:\n        return body_block_tags\n\n    sibling_tags = first_sibling_node.find_next_siblings(nodenames)\n\n    # Add the first component tag and the ResultSet tags together\n    body_block_tags.append(first_sibling_node)\n\n    for tag in sibling_tags:\n        body_block_tags.append(tag)\n\n    return body_block_tags"}
{"func_code_string": "def abstract_json(soup):\n    \"\"\"abstract in article json format\"\"\"\n    abstract_tags = raw_parser.abstract(soup)\n    abstract_json = None\n    for tag in abstract_tags:\n        if tag.get(\"abstract-type\") is None:\n            abstract_json = render_abstract_json(tag)\n    return abstract_json"}
{"func_code_string": "def digest_json(soup):\n    \"\"\"digest in article json format\"\"\"\n    abstract_tags = raw_parser.abstract(soup, abstract_type=\"executive-summary\")\n    abstract_json = None\n    for tag in abstract_tags:\n        abstract_json = render_abstract_json(tag)\n    return abstract_json"}
{"func_code_string": "def author_affiliations(author, html_flag=True):\n    \"\"\"compile author affiliations for json output\"\"\"\n\n    # Configure the XML to HTML conversion preference for shorthand use below\n    convert = lambda xml_string: xml_to_html(html_flag, xml_string)\n\n    affilations = []\n\n    if author.get(\"affiliations\"):\n        for affiliation in author.get(\"affiliations\"):\n            affiliation_json = OrderedDict()\n            affiliation_json[\"name\"] = []\n            if affiliation.get(\"dept\"):\n                affiliation_json[\"name\"].append(convert(affiliation.get(\"dept\")))\n            if affiliation.get(\"institution\") and affiliation.get(\"institution\").strip() != '':\n                affiliation_json[\"name\"].append(convert(affiliation.get(\"institution\")))\n            # Remove if empty\n            if affiliation_json[\"name\"] == []:\n                del affiliation_json[\"name\"]\n\n            if ((affiliation.get(\"city\") and affiliation.get(\"city\").strip() != '')\n                or affiliation.get(\"country\") and affiliation.get(\"country\").strip() != ''):\n                affiliation_address = OrderedDict()\n                affiliation_address[\"formatted\"] = []\n                affiliation_address[\"components\"] = OrderedDict()\n                if affiliation.get(\"city\") and affiliation.get(\"city\").strip() != '':\n                    affiliation_address[\"formatted\"].append(affiliation.get(\"city\"))\n                    affiliation_address[\"components\"][\"locality\"] = []\n                    affiliation_address[\"components\"][\"locality\"].append(affiliation.get(\"city\"))\n                if affiliation.get(\"country\") and affiliation.get(\"country\").strip() != '':\n                    affiliation_address[\"formatted\"].append(affiliation.get(\"country\"))\n                    affiliation_address[\"components\"][\"country\"] = affiliation.get(\"country\")\n                # Add if not empty\n                if affiliation_address != {}:\n                    affiliation_json[\"address\"] = affiliation_address\n\n            # Add if not empty\n            if affiliation_json != {}:\n                affilations.append(affiliation_json)\n\n    if affilations != []:\n        return affilations\n    else:\n        return None"}
{"func_code_string": "def author_json_details(author, author_json, contributions, correspondence,\n                        competing_interests, equal_contributions_map, present_address_data,\n                        foot_notes_data, html_flag=True):\n    # Configure the XML to HTML conversion preference for shorthand use below\n    convert = lambda xml_string: xml_to_html(html_flag, xml_string)\n\n    \"\"\"add more author json\"\"\"\n    if author_affiliations(author):\n        author_json[\"affiliations\"] = author_affiliations(author)\n\n    # foot notes or additionalInformation\n    if author_foot_notes(author, foot_notes_data):\n        author_json[\"additionalInformation\"] = author_foot_notes(author, foot_notes_data)\n\n    # email\n    if author_email_addresses(author, correspondence):\n        author_json[\"emailAddresses\"] = author_email_addresses(author, correspondence)\n\n    # phone\n    if author_phone_numbers(author, correspondence):\n        author_json[\"phoneNumbers\"] = author_phone_numbers_json(author, correspondence)\n\n    # contributions\n    if author_contribution(author, contributions):\n        author_json[\"contribution\"] = convert(author_contribution(author, contributions))\n\n    # competing interests\n    if author_competing_interests(author, competing_interests):\n        author_json[\"competingInterests\"] = convert(\n            author_competing_interests(author, competing_interests))\n\n    # equal-contributions\n    if author_equal_contribution(author, equal_contributions_map):\n        author_json[\"equalContributionGroups\"] = author_equal_contribution(author, equal_contributions_map)\n\n    # postalAddress\n    if author_present_address(author, present_address_data):\n        author_json[\"postalAddresses\"] = author_present_address(author, present_address_data)\n\n    return author_json"}
{"func_code_string": "def collab_to_group_author_key_map(authors):\n    \"\"\"compile a map of author collab to group-author-key\"\"\"\n    collab_map = {}\n    for author in authors:\n        if author.get(\"collab\"):\n            collab_map[author.get(\"collab\")] = author.get(\"group-author-key\")\n    return collab_map"}
{"func_code_string": "def map_equal_contributions(contributors):\n    \"\"\"assign numeric values to each unique equal-contrib id\"\"\"\n    equal_contribution_map = {}\n    equal_contribution_keys = []\n    for contributor in contributors:\n        if contributor.get(\"references\") and \"equal-contrib\" in contributor.get(\"references\"):\n            for key in contributor[\"references\"][\"equal-contrib\"]:\n                if key not in equal_contribution_keys:\n                    equal_contribution_keys.append(key)\n    # Do a basic sort\n    equal_contribution_keys = sorted(equal_contribution_keys)\n    # Assign keys based on sorted values\n    for i, equal_contribution_key in enumerate(equal_contribution_keys):\n        equal_contribution_map[equal_contribution_key] = i+1\n    return equal_contribution_map"}
{"func_code_string": "def authors_json(soup):\n    \"\"\"authors list in article json format\"\"\"\n    authors_json_data = []\n    contributors_data = contributors(soup, \"full\")\n    author_contributions_data = author_contributions(soup, None)\n    author_competing_interests_data = competing_interests(soup, None)\n    author_correspondence_data = full_correspondence(soup)\n    authors_non_byline_data = authors_non_byline(soup)\n    equal_contributions_map = map_equal_contributions(contributors_data)\n    present_address_data = present_addresses(soup)\n    foot_notes_data = other_foot_notes(soup)\n\n    # First line authors builds basic structure\n    for contributor in contributors_data:\n        author_json = None\n        if contributor[\"type\"] == \"author\" and contributor.get(\"collab\"):\n            author_json = author_group(contributor, author_contributions_data,\n                                       author_correspondence_data, author_competing_interests_data,\n                                       equal_contributions_map, present_address_data,\n                                       foot_notes_data)\n        elif contributor.get(\"on-behalf-of\"):\n            author_json = author_on_behalf_of(contributor)\n        elif contributor[\"type\"] == \"author\" and not contributor.get(\"group-author-key\"):\n            author_json = author_person(contributor, author_contributions_data,\n                                        author_correspondence_data, author_competing_interests_data,\n                                        equal_contributions_map, present_address_data, foot_notes_data)\n\n        if author_json:\n            authors_json_data.append(author_json)\n\n    # Second, add byline author data\n    collab_map = collab_to_group_author_key_map(contributors_data)\n    for contributor in [elem for elem in contributors_data if elem.get(\"group-author-key\") and not elem.get(\"collab\")]:\n        for group_author in [elem for elem in authors_json_data if elem.get('type') == 'group']:\n            group_author_key = None\n            if group_author[\"name\"] in collab_map:\n                group_author_key = collab_map[group_author[\"name\"]]\n            if contributor.get(\"group-author-key\") == group_author_key:\n                author_json = author_person(contributor, author_contributions_data,\n                                            author_correspondence_data, author_competing_interests_data,\n                                            equal_contributions_map, present_address_data, foot_notes_data)\n                if contributor.get(\"sub-group\"):\n                    if \"groups\" not in group_author:\n                        group_author[\"groups\"] = OrderedDict()\n                    if contributor.get(\"sub-group\") not in group_author[\"groups\"]:\n                        group_author[\"groups\"][contributor.get(\"sub-group\")] = []\n                    group_author[\"groups\"][contributor.get(\"sub-group\")].append(author_json)\n                else:\n                    if \"people\" not in group_author:\n                        group_author[\"people\"] = []\n                    group_author[\"people\"].append(author_json)\n\n    authors_json_data_rewritten = elifetools.json_rewrite.rewrite_json(\"authors_json\", soup, authors_json_data)\n    return authors_json_data_rewritten"}
{"func_code_string": "def author_line(soup):\n    \"\"\"take preferred names from authors json and format them into an author line\"\"\"\n    author_line = None\n    authors_json_data = authors_json(soup)\n    author_names = extract_author_line_names(authors_json_data)\n    if len(author_names) > 0:\n        author_line = format_author_line(author_names)\n    return author_line"}
{"func_code_string": "def format_author_line(author_names):\n    \"\"\"authorLine format depends on if there is 1, 2 or more than 2 authors\"\"\"\n    author_line = None\n    if not author_names:\n        return author_line\n    if len(author_names) <= 2:\n        author_line = \", \".join(author_names)\n    elif len(author_names) > 2:\n        author_line = author_names[0] + \" et al.\"\n    return author_line"}
{"func_code_string": "def references_date(year=None):\n    \"Handle year value parsing for some edge cases\"\n    date = None\n    discriminator = None\n    in_press = None\n    if year and \"in press\" in year.lower().strip():\n        in_press = True\n    elif year and re.match(\"^[0-9]+$\", year):\n        date = year\n    elif year:\n        discriminator_match = re.match(\"^([0-9]+?)([a-z]+?)$\", year)\n        if discriminator_match:\n            date = discriminator_match.group(1)\n            discriminator = discriminator_match.group(2)\n        else:\n            date = year\n    return (date, discriminator, in_press)"}
{"func_code_string": "def references_json_authors(ref_authors, ref_content):\n    \"build the authors for references json here for testability\"\n    all_authors = references_authors(ref_authors)\n    if all_authors != {}:\n        if ref_content.get(\"type\") in [\"conference-proceeding\", \"journal\", \"other\",\n                                           \"periodical\", \"preprint\", \"report\", \"web\"]:\n            for author_type in [\"authors\", \"authorsEtAl\"]:\n                set_if_value(ref_content, author_type, all_authors.get(author_type))\n        elif ref_content.get(\"type\") in [\"book\", \"book-chapter\"]:\n            for author_type in [\"authors\", \"authorsEtAl\", \"editors\", \"editorsEtAl\"]:\n                set_if_value(ref_content, author_type, all_authors.get(author_type))\n        elif ref_content.get(\"type\") in [\"clinical-trial\"]:\n            # Always set as authors, once,  then add the authorsType\n            for author_type in [\"authors\", \"collaborators\", \"sponsors\"]:\n                if \"authorsType\" not in ref_content and all_authors.get(author_type):\n                    set_if_value(ref_content, \"authors\", all_authors.get(author_type))\n                    set_if_value(ref_content, \"authorsEtAl\", all_authors.get(author_type + \"EtAl\"))\n                    ref_content[\"authorsType\"] = author_type\n        elif ref_content.get(\"type\") in [\"data\", \"software\"]:\n            for author_type in [\"authors\", \"authorsEtAl\",\n                                \"compilers\", \"compilersEtAl\", \"curators\", \"curatorsEtAl\"]:\n                set_if_value(ref_content, author_type, all_authors.get(author_type))\n        elif ref_content.get(\"type\") in [\"patent\"]:\n            for author_type in [\"inventors\", \"inventorsEtAl\", \"assignees\", \"assigneesEtAl\"]:\n                set_if_value(ref_content, author_type, all_authors.get(author_type))\n        elif ref_content.get(\"type\") in [\"thesis\"]:\n            # Convert list to a non-list\n            if all_authors.get(\"authors\") and len(all_authors.get(\"authors\")) > 0:\n                ref_content[\"author\"] = all_authors.get(\"authors\")[0]\n    return ref_content"}
{"func_code_string": "def convert_references_json(ref_content, soup=None):\n    \"Check for references that will not pass schema validation, fix or convert them to unknown\"\n\n    # Convert reference to unkonwn if still missing important values\n    if (\n        (ref_content.get(\"type\") == \"other\")\n        or\n        (ref_content.get(\"type\") == \"book-chapter\" and \"editors\" not in ref_content)\n        or\n        (ref_content.get(\"type\") == \"journal\" and \"articleTitle\" not in ref_content)\n        or\n        (ref_content.get(\"type\") in [\"journal\", \"book-chapter\"]\n         and not \"pages\" in ref_content)\n        or\n        (ref_content.get(\"type\") == \"journal\" and \"journal\" not in ref_content)\n        or\n        (ref_content.get(\"type\") in [\"book\", \"book-chapter\", \"report\", \"thesis\", \"software\"]\n         and \"publisher\" not in ref_content)\n        or\n        (ref_content.get(\"type\") == \"book\" and \"bookTitle\" not in ref_content)\n        or\n        (ref_content.get(\"type\") == \"data\" and \"source\" not in ref_content)\n        or\n        (ref_content.get(\"type\") == \"conference-proceeding\" and \"conference\" not in ref_content)\n       ):\n        ref_content = references_json_to_unknown(ref_content, soup)\n\n    return ref_content"}
{"func_code_string": "def references_json_unknown_details(ref_content, soup=None):\n    \"Extract detail value for references of type unknown\"\n    details = \"\"\n\n    # Try adding pages values first\n    if \"pages\" in ref_content:\n        if \"range\" in ref_content[\"pages\"]:\n            details += ref_content[\"pages\"][\"range\"]\n        else:\n            details += ref_content[\"pages\"]\n\n    if soup:\n        # Attempt to find the XML element by id, and convert it to details\n        if \"id\" in ref_content:\n            ref_tag = first(soup.select(\"ref#\" + ref_content[\"id\"]))\n            if ref_tag:\n                # Now remove tags that would be already part of the unknown reference by now\n                for remove_tag in [\"person-group\", \"year\", \"article-title\",\n                                   \"elocation-id\", \"fpage\", \"lpage\"]:\n                    ref_tag = remove_tag_from_tag(ref_tag, remove_tag)\n                # Add the remaining tag content comma separated\n                for tag in first(raw_parser.element_citation(ref_tag)):\n                    if node_text(tag) is not None:\n                        if details != \"\":\n                            details += \", \"\n                        details += node_text(tag)\n    if details == \"\":\n        return None\n    else:\n        return details"}
{"func_code_string": "def unwrap_appendix_box(json_content):\n    \"\"\"for use in removing unwanted boxed-content from appendices json\"\"\"\n    if json_content.get(\"content\") and len(json_content[\"content\"]) > 0:\n        first_block = json_content[\"content\"][0]\n        if (first_block.get(\"type\")\n            and first_block.get(\"type\") == \"box\"\n            and first_block.get(\"content\")):\n            if first_block.get(\"doi\") and not json_content.get(\"doi\"):\n                json_content[\"doi\"] = first_block.get(\"doi\")\n            json_content[\"content\"] = first_block[\"content\"]\n    return json_content"}
{"func_code_string": "def extract_schemas_from_file(source_path):\n    \"\"\"Extract schemas from 'source_path'.\n\n    :returns: a list of ViewSchema objects on success, None if no schemas\n        could be extracted.\n    \"\"\"\n    logging.info(\"Extracting schemas from %s\", source_path)\n    try:\n        with open(source_path, 'r') as source_file:\n            source = source_file.read()\n    except (FileNotFoundError, PermissionError) as e:\n        logging.error(\"Cannot extract schemas: %s\", e.strerror)\n    else:\n        try:\n            schemas = extract_schemas_from_source(source, source_path)\n        except SyntaxError as e:\n            logging.error(\"Cannot extract schemas: %s\", str(e))\n        else:\n            logging.info(\n                \"Extracted %d %s\",\n                len(schemas),\n                \"schema\" if len(schemas) == 1 else \"schemas\")\n            return schemas"}
{"func_code_string": "def _get_simple_assignments(tree):\n    \"\"\"Get simple assignments from node tree.\"\"\"\n    result = {}\n    for node in ast.walk(tree):\n        if isinstance(node, ast.Assign):\n            for target in node.targets:\n                if isinstance(target, ast.Name):\n                    result[target.id] = node.value\n    return result"}
{"func_code_string": "def extract_schemas_from_source(source, filename='<unknown>'):\n    \"\"\"Extract schemas from 'source'.\n\n    The 'source' parameter must be a string, and should be valid python\n    source.\n\n    If 'source' is not valid python source, a SyntaxError will be raised.\n\n    :returns: a list of ViewSchema objects.\n    \"\"\"\n    # Track which acceptable services have been configured.\n    acceptable_services = set()\n    # Track which acceptable views have been configured:\n    acceptable_views = {}\n    schemas_found = []\n    ast_tree = ast.parse(source, filename)\n    simple_names = _get_simple_assignments(ast_tree)\n\n    assigns = [n for n in ast_tree.body if isinstance(n, ast.Assign)]\n    call_assigns = [n for n in assigns if isinstance(n.value, ast.Call)]\n\n    # We need to extract the AcceptableService-related views. We parse the\n    # assignations twice: The first time to extract the AcceptableService\n    # instances, the second to extract the views created on those services.\n    for assign in call_assigns:\n        if isinstance(assign.value.func, ast.Attribute):\n            continue\n        if assign.value.func.id == 'AcceptableService':\n            for target in assign.targets:\n                acceptable_services.add(target.id)\n\n    for assign in call_assigns:\n        # only consider calls which are attribute accesses, AND\n        # calls where the object being accessed is in acceptable_services, AND\n        # calls where the attribute being accessed is the 'api' method.\n        if isinstance(assign.value.func, ast.Attribute) and \\\n           assign.value.func.value.id in acceptable_services and \\\n           assign.value.func.attr == 'api':\n            # this is a view. We need to extract the url and methods specified.\n            # they may be specified positionally or via a keyword.\n            url = None\n            name = None\n            # methods has a default value:\n            methods = ['GET']\n\n            # This is a view - the URL is the first positional argument:\n            args = assign.value.args\n            if len(args) >= 1:\n                url = ast.literal_eval(args[0])\n            if len(args) >= 2:\n                name = ast.literal_eval(args[1])\n            kwargs = assign.value.keywords\n            for kwarg in kwargs:\n                if kwarg.arg == 'url':\n                    url = ast.literal_eval(kwarg.value)\n                if kwarg.arg == 'methods':\n                    methods = ast.literal_eval(kwarg.value)\n                if kwarg.arg == 'view_name':\n                    name = ast.literal_eval(kwarg.value)\n            if url and name:\n                for target in assign.targets:\n                    acceptable_views[target.id] = {\n                        'url': url,\n                        'name': name,\n                        'methods': methods,\n                    }\n\n    # iterate over all functions, attempting to find the views.\n    functions = [n for n in ast_tree.body if isinstance(n, ast.FunctionDef)]\n    for function in functions:\n        input_schema = None\n        output_schema = None\n        doc = ast.get_docstring(function)\n        api_options_list = []\n        for decorator in function.decorator_list:\n            if not isinstance(decorator, ast.Call):\n                continue\n            if isinstance(decorator.func, ast.Attribute):\n                decorator_name = decorator.func.value.id\n                # extract version this view was introduced at, which can be\n                # specified as an arg or a kwarg:\n                version = None\n                for kwarg in decorator.keywords:\n                    if kwarg.arg == 'introduced_at':\n                        version = ast.literal_eval(kwarg.value)\n                        break\n                if len(decorator.args) == 1:\n                    version = ast.literal_eval(decorator.args[0])\n\n                if decorator_name in acceptable_views:\n                    api_options = acceptable_views[decorator_name]\n                    api_options['version'] = version\n                    api_options_list.append(api_options)\n            else:\n                decorator_name = decorator.func.id\n                if decorator_name == 'validate_body':\n                    _SimpleNamesResolver(simple_names).visit(decorator.args[0])\n                    input_schema = ast.literal_eval(decorator.args[0])\n                if decorator_name == 'validate_output':\n                    _SimpleNamesResolver(simple_names).visit(decorator.args[0])\n                    output_schema = ast.literal_eval(decorator.args[0])\n        for api_options in api_options_list:\n            schema = ViewSchema(\n                    view_name=api_options['name'],\n                    version=api_options['version'],\n                    input_schema=input_schema,\n                    output_schema=output_schema,\n                    methods=api_options['methods'],\n                    url=api_options['url'],\n                    doc=doc,\n                )\n            schemas_found.append(schema)\n    return schemas_found"}
{"func_code_string": "def render_value(value):\n    \"\"\"Render a value, ensuring that any nested dicts are sorted by key.\"\"\"\n    if isinstance(value, list):\n        return '[' + ', '.join(render_value(v) for v in value) + ']'\n    elif isinstance(value, dict):\n        return (\n            '{' +\n            ', '.join('{k!r}: {v}'.format(\n                k=k, v=render_value(v)) for k, v in sorted(value.items())) +\n            '}')\n    else:\n        return repr(value)"}
{"func_code_string": "def write_service_double_file(target_root, service_name, rendered):\n    \"\"\"Render syntactically valid python service double code.\"\"\"\n    target_path = os.path.join(\n        target_root,\n        'snapstore_schemas', 'service_doubles', '%s.py' % service_name\n    )\n    with open(target_path, 'w') as target_file:\n        target_file.write(rendered)"}
{"func_code_string": "def clean_docstring(docstring):\n    \"\"\"Dedent docstring, special casing the first line.\"\"\"\n    docstring = docstring.strip()\n    if '\\n' in docstring:\n        # multiline docstring\n        if docstring[0].isspace():\n            # whole docstring is indented\n            return textwrap.dedent(docstring)\n        else:\n            # first line not indented, rest maybe\n            first, _, rest = docstring.partition('\\n')\n            return first + '\\n' + textwrap.dedent(rest)\n    return docstring"}
{"func_code_string": "def _sort_schema(schema):\n    \"\"\"Recursively sorts a JSON schema by dict key.\"\"\"\n\n    if isinstance(schema, dict):\n        for k, v in sorted(schema.items()):\n            if isinstance(v, dict):\n                yield k, OrderedDict(_sort_schema(v))\n            elif isinstance(v, list):\n                yield k, list(_sort_schema(v))\n            else:\n                yield k, v\n    elif isinstance(schema, list):\n        for v in schema:\n            if isinstance(v, dict):\n                yield OrderedDict(_sort_schema(v))\n            elif isinstance(v, list):\n                yield list(_sort_schema(v))\n            else:\n                yield v\n    else:\n        yield d"}
{"func_code_string": "def urlmap(patterns):\n    \"\"\"Recursively build a map of (group, name) => url patterns.\n\n    Group is either the resolver namespace or app name for the url config.\n\n    The urls are joined with any prefixes, and cleaned up of extraneous regex\n    specific syntax.\"\"\"\n    for pattern in patterns:\n        group = getattr(pattern, 'namespace', None)\n        if group is None:\n            group = getattr(pattern, 'app_name', None)\n        path = '/' + get_pattern(pattern).lstrip('^').rstrip('$')\n        if isinstance(pattern, PATTERNS):\n            yield (group, pattern.name), path\n        elif isinstance(pattern, RESOLVERS):\n            subpatterns = pattern.url_patterns\n            for (_, name), subpath in urlmap(subpatterns):\n                yield (group, name), path.rstrip('/') + subpath"}
{"func_code_string": "def get_field_schema(name, field):\n    \"\"\"Returns a JSON Schema representation of a form field.\"\"\"\n    field_schema = {\n        'type': 'string',\n    }\n\n    if field.label:\n        field_schema['title'] = str(field.label)  # force translation\n\n    if field.help_text:\n        field_schema['description'] = str(field.help_text)  # force translation\n\n    if isinstance(field, (fields.URLField, fields.FileField)):\n        field_schema['format'] = 'uri'\n    elif isinstance(field, fields.EmailField):\n        field_schema['format'] = 'email'\n    elif isinstance(field, fields.DateTimeField):\n        field_schema['format'] = 'date-time'\n    elif isinstance(field, fields.DateField):\n        field_schema['format'] = 'date'\n    elif isinstance(field, (fields.DecimalField, fields.FloatField)):\n        field_schema['type'] = 'number'\n    elif isinstance(field, fields.IntegerField):\n        field_schema['type'] = 'integer'\n    elif isinstance(field, fields.NullBooleanField):\n        field_schema['type'] = 'boolean'\n    elif isinstance(field.widget, widgets.CheckboxInput):\n        field_schema['type'] = 'boolean'\n\n    if getattr(field, 'choices', []):\n        field_schema['enum'] = sorted([choice[0] for choice in field.choices])\n\n    # check for multiple values\n    if isinstance(field.widget, (widgets.Select, widgets.ChoiceWidget)):\n        if field.widget.allow_multiple_selected:\n            # promote to array of <type>, move details into the items field\n            field_schema['items'] = {\n                'type': field_schema['type'],\n            }\n            if 'enum' in field_schema:\n                field_schema['items']['enum'] = field_schema.pop('enum')\n            field_schema['type'] = 'array'\n\n    return field_schema"}
{"func_code_string": "def get_form_schema(form):\n    \"\"\"Return a JSON Schema object for a Django Form.\"\"\"\n    schema = {\n        'type': 'object',\n        'properties': {},\n    }\n\n    for name, field in form.base_fields.items():\n        schema['properties'][name] = get_field_schema(name, field)\n        if field.required:\n            schema.setdefault('required', []).append(name)\n\n    return schema"}
{"func_code_string": "def handler(self, handler_class):\n        \"\"\"Link to an API handler class (e.g. piston or DRF).\"\"\"\n        self.handler_class = handler_class\n        # we take the docstring from the handler class, not the methods\n        if self.docs is None and handler_class.__doc__:\n            self.docs = clean_docstring(handler_class.__doc__)\n        return handler_class"}
{"func_code_string": "def xml_to_html(html_flag, xml_string, base_url=None):\n    \"For formatting json output into HTML friendly format\"\n    if not xml_string or not html_flag is True:\n        return xml_string\n    html_string = xml_string\n    html_string = remove_comment_tags(html_string)\n    #  Escape unmatched angle brackets\n    if '<' in html_string or '>' in html_string:\n        html_string = escape_html(html_string)\n    # Replace more tags\n    html_string = replace_xref_tags(html_string)\n    html_string = replace_ext_link_tags(html_string)\n    html_string = replace_email_tags(html_string)\n    html_string = replace_inline_graphic_tags(html_string, base_url)\n    html_string = replace_named_content_tags(html_string)\n    html_string = replace_mathml_tags(html_string)\n    html_string = replace_table_style_author_callout(html_string)\n    html_string = replace_simple_tags(html_string, 'italic', 'i')\n    html_string = replace_simple_tags(html_string, 'bold', 'b')\n    html_string = replace_simple_tags(html_string, 'underline', 'span', '<span class=\"underline\">')\n    html_string = replace_simple_tags(html_string, 'sc', 'span', '<span class=\"small-caps\">')\n    html_string = replace_simple_tags(html_string, 'monospace', 'span', '<span class=\"monospace\">')\n    html_string = replace_simple_tags(html_string, 'inline-formula', None)\n    html_string = replace_simple_tags(html_string, 'break', 'br')\n    return html_string"}
{"func_code_string": "def replace_simple_tags(s, from_tag='italic', to_tag='i', to_open_tag=None):\n    \"\"\"\n    Replace tags such as <italic> to <i>\n    This does not validate markup\n    \"\"\"\n    if to_open_tag:\n        s = s.replace('<' + from_tag + '>', to_open_tag)\n    elif to_tag:\n        s = s.replace('<' + from_tag + '>', '<' + to_tag + '>')\n        s = s.replace('<' + from_tag + '/>', '<' + to_tag + '/>')\n    else:\n        s = s.replace('<' + from_tag + '>', '')\n        s = s.replace('<' + from_tag + '/>', '')\n\n    if to_tag:\n        s = s.replace('</' + from_tag + '>', '</' + to_tag + '>')\n    else:\n        s = s.replace('</' + from_tag + '>', '')\n\n    return s"}
{"func_code_string": "def validate_body(schema):\n    \"\"\"Validate the body of incoming requests for a flask view.\n\n    An example usage might look like this::\n\n        from snapstore_schemas import validate_body\n\n\n        @validate_body({\n            'type': 'array',\n            'items': {\n                'type': 'object',\n                'properties': {\n                    'snap_id': {'type': 'string'},\n                    'series': {'type': 'string'},\n                    'name': {'type': 'string'},\n                    'title': {'type': 'string'},\n                    'keywords': {\n                        'type': 'array',\n                        'items': {'type': 'string'}\n                    },\n                    'summary': {'type': 'string'},\n                    'description': {'type': 'string'},\n                    'created_at': {'type': 'string'},\n                },\n                'required': ['snap_id', 'series'],\n                'additionalProperties': False\n            }\n        })\n        def my_flask_view():\n            # view code here\n            return \"Hello World\", 200\n\n    All incoming request that have been routed to this view will be matched\n    against the specified schema. If the request body does not match the schema\n    an instance of `DataValidationError` will be raised.\n\n    By default this will cause the flask application to return a 500 response,\n    but this can be customised by telling flask how to handle these exceptions.\n    The exception instance has an 'error_list' attribute that contains a list\n    of all the errors encountered while processing the request body.\n    \"\"\"\n    location = get_callsite_location()\n\n    def decorator(fn):\n        validate_schema(schema)\n        wrapper = wrap_request(fn, schema)\n        record_schemas(\n            fn, wrapper, location, request_schema=sort_schema(schema))\n        return wrapper\n\n    return decorator"}
{"func_code_string": "def record_schemas(\n        fn, wrapper, location, request_schema=None, response_schema=None):\n    \"\"\"Support extracting the schema from the decorated function.\"\"\"\n    # have we already been decorated by an acceptable api call?\n    has_acceptable = hasattr(fn, '_acceptable_metadata')\n\n    if request_schema is not None:\n        # preserve schema for later use\n        wrapper._request_schema = wrapper._request_schema = request_schema\n        wrapper._request_schema_location = location\n        if has_acceptable:\n            fn._acceptable_metadata._request_schema = request_schema\n            fn._acceptable_metadata._request_schema_location = location\n\n    if response_schema is not None:\n        # preserve schema for later use\n        wrapper._response_schema = wrapper._response_schema = response_schema\n        wrapper._response_schema_location = location\n        if has_acceptable:\n            fn._acceptable_metadata._response_schema = response_schema\n            fn._acceptable_metadata._response_schema_location = location"}
{"func_code_string": "def validate_output(schema):\n    \"\"\"Validate the body of a response from a flask view.\n\n    Like `validate_body`, this function compares a json document to a\n    jsonschema specification. However, this function applies the schema to the\n    view response.\n\n    Instead of the view returning a flask response object, it should instead\n    return a Python list or dictionary. For example::\n\n        from snapstore_schemas import validate_output\n\n        @validate_output({\n            'type': 'object',\n            'properties': {\n                'ok': {'type': 'boolean'},\n            },\n            'required': ['ok'],\n            'additionalProperties': False\n        }\n        def my_flask_view():\n            # view code here\n            return {'ok': True}\n\n    Every view response will be evaluated against the schema. Any that do not\n    comply with the schema will cause DataValidationError to be raised.\n    \"\"\"\n    location = get_callsite_location()\n\n    def decorator(fn):\n        validate_schema(schema)\n        wrapper = wrap_response(fn, schema)\n        record_schemas(\n            fn, wrapper, location, response_schema=sort_schema(schema))\n        return wrapper\n\n    return decorator"}
{"func_code_string": "def validate(payload, schema):\n    \"\"\"Validate `payload` against `schema`, returning an error list.\n\n    jsonschema provides lots of information in it's errors, but it can be a bit\n    of work to extract all the information.\n    \"\"\"\n    v = jsonschema.Draft4Validator(\n        schema, format_checker=jsonschema.FormatChecker())\n    error_list = []\n    for error in v.iter_errors(payload):\n        message = error.message\n        location = '/' + '/'.join([str(c) for c in error.absolute_path])\n        error_list.append(message + ' at ' + location)\n    return error_list"}
{"func_code_string": "def connect(url, max_retries=None, **kwargs):\n    \"\"\"Connects to a Phoenix query server.\n\n    :param url:\n        URL to the Phoenix query server, e.g. ``http://localhost:8765/``\n\n    :param autocommit:\n        Switch the connection to autocommit mode.\n\n    :param readonly:\n        Switch the connection to readonly mode.\n\n    :param max_retries:\n        The maximum number of retries in case there is a connection error.\n\n    :param cursor_factory:\n        If specified, the connection's :attr:`~phoenixdb.connection.Connection.cursor_factory` is set to it.\n\n    :returns:\n        :class:`~phoenixdb.connection.Connection` object.\n    \"\"\"\n    client = AvaticaClient(url, max_retries=max_retries)\n    client.connect()\n    return Connection(client, **kwargs)"}
{"func_code_string": "def connect(self):\n        \"\"\"Opens a HTTP connection to the RPC server.\"\"\"\n        logger.debug(\"Opening connection to %s:%s\", self.url.hostname, self.url.port)\n        try:\n            self.connection = httplib.HTTPConnection(self.url.hostname, self.url.port)\n            self.connection.connect()\n        except (httplib.HTTPException, socket.error) as e:\n            raise errors.InterfaceError('Unable to connect to the specified service', e)"}
{"func_code_string": "def close(self):\n        \"\"\"Closes the HTTP connection to the RPC server.\"\"\"\n        if self.connection is not None:\n            logger.debug(\"Closing connection to %s:%s\", self.url.hostname, self.url.port)\n            try:\n                self.connection.close()\n            except httplib.HTTPException:\n                logger.warning(\"Error while closing connection\", exc_info=True)\n            self.connection = None"}
{"func_code_string": "def connection_sync(self, connection_id, connProps=None):\n        \"\"\"Synchronizes connection properties with the server.\n\n        :param connection_id:\n            ID of the current connection.\n\n        :param connProps:\n            Dictionary with the properties that should be changed.\n\n        :returns:\n            A ``common_pb2.ConnectionProperties`` object.\n        \"\"\"\n        if connProps is None:\n            connProps = {}\n\n        request = requests_pb2.ConnectionSyncRequest()\n        request.connection_id = connection_id\n        request.conn_props.auto_commit = connProps.get('autoCommit', False)\n        request.conn_props.has_auto_commit = True\n        request.conn_props.read_only = connProps.get('readOnly', False)\n        request.conn_props.has_read_only = True\n        request.conn_props.transaction_isolation = connProps.get('transactionIsolation', 0)\n        request.conn_props.catalog = connProps.get('catalog', '')\n        request.conn_props.schema = connProps.get('schema', '')\n\n        response_data = self._apply(request)\n        response = responses_pb2.ConnectionSyncResponse()\n        response.ParseFromString(response_data)\n        return response.conn_props"}
{"func_code_string": "def open_connection(self, connection_id, info=None):\n        \"\"\"Opens a new connection.\n\n        :param connection_id:\n            ID of the connection to open.\n        \"\"\"\n        request = requests_pb2.OpenConnectionRequest()\n        request.connection_id = connection_id\n        if info is not None:\n            # Info is a list of repeated pairs, setting a dict directly fails\n            for k, v in info.items():\n                request.info[k] = v\n\n        response_data = self._apply(request)\n        response = responses_pb2.OpenConnectionResponse()\n        response.ParseFromString(response_data)"}
{"func_code_string": "def close_connection(self, connection_id):\n        \"\"\"Closes a connection.\n\n        :param connection_id:\n            ID of the connection to close.\n        \"\"\"\n        request = requests_pb2.CloseConnectionRequest()\n        request.connection_id = connection_id\n        self._apply(request)"}
{"func_code_string": "def create_statement(self, connection_id):\n        \"\"\"Creates a new statement.\n\n        :param connection_id:\n            ID of the current connection.\n\n        :returns:\n            New statement ID.\n        \"\"\"\n        request = requests_pb2.CreateStatementRequest()\n        request.connection_id = connection_id\n\n        response_data = self._apply(request)\n        response = responses_pb2.CreateStatementResponse()\n        response.ParseFromString(response_data)\n        return response.statement_id"}
{"func_code_string": "def close_statement(self, connection_id, statement_id):\n        \"\"\"Closes a statement.\n\n        :param connection_id:\n            ID of the current connection.\n\n        :param statement_id:\n            ID of the statement to close.\n        \"\"\"\n        request = requests_pb2.CloseStatementRequest()\n        request.connection_id = connection_id\n        request.statement_id = statement_id\n\n        self._apply(request)"}
{"func_code_string": "def prepare_and_execute(self, connection_id, statement_id, sql, max_rows_total=None, first_frame_max_size=None):\n        \"\"\"Prepares and immediately executes a statement.\n\n        :param connection_id:\n            ID of the current connection.\n\n        :param statement_id:\n            ID of the statement to prepare.\n\n        :param sql:\n            SQL query.\n\n        :param max_rows_total:\n            The maximum number of rows that will be allowed for this query.\n\n        :param first_frame_max_size:\n            The maximum number of rows that will be returned in the first Frame returned for this query.\n\n        :returns:\n            Result set with the signature of the prepared statement and the first frame data.\n        \"\"\"\n        request = requests_pb2.PrepareAndExecuteRequest()\n        request.connection_id = connection_id\n        request.statement_id = statement_id\n        request.sql = sql\n        if max_rows_total is not None:\n            request.max_rows_total = max_rows_total\n        if first_frame_max_size is not None:\n            request.first_frame_max_size = first_frame_max_size\n\n        response_data = self._apply(request, 'ExecuteResponse')\n        response = responses_pb2.ExecuteResponse()\n        response.ParseFromString(response_data)\n        return response.results"}
